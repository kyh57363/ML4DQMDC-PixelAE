{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "611f629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'HyperRectangleFitter' from '/eos/home-i01/k/khowey/SWAN_projects/ML4DQMDC-PixelAE/KH-AutoencoderTest/../src/cloudfitters/HyperRectangleFitter.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "# local modules\n",
    "sys.path.append('../utils')\n",
    "import csv_utils as csvu\n",
    "import json_utils as jsonu\n",
    "import dataframe_utils as dfu\n",
    "import hist_utils as hu\n",
    "import autoencoder_utils as aeu\n",
    "import plot_utils as pu\n",
    "import generate_data_utils as gdu\n",
    "import refruns_utils as rru\n",
    "importlib.reload(csvu)\n",
    "importlib.reload(jsonu)\n",
    "importlib.reload(dfu)\n",
    "importlib.reload(hu)\n",
    "importlib.reload(aeu)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(gdu)\n",
    "importlib.reload(rru)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/classifiers')\n",
    "sys.path.append('../src/cloudfitters')\n",
    "import HistStruct\n",
    "importlib.reload(HistStruct)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "import AutoEncoder\n",
    "importlib.reload(AutoEncoder)\n",
    "import SeminormalFitter\n",
    "import GaussianKdeFitter\n",
    "import HyperRectangleFitter\n",
    "importlib.reload(SeminormalFitter)\n",
    "importlib.reload(GaussianKdeFitter)\n",
    "importlib.reload(HyperRectangleFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12373bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Controls\n",
    "# This cell sets all major run properties\n",
    "\n",
    "# Define a list of good 'reference' runs\n",
    "goodrunsls = {'2017':\n",
    "              {\n",
    "                  \"297056\":[[-1]],\n",
    "                  \"297177\":[[-1]],\n",
    "                  \"301449\":[[-1]]  \n",
    "              },\n",
    "              '2018':{\n",
    "                  \"315267\":[[-1]]\n",
    "              }}\n",
    "\n",
    "# Define a list of bad runs\n",
    "badrunsls = {'2017':\n",
    "             {\n",
    "                \"297287\":[[-1]],\n",
    "                \"297288\":[[-1]],\n",
    "                \"297289\":[[-1]],\n",
    "                \"299316\":[[-1]],\n",
    "                \"299324\":[[-1]],\n",
    "                \"299326\":[[-1]],\n",
    "                \"301086\":[[88,126]]\n",
    "             },\n",
    "             '2018':\n",
    "             {\n",
    "                 #\"317479\":[[-1]],\n",
    "                \"317480\":[[-1]],\n",
    "                \"317481\":[[-1]],\n",
    "                \"317482\":[[-1]],\n",
    "                #\"319847\":[[1,35]]\n",
    "             }}\n",
    "\n",
    "# Set the year to be used\n",
    "year = '2017'\n",
    "\n",
    "# Set whether to train globally or locally\n",
    "training_mode = 'global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0085c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset Controls\n",
    "\n",
    "# Set of histogram names to use\n",
    "histnames = ([\n",
    "    'NormalizedHitResiduals_TIB__Layer__1',\n",
    "    'Summary_ClusterStoNCorr__OnTrack__TIB__layer__1',\n",
    "    'NormalizedHitResiduals_TIB__Layer__2',\n",
    "    'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2',\n",
    "    'NormalizedHitResiduals_TIB__Layer__3',\n",
    "    'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3',\n",
    "    'NormalizedHitResiduals_TIB__Layer__4',\n",
    "    'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4',\n",
    "    'chargeInner_PXLayer_1',\n",
    "    'chargeInner_PXLayer_2',\n",
    "    'chargeInner_PXLayer_3',\n",
    "    'chargeInner_PXLayer_4',\n",
    "    'chargeOuter_PXLayer_1',\n",
    "    'chargeOuter_PXLayer_2',\n",
    "    'chargeOuter_PXLayer_3',\n",
    "    'chargeOuter_PXLayer_4',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1571268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Decision to train on entire dataset or a subset\n",
    "\n",
    "# Full dataset training\n",
    "if training_mode == 'global':\n",
    "    runsls_training = None # Decide whether to add a mask for training\n",
    "    runsls_good = None # Decide whether to add a mask for good runs\n",
    "    runsls_bad = badrunsls[year] # Call up our defined set of bad runs\n",
    "\n",
    "# Train on a portion of the dataset\n",
    "elif training_mode == 'local':\n",
    "    # train locally on a small set of runs\n",
    "    # - either on n runs preceding a chosen application run,\n",
    "    # - or on the run associated as reference to the chosen application run.\n",
    "    \n",
    "    # Select application run and apply filter for DCS-bit on\n",
    "    available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+'_'+histnames[0]+'.csv') ) )\n",
    "    run_application = 299316\n",
    "    run_application_index = available_runs.index(run_application)\n",
    "    \n",
    "    # Select a training set/determine whether to use a reference run or not\n",
    "    # Pretty sure this is an alternative to averaging the whole dataset?\n",
    "    usereference = False\n",
    "    if usereference:\n",
    "        run_reference = rru.get_reference_run( run_application, jsonfile='../utils/json_allRunsRefRuns.json' )\n",
    "        if run_reference<0:\n",
    "            raise Exception('no valid reference run has been defined for run {}'.format(run_application))\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(run_reference,[-1])])\n",
    "    else:\n",
    "        ntraining = 5\n",
    "        offset = 0 # normal case: offset = 0 (just use 5 previous runs)\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining-offset:run_application_index-offset]])\n",
    "    \n",
    "    # I don't know what this is for\n",
    "    #runsls_bad = badrunsls[year]\n",
    "    #runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    \n",
    "    # Set up list of good and bad runs for training/evaluation\n",
    "    runsls_bad = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_good = runsls_training\n",
    "    \n",
    "    # Inform user\n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af4938de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding NormalizedHitResiduals_TIB__Layer__1...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding NormalizedHitResiduals_TIB__Layer__2...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding NormalizedHitResiduals_TIB__Layer__3...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding NormalizedHitResiduals_TIB__Layer__4...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeInner_PXLayer_1...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeInner_PXLayer_2...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeInner_PXLayer_3...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeInner_PXLayer_4...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeOuter_PXLayer_1...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeOuter_PXLayer_2...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeOuter_PXLayer_3...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "adding chargeOuter_PXLayer_4...\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "found 28335 histograms\n",
      "created a histstruct with the following properties:\n",
      "- number of histogram types: 16\n",
      "- number of lumisections: 28335\n",
      "- masks: ['dcson', 'golden', 'highstat', 'lowstat', 'bad', 'bad0', 'bad1', 'bad2', 'bad3', 'bad4', 'bad5', 'bad6']\n"
     ]
    }
   ],
   "source": [
    "### Read in data\n",
    "\n",
    "# Controls\n",
    "readnew = True\n",
    "save = False\n",
    "\n",
    "if readnew:\n",
    "    \n",
    "    # Initializations\n",
    "    dloader = DataLoader.DataLoader()\n",
    "    histstruct = HistStruct.HistStruct()\n",
    "    \n",
    "    # Loop over histogram types to store them\n",
    "    for histname in histnames:\n",
    "        print('adding {}...'.format(histname))\n",
    "        \n",
    "        # Bring the histograms into memory from storage for later use\n",
    "        filename = '../data/DF'+year+'B_'+histname+'.csv'\n",
    "        df = dloader.get_dataframe_from_file( filename )\n",
    "        \n",
    "        # In case of local training, we can remove most of the histograms\n",
    "        if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ):\n",
    "            runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()}\n",
    "            df = dfu.select_runsls( df, runsls_total )\n",
    "            \n",
    "        # Store the data in the histstruct object managing this whole thing\n",
    "        histstruct.add_dataframe( df )\n",
    "        \n",
    "    print('found {} histograms'.format(len(histstruct.runnbs)))\n",
    "    \n",
    "    # Add masks - I think this is important for differentiating data with\n",
    "    # different attributes down the line\n",
    "    histstruct.add_dcsonjson_mask( 'dcson' )\n",
    "    histstruct.add_goldenjson_mask('golden' )\n",
    "    histstruct.add_highstat_mask( 'highstat' )\n",
    "    histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=100 )\n",
    "    if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training )\n",
    "    if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good )\n",
    "    \n",
    "    # Count of bad runs for use in applying more masks\n",
    "    nbadruns = 0\n",
    "    if runsls_bad is not None:\n",
    "        histstruct.add_json_mask( 'bad', runsls_bad )\n",
    "        # special case for bad runs: add a mask per run (different bad runs have different characteristics)\n",
    "        nbadruns = len(runsls_bad.keys())\n",
    "        for i,badrun in enumerate(runsls_bad.keys()):\n",
    "            histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} )\n",
    "    \n",
    "    # Save the histogram for later access\n",
    "    if save:\n",
    "        histstruct.save('test.pk1')\n",
    "        \n",
    "if not readnew:\n",
    "    \n",
    "    # Load histstruct from storage\n",
    "    histstruct = HistStruct.HistStruct.load('test.pk1')\n",
    "    \n",
    "    # Count of bad runs, presumably for later use\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name])\n",
    "    \n",
    "# Output to user\n",
    "print('created a histstruct with the following properties:')\n",
    "print('- number of histogram types: {}'.format(len(histstruct.histnames)))\n",
    "print('- number of lumisections: {}'.format(len(histstruct.lsnbs)))\n",
    "print('- masks: {}'.format(list(histstruct.masks.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176b80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the training and/or test sets\n",
    "# Useful for local mode to determine if training set is reliable\n",
    "# and if application run is anomalous\n",
    "\n",
    "skipthiscell = True\n",
    "\n",
    "if(training_mode == 'local' and not skipthiscell):\n",
    "    \n",
    "    # Training and application runs (training vs good set)\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','training'],['dcson','highstat','good']],\n",
    "                                labellist = ['training','testing'],\n",
    "                                colorlist = ['blue','green']\n",
    "                              )\n",
    "    \n",
    "    # Application run and bad test runs (good vs bad set)\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad']],\n",
    "                                labellist = ['good','bad'],\n",
    "                                colorlist = ['green','red']\n",
    "                              )\n",
    "    \n",
    "if( training_mode=='global' and not skipthiscell ):\n",
    "    \n",
    "    # Bad test runs\n",
    "    for i in [0,1,2,3,4,5,6]:\n",
    "        \n",
    "        # Plot good vs bad set\n",
    "        histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad{}'.format(i)]],\n",
    "                                labellist = ['typical good histograms','bad'],\n",
    "                                colorlist = ['blue','red'],\n",
    "                                transparencylist = [0.01,1.]\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extend training with artifical data\n",
    "\n",
    "# Control for the cell\n",
    "extendtraining = False\n",
    "\n",
    "if extendtraining:\n",
    "    \n",
    "    # Bypasses run/lumisection checks since histos are artificial\n",
    "    histstruct.exthistograms['training'] = {}\n",
    "    \n",
    "    # Generate random histograms based on good data\n",
    "    for histname in histstruct.histnames:\n",
    "        # option 1: start from 'training' mask\n",
    "        hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat','training'] )\n",
    "        # option 2: start from averages of DCS-on data\n",
    "        #hists = hu.averagehists( \n",
    "        #            histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] ), \n",
    "        #            1000 )\n",
    "        print('generating artificial training data for '+histname)\n",
    "        (exthists,_,_) = gdu.upsample_hist_set(hists, 5e4, doplot=True )\n",
    "        histstruct.add_exthistograms( 'training', histname, exthists )\n",
    "        print(' -> generated {} histograms'.format(len(histstruct.exthistograms['training'][histname])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e085c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define and train an autoencoder for each element\n",
    "\n",
    "# Controls\n",
    "trainnew = True\n",
    "save = False\n",
    "modelloc = '../models/autoencoders_global_training_dcson_highstat_v20210622'\n",
    "modelbasename = ''\n",
    "\n",
    "if trainnew:\n",
    "    for histname in histstruct.histnames:\n",
    "        \n",
    "        # Choose training set\n",
    "        hists = histstruct.get_histograms( histname=histname, masknames=['dcson','highstat'] )\n",
    "        if extendtraining: hists = histstruct.get_exthistograms( 'training', histname=histname )\n",
    "        print('size of training set: {}'.format(hists.shape))\n",
    "        \n",
    "        # Choose whether to save the model\n",
    "        modelname = modelbasename+'_'+histname+'.h5'\n",
    "        modelname = os.path.join(modelloc,modelname)\n",
    "        if not save: modelname = '' # empty string means do not save models\n",
    "        nepochs = 40 # manual number of epochs\n",
    "        model = aeu.train_simple_autoencoder(hists,nepochs=nepochs,modelname=modelname,\n",
    "                                            batch_size=2000\n",
    "                                            )\n",
    "        classifier = AutoEncoder.AutoEncoder( model=model )\n",
    "        histstruct.add_classifier(histname,classifier)\n",
    "    \n",
    "else:\n",
    "    from autoencoder_utils import mseTop10\n",
    "    for histname in histstruct.histnames:\n",
    "        print('loading model for {}'.format(histname))\n",
    "        modelpath = modelbasename+'_'+histname+'.h5'\n",
    "        modelpath = os.path.join(modelloc,modelpath)\n",
    "        classifier = AutoEncoder.AutoEncoder( modelpath=modelpath )\n",
    "        histstruct.add_classifier(histname,classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
