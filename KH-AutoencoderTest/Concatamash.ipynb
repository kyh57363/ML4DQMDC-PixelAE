{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba412492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-08 18:12:25.368996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.1-8d929/x86_64-centos7-gcc8-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.1-f3599/x86_64-centos7-gcc8-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u222-884d8/x86_64-centos7-gcc8-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_101swan/x86_64-centos7-gcc8-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/8.3.0-cebb0/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.30-e5b21/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/3.6.3-dfb24/x86_64-centos7-gcc8-opt/lib64/R/library/readr/rcon\n",
      "2022-07-08 18:12:25.369056: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'HyperRectangleFitter' from '/eos/home-k/khowey/SWAN_projects/ML4DQMDC-PixelAE/KH-AutoencoderTest/../src/cloudfitters/HyperRectangleFitter.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import importlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# local modules\n",
    "sys.path.append('../utils')\n",
    "import csv_utils as csvu\n",
    "import json_utils as jsonu\n",
    "import dataframe_utils as dfu\n",
    "import hist_utils as hu\n",
    "import autoencoder_utils as aeu\n",
    "import plot_utils as pu\n",
    "import generate_data_utils as gdu\n",
    "import refruns_utils as rru\n",
    "importlib.reload(csvu)\n",
    "importlib.reload(jsonu)\n",
    "importlib.reload(dfu)\n",
    "importlib.reload(hu)\n",
    "importlib.reload(aeu)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(gdu)\n",
    "importlib.reload(rru)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/classifiers')\n",
    "sys.path.append('../src/cloudfitters')\n",
    "import HistStruct\n",
    "importlib.reload(HistStruct)\n",
    "import SubHistStruct\n",
    "importlib.reload(SubHistStruct)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "import AutoEncoder\n",
    "importlib.reload(AutoEncoder)\n",
    "import SeminormalFitter\n",
    "import GaussianKdeFitter\n",
    "import HyperRectangleFitter\n",
    "importlib.reload(SeminormalFitter)\n",
    "importlib.reload(GaussianKdeFitter)\n",
    "importlib.reload(HyperRectangleFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9dbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speed Controls and Run Mode\n",
    "\n",
    "# Disables all plots for large datasets where speed is more important\n",
    "createPlots = False\n",
    "\n",
    "# Control for the notebook - turn off user-friendly mode to enable faster runtimes\n",
    "userfriendly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2020c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Parameters\n",
    "\n",
    "# Select the bias towards recall against precision, treated as a factor (so < 1 biases towards precision, 1 is equal importance, and > 1 biases towards recall)\n",
    "wpBiasFactor = 20\n",
    "fmBiasFactor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining bad runs\n",
    "badruns = {'2017B':\n",
    "                [\n",
    "                    297048,\n",
    "                    297282,\n",
    "                    297283,\n",
    "                    297284,\n",
    "                    297287,\n",
    "                    297288,\n",
    "                    297289,\n",
    "                    299316,\n",
    "                    299317,\n",
    "                    299318,\n",
    "                    299324,\n",
    "                    299326,\n",
    "                    301086,\n",
    "                    301086,\n",
    "                    303948,\n",
    "                    297047, #close but, true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297211, #Reconstructs well\n",
    "                    299325, #Reconstructs well\n",
    "                    297664, #true bad for all 8\n",
    "                    299317, #true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297502\n",
    "                ],\n",
    "             '2017C':[\n",
    "                  300781, # bad for tracking (pixels were excluded.\n",
    "                  300079, # is bad for strips and then also for tracking\n",
    "                  302029, # Poor detector elements for strips - Should be mildly anomalous, but technically good \n",
    "                  300576, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300574, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300282, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  301912, # Half bad for pixels (lost HV or readout card)  \n",
    "                  301086, # Half bad for pixels (lost HV or readout card)  \n",
    "                  300283, # Half bad for pixels (lost HV or readout card) \n",
    "                  300282, # Half bad for pixels (lost HV or readout card) \n",
    "                  300281, # Half bad for pixels (lost HV or readout card) \n",
    "                  300239, # Half bad for pixels (lost HV or readout card)\n",
    "                  301394, # Marginal for pixels\n",
    "                  301183, # Marginal for pixels\n",
    "                  300398, # Marginal for pixels\n",
    "                  300389, # Marginal for pixels\n",
    "                  300365  # Marginal for pixels\n",
    "             ],\n",
    "             '2017E':[\n",
    "                 304740, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304776, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304506, # Portcard problem for pixels\n",
    "                 304507, # Portcard problem for pixels \n",
    "                 303989, # Bad for pixels, power supply died\n",
    "                 303824  # Partly bad for strips due to a test\n",
    "             ],\n",
    "             '2017F':[\n",
    "                 306422, # Partly bad for strips - 2 data readouts failed \n",
    "                 306423, # Partly bad for strips - 2 data readouts failed\n",
    "                 306425, # Partly bad for strips - 2 data readouts failed\n",
    "                 305440, # Partly bad for strips - 1 data readout failed\n",
    "                 305441, # Partly bad for strips - 1 data readout failed\n",
    "                 305249, # Bad for pixels - half of disk failed \n",
    "                 305250, # Bad for pixels - half of disk failed\n",
    "                 305064, # Marginal for pixels - some readout failed\n",
    "             ],\n",
    "            '2018': # needs to be re-checked, not guaranteed to be fully correct or representative.\n",
    "                [\n",
    "                317479,\n",
    "                317480,\n",
    "                317481,\n",
    "                317482,\n",
    "                319847\n",
    "                ]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66ed68a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad run :{'run_number': 303824, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 303989, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 304740, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n"
     ]
    }
   ],
   "source": [
    "### Select a reference run and get data\n",
    "rundict = jsonu.loadjson('../jsons/CertHelperRefRuns.json')\n",
    "\n",
    "runNum = 303824\n",
    "runls = {}\n",
    "for run in rundict:\n",
    "    if run['run_number'] == runNum:\n",
    "        runls.update(run)\n",
    "if runls == {}:\n",
    "    raise Exception('Run not found!')\n",
    "\n",
    "year = runls['dataset'][11:15]\n",
    "era = runls['dataset'][15]\n",
    "ref_run = runls['reference_run_number']\n",
    "\n",
    "\n",
    "outputRuns = {}\n",
    "outputBad = {}\n",
    "for run in rundict:\n",
    "    tempRef = run['reference_run_number']\n",
    "    if tempRef == ref_run:\n",
    "        runls = {}\n",
    "        runls[str(run['run_number'])] = [[-1]]\n",
    "        if run['run_number'] in badruns[year+era]:\n",
    "            print('Found bad run :' + str(run))\n",
    "            outputBad.update(runls)\n",
    "        else:\n",
    "            outputRuns.update(runls)\n",
    "\n",
    "# Perform structuring for compatibility with autoencoders\n",
    "dataDict = {}\n",
    "dataDict[year + era] = outputRuns\n",
    "\n",
    "badrunsls = {}\n",
    "badrunsls[year + era] = outputBad\n",
    "\n",
    "trainrunsls = {}\n",
    "goodrunsls = {}\n",
    "trainrunsls[year + era] = {}\n",
    "goodrunsls[year + era] = {}\n",
    "for i,run in enumerate(dataDict[year + era]):\n",
    "    if i > 5 and i < 11:\n",
    "        goodrunsls[year + era][str(run)] = [[-1]]\n",
    "    else:\n",
    "        trainrunsls[year + era][str(run)] = [[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4590f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Controls and Selection - 1D Autoncoder\n",
    "\n",
    "# The directory data is located in\n",
    "datadir = '../data/'\n",
    "\n",
    "# Create a list of histograms to include\n",
    "# Pair histograms to be combined on the same line\n",
    "histnames = [\n",
    "    ['NormalizedHitResiduals_TIB__Layer__1', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__1', 'NormalizedHitResiduals_TIB__Layer__2', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2',\n",
    "     'NormalizedHitResiduals_TIB__Layer__3', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3' , 'NormalizedHitResiduals_TIB__Layer__4', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4'],\n",
    "    ['chargeInner_PXLayer_1', 'chargeOuter_PXLayer_1', 'adc_PXLayer_1', 'size_PXLayer_1'],\n",
    "#    ['chargeInner_PXLayer_2', 'chargeOuter_PXLayer_2', 'adc_PXLayer_2', 'size_PXLayer_2'],\n",
    "#    ['chargeInner_PXLayer_3', 'chargeOuter_PXLayer_3', 'adc_PXLayer_3', 'size_PXLayer_3'],\n",
    "#    ['chargeInner_PXLayer_4', 'chargeOuter_PXLayer_4', 'adc_PXLayer_4', 'size_PXLayer_4'],\n",
    "#    ['charge_PXDisk_+1', 'adc_PXDisk_+1'],\n",
    "#    ['charge_PXDisk_-1', 'adc_PXDisk_-1'],\n",
    "#    ['charge_PXDisk_+2', 'adc_PXDisk_+2'],\n",
    "#    ['charge_PXDisk_-2', 'adc_PXDisk_-2'],\n",
    "#    ['charge_PXDisk_+3', 'adc_PXDisk_+3'],\n",
    "#    ['charge_PXDisk_-3', 'adc_PXDisk_-3'],\n",
    "#    ['NormalizedHitResiduals_TOB__Layer__1', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__1', 'NormalizedHitResiduals_TOB__Layer__2', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__2',\n",
    "#     'NormalizedHitResiduals_TOB__Layer__3', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__3' , 'NormalizedHitResiduals_TOB__Layer__4', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__4']\n",
    "]\n",
    "\n",
    "# Read new data or use previously saved data & should data be saved\n",
    "readnew = True\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ffebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Run Properties - Combined Autoencoder\n",
    "# in this cell all major run properties are going to be set,\n",
    "\n",
    "# Set whether to train globally or locally or in a development/testing mode\n",
    "training_mode = 'development'\n",
    "\n",
    "# Selects whether to create a new histstruct or use a saved one\n",
    "readnew = True\n",
    "\n",
    "# Select whether to save a new histstruct\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6394238",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Controls and Selection - 1D Autoencoder\n",
    "\n",
    "plotNames = 'Test'\n",
    "name = plotNames+'plots'\n",
    "\n",
    "# Choose whether to train a new model or load one\n",
    "trainnew = True\n",
    "savemodel = True\n",
    "modelname = plotNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4051dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected runs/lumisections for training: \n",
      "{'303819': [[-1]], '303999': [[-1]], '304119': [[-1]], '304120': [[-1]], '304197': [[-1]], '304505': [[-1]], '304449': [[-1]], '304452': [[-1]], '304508': [[-1]], '304625': [[-1]], '304655': [[-1]], '304737': [[-1]], '304778': [[-1]], '306459': [[-1]], '304196': [[-1]]}\n",
      "selected runs/lumisections as good test set:\n",
      "{'304198': [[-1]], '304199': [[-1]], '304209': [[-1]], '304333': [[-1]], '304446': [[-1]]}\n",
      "selected runs/lumisections as bad test set:\n",
      "{'303824': [[-1]], '303989': [[-1]], '304740': [[-1]]}\n"
     ]
    }
   ],
   "source": [
    "### Define Training Mode Parameters - Combined Autoencoder\n",
    "if training_mode == 'global':\n",
    "    runsls_training = None # use none to not add a mask for training (can e.g. use DCS-bit on mask)\n",
    "    runsls_good = None # use none to not add a mask for good runs (can e.g. use averages of training set)\n",
    "    runsls_bad = badrunsls[year] # predefined bad runs\n",
    "    print('selected runs/lumisections for training: all')\n",
    "    \n",
    "elif training_mode == 'local':\n",
    "    # train locally on a small set of runs\n",
    "    # - either on n runs preceding a chosen application run,\n",
    "    # - or on the run associated as reference to the chosen application run.\n",
    "    # - this only works for a single era\n",
    "    \n",
    "    available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+era+'_'+histnames[0][0]+'.csv') ) )\n",
    "    # Cherry picked really bad run\n",
    "    run_application = 299316\n",
    "    #run_application = 299317\n",
    "    run_application_index = available_runs.index(run_application)\n",
    "    # select training set\n",
    "    usereference = False\n",
    "    if usereference:\n",
    "        run_reference = rru.get_reference_run( run_application, jsonfile='../utils/json_allRunsRefRuns.json' )\n",
    "        if run_reference<0:\n",
    "            raise Exception('no valid reference run has been defined for run {}'.format(run_application))\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(run_reference,[-1])])\n",
    "    else:\n",
    "        ntraining = 5\n",
    "        offset = 0 # normal case: offset = 0 (just use 5 previous runs)\n",
    "        \n",
    "        # Selects the 5 previous runs for training\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining-offset:run_application_index-offset]])\n",
    "    #runsls_bad = badrunsls[year]\n",
    "    #runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_bad = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_good = runsls_training\n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)\n",
    "        \n",
    "elif training_mode == 'development':\n",
    "    # train on a user-defined subset of runs\n",
    "    \n",
    "    # Select runs to be used in training from the user-defined list\n",
    "    runsls_training = trainrunsls[year + era]\n",
    "    # Select bad runs to test on in the user-defined list\n",
    "    runsls_bad = badrunsls[year + era]\n",
    "    # Select good runs to test on in the user-defined list\n",
    "    runsls_good = goodrunsls[year + era]\n",
    "    \n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54180f06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Classifiers and masks cleared to preserve consistency\n",
      "Adding NormalizedHitResiduals_TIB__Layer__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__4...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4...\n",
      "Adding chargeInner_PXLayer_1...\n",
      "Adding chargeOuter_PXLayer_1...\n",
      "Adding adc_PXLayer_1...\n",
      "Adding size_PXLayer_1...\n",
      "Found 9290 histograms\n",
      "Created a histstruct with the following properties:\n",
      "- number of histogram types: 12\n",
      "- number of lumisections: 9290\n"
     ]
    }
   ],
   "source": [
    "### Data Import\n",
    "\n",
    "# Create a new HistStruct from the data\n",
    "if readnew:\n",
    "    # Initializations\n",
    "    dloader = DataLoader.DataLoader()\n",
    "    histstruct = SubHistStruct.SubHistStruct()\n",
    "    histstruct.reset_histlist(histnames)\n",
    "    \n",
    "    # Unpack histnames and add every histogram individually\n",
    "    for histnamegroup in histnames:\n",
    "        for histname in histnamegroup:\n",
    "            print('Adding {}...'.format(histname))\n",
    "            \n",
    "            # Bring the histograms into memory from storage for later use\n",
    "            filename = datadir + year + era + '/DF' + year + era + '_' + histname + '.csv'\n",
    "            df = dloader.get_dataframe_from_file( filename )\n",
    "            \n",
    "            # In case of local training, we can remove most of the histograms\n",
    "            if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ):\n",
    "                runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()}\n",
    "                df = dfu.select_runsls( df, runsls_total )    \n",
    "                \n",
    "            # Store the data in the histstruct object managing this whole thing\n",
    "            histstruct.add_dataframe( df, rebinningfactor = 1, standardbincount = 102 )\n",
    "    print('Found {} histograms'.format(len(histstruct.runnbs)))\n",
    "\n",
    "# Load a previously saved HistStruct\n",
    "else:\n",
    "    # Load histstruct from storage\n",
    "    histstruct = SubHistStruct.SubHistStruct.load( 'histstruct_global_20220201.zip', verbose=False )\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if ('bad' in name and name!='bad')])\n",
    "    \n",
    "    print('loaded a histstruct with the following properties:')\n",
    "    print(histstruct)\n",
    "    # Count of bad runs, presumably for later use\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name])\n",
    "    \n",
    "if userfriendly:\n",
    "    print('Created a histstruct with the following properties:')\n",
    "    print('- number of histogram types: {}'.format(len(histstruct.histnames)))\n",
    "    print('- number of lumisections: {}'.format(len(histstruct.lsnbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45931fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'303824': [[-1]], '303989': [[-1]], '304740': [[-1]]}\n",
      "Assigned masks: ['dcson', 'golden', 'highstat', 'lowstat', 'training', 'good', 'bad', 'bad0', 'bad1', 'bad2']\n"
     ]
    }
   ],
   "source": [
    "### Add Masks to Data\n",
    "\n",
    "if readnew:\n",
    "    histstruct.add_dcsonjson_mask( 'dcson' )\n",
    "    histstruct.add_goldenjson_mask('golden' )\n",
    "    histstruct.add_highstat_mask( 'highstat' )\n",
    "    histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=100 )\n",
    "    if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training )\n",
    "    if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good )\n",
    "        \n",
    "    # Distinguishing bad runs\n",
    "    nbadruns = 0\n",
    "    if runsls_bad is not None:\n",
    "        print(runsls_bad)\n",
    "        histstruct.add_json_mask( 'bad', runsls_bad )\n",
    "        \n",
    "        # Special case for bad runs: add a mask per run (different bad runs have different characteristics)\n",
    "        nbadruns = len(runsls_bad.keys())\n",
    "        for i,badrun in enumerate(runsls_bad.keys()):\n",
    "            histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} )\n",
    "            \n",
    "    if save:\n",
    "        histstruct.save('test.pk1')\n",
    "if userfriendly: print('Assigned masks: {}'.format(list(histstruct.masks.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30d6a8ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting the input data for analysis\n",
    "\n",
    "if((training_mode=='local' or training_mode == 'development') and createPlots):\n",
    "\n",
    "    # training and application runs\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','training'],['dcson','highstat','good']],\n",
    "                                labellist = ['training','testing'],\n",
    "                                colorlist = ['blue','green']\n",
    "                              )\n",
    "    \n",
    "    # application run and bad test runs\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad0']],\n",
    "                                labellist = ['good','bad'],\n",
    "                                colorlist = ['green','red']\n",
    "                              )\n",
    "    \n",
    "elif( training_mode=='global' and createPlots):\n",
    "    \n",
    "    # bad test runs\n",
    "    for i in [0,1,2,3,4,5,6]:\n",
    "        histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad{}'.format(i)]],\n",
    "                                labellist = ['typical good histograms','bad'],\n",
    "                                colorlist = ['blue','red'],\n",
    "                                transparencylist = [0.01,1.]\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dc93811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_concatamash_autoencoder(histstruct):\n",
    "    \n",
    "    histslist = []\n",
    "    vallist = []\n",
    "    autoencoders = []\n",
    "    if trainnew:\n",
    "        for i,histnamegroup in enumerate(histnames):\n",
    "            \n",
    "            train_normhist = np.array([hu.normalize(histstruct.get_histograms(\n",
    "                histname = hname, masknames = ['dcson','highstat', 'training']), \n",
    "                                                 norm=\"l1\", axis=1) \n",
    "                                       for hname in histnamegroup]).transpose((1,0,2))\n",
    "            X_train, X_val = train_test_split(train_normhist, test_size=0.4, random_state=42)\n",
    "            \n",
    "            if userfriendly:\n",
    "                print('\\nNow Defining model {}/'.format(i + 1) \n",
    "                      + str(len(histnames)))\n",
    "                print(' - Size of training set: {}'.format(X_train.shape))\n",
    "            \n",
    "            # Half the total bin count\n",
    "            arch = 51 * len(histnamegroup)\n",
    "            \n",
    "            ## Model parameters\n",
    "            print(X_train.shape)\n",
    "            \n",
    "            input_dim = X_train.shape[2] #num of predictor variables\n",
    "            Input_layers=[Input(shape=input_dim) for i in range((X_train.shape[1]))]\n",
    "            \n",
    "            # Defining layers\n",
    "            conc_layer = Concatenate()(Input_layers)\n",
    "            encoder = Dense(arch * 2, activation=\"tanh\")(conc_layer)\n",
    "            #encoder = Dense(128, activation='relu')(encoder)\n",
    "            #\n",
    "            #encoder = Dense(32, activation='relu')(encoder)\n",
    "            \n",
    "            decoder = Dense(arch, activation=\"relu\")(encoder)\n",
    "            #decoder = Dense(256, activation=\"tanh\")(decoder)\n",
    "            \n",
    "            Output_layers=[Dense(input_dim, activation=\"tanh\")(decoder) for i in range(X_train.shape[1])]\n",
    "\n",
    "            autoencoder = Model(inputs=Input_layers, outputs=Output_layers)\n",
    "            autoencoder.summary()\n",
    "            autoencoders.append(autoencoder)\n",
    "            \n",
    "            histslist.append(X_train)\n",
    "            vallist.append(X_val)\n",
    "     \n",
    "    # Return the histograms stored 2-Dimensionally and the autoencoders corresponding\n",
    "    return(histslist, vallist, autoencoders, train_normhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c76ea4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now Defining model 1/2\n",
      " - Size of training set: (2926, 8, 102)\n",
      "(2926, 8, 102)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 816)          0           input_13[0][0]                   \n",
      "                                                                 input_14[0][0]                   \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "                                                                 input_18[0][0]                   \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 816)          666672      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 408)          333336      dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 102)          41718       dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,333,752\n",
      "Trainable params: 1,333,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Now Defining model 2/2\n",
      " - Size of training set: (2926, 4, 102)\n",
      "(2926, 4, 102)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_23 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           [(None, 102)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 408)          0           input_21[0][0]                   \n",
      "                                                                 input_22[0][0]                   \n",
      "                                                                 input_23[0][0]                   \n",
      "                                                                 input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 408)          166872      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 204)          83436       dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 102)          20910       dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 102)          20910       dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 102)          20910       dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 102)          20910       dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 333,948\n",
      "Trainable params: 333,948\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "(histslist, vallist, autoencoders, train_normhist) = define_concatamash_autoencoder(histstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a44e038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trains a combined autoencoder for every merge set\n",
    "def train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders):\n",
    "    \n",
    "    # Iterate through the training data to train corresponding autoencoders\n",
    "    for i in range(len(histslist)):\n",
    "        \n",
    "        if userfriendly: print('\\nNow training model {}/'.format(i + 1) + str(len(histslist)))\n",
    "        \n",
    "        # Set variables to temporary values for better transparency\n",
    "        X_train = histslist[i]\n",
    "        X_val = vallist[i]\n",
    "        autoencoder = autoencoders[i]\n",
    "        \n",
    "        \n",
    "        ## Model parameters\n",
    "        nb_epoch = 500\n",
    "        batch_size = 500\n",
    "        \n",
    "        #checkpoint_filepath = 'checkpoint'\n",
    "        #model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #   filepath=checkpoint_filepath,\n",
    "        #   save_weights_only=False,\n",
    "        #   verbose=1,\n",
    "        #   save_best_only=True,\n",
    "        #   monitor='val_loss',\n",
    "        #   mode='min')\n",
    "        \n",
    "        # Tell the model when to stop\n",
    "        earlystop = EarlyStopping(monitor='val_loss',\n",
    "            min_delta=1e-7,\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        lr =0.001\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        autoencoder.compile(loss='mse',\n",
    "                            optimizer=opt)\n",
    "        \n",
    "        ## Train autoencoder\n",
    "        train = autoencoder.fit(x=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                                y=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_data=([X_val[:,i] for i in range(X_val.shape[1])], [X_val[:,i] for i in range(X_val.shape[1])]),\n",
    "                            verbose=1,\n",
    "                            callbacks= [earlystop],    \n",
    "                            )\n",
    "        \n",
    "        # Create a plot of the model\n",
    "        \n",
    "        tf.keras.utils.plot_model(\n",
    "            autoencoder,\n",
    "            to_file=\"models/modelConcatamash{}.png\".format(i),\n",
    "            show_shapes=True,\n",
    "            show_dtype=False,\n",
    "            show_layer_names=False,\n",
    "            rankdir=\"TB\")\n",
    "        \n",
    "        # Save classifier for evaluation\n",
    "        classifier = AutoEncoder.AutoEncoder(model=autoencoder)\n",
    "        histstruct.add_classifier(histnames[i][0], classifier)\n",
    "        if savemodel:\n",
    "            autoencoder.save('../SavedModels/Concatamash' + '/AE' + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52397fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now training model 1/2\n",
      "Epoch 1/500\n",
      "6/6 [==============================] - 4s 192ms/step - loss: 0.0029 - dense_18_loss: 2.6497e-04 - dense_19_loss: 3.8351e-04 - dense_20_loss: 2.9741e-04 - dense_21_loss: 4.4036e-04 - dense_22_loss: 3.4179e-04 - dense_23_loss: 4.0342e-04 - dense_24_loss: 3.1901e-04 - dense_25_loss: 4.1125e-04 - val_loss: 0.0014 - val_dense_18_loss: 1.1608e-04 - val_dense_19_loss: 1.8701e-04 - val_dense_20_loss: 1.1728e-04 - val_dense_21_loss: 2.2729e-04 - val_dense_22_loss: 1.5573e-04 - val_dense_23_loss: 2.2247e-04 - val_dense_24_loss: 1.3995e-04 - val_dense_25_loss: 2.1350e-04\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 8.9022e-04 - dense_18_loss: 7.0477e-05 - dense_19_loss: 1.2682e-04 - dense_20_loss: 7.8816e-05 - dense_21_loss: 1.4315e-04 - dense_22_loss: 1.0668e-04 - dense_23_loss: 1.3036e-04 - dense_24_loss: 8.8338e-05 - dense_25_loss: 1.4558e-04 - val_loss: 3.4678e-04 - val_dense_18_loss: 2.3184e-05 - val_dense_19_loss: 5.2254e-05 - val_dense_20_loss: 3.1989e-05 - val_dense_21_loss: 5.9865e-05 - val_dense_22_loss: 3.5194e-05 - val_dense_23_loss: 5.6487e-05 - val_dense_24_loss: 2.7928e-05 - val_dense_25_loss: 5.9875e-05\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 2.3820e-04 - dense_18_loss: 1.7976e-05 - dense_19_loss: 3.5688e-05 - dense_20_loss: 2.1077e-05 - dense_21_loss: 3.9732e-05 - dense_22_loss: 2.3922e-05 - dense_23_loss: 3.7607e-05 - dense_24_loss: 1.9955e-05 - dense_25_loss: 4.2243e-05 - val_loss: 1.6699e-04 - val_dense_18_loss: 1.9539e-05 - val_dense_19_loss: 2.0709e-05 - val_dense_20_loss: 2.0364e-05 - val_dense_21_loss: 2.2508e-05 - val_dense_22_loss: 2.0703e-05 - val_dense_23_loss: 2.1412e-05 - val_dense_24_loss: 1.8986e-05 - val_dense_25_loss: 2.2769e-05\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 1.3666e-04 - dense_18_loss: 1.5346e-05 - dense_19_loss: 1.7843e-05 - dense_20_loss: 1.6719e-05 - dense_21_loss: 1.7809e-05 - dense_22_loss: 1.7421e-05 - dense_23_loss: 1.6548e-05 - dense_24_loss: 1.6910e-05 - dense_25_loss: 1.8061e-05 - val_loss: 8.8245e-05 - val_dense_18_loss: 1.1324e-05 - val_dense_19_loss: 1.1597e-05 - val_dense_20_loss: 1.3897e-05 - val_dense_21_loss: 9.2340e-06 - val_dense_22_loss: 1.2185e-05 - val_dense_23_loss: 8.7454e-06 - val_dense_24_loss: 1.2231e-05 - val_dense_25_loss: 9.0319e-06\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8947e-05 - dense_18_loss: 8.2203e-06 - dense_19_loss: 8.6544e-06 - dense_20_loss: 1.0866e-05 - dense_21_loss: 7.0217e-06 - dense_22_loss: 9.5765e-06 - dense_23_loss: 8.3636e-06 - dense_24_loss: 8.8959e-06 - dense_25_loss: 7.3486e-06 - val_loss: 4.5507e-05 - val_dense_18_loss: 5.4956e-06 - val_dense_19_loss: 6.8315e-06 - val_dense_20_loss: 5.2152e-06 - val_dense_21_loss: 5.2313e-06 - val_dense_22_loss: 6.1202e-06 - val_dense_23_loss: 5.5715e-06 - val_dense_24_loss: 4.6255e-06 - val_dense_25_loss: 6.4164e-06\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 3.6620e-05 - dense_18_loss: 4.2310e-06 - dense_19_loss: 5.5666e-06 - dense_20_loss: 4.1524e-06 - dense_21_loss: 4.6591e-06 - dense_22_loss: 4.8247e-06 - dense_23_loss: 4.3013e-06 - dense_24_loss: 3.5451e-06 - dense_25_loss: 5.3399e-06 - val_loss: 2.5286e-05 - val_dense_18_loss: 2.9501e-06 - val_dense_19_loss: 4.2140e-06 - val_dense_20_loss: 2.6438e-06 - val_dense_21_loss: 3.6118e-06 - val_dense_22_loss: 3.1545e-06 - val_dense_23_loss: 2.7645e-06 - val_dense_24_loss: 2.9127e-06 - val_dense_25_loss: 3.0345e-06\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 2.1456e-05 - dense_18_loss: 2.4557e-06 - dense_19_loss: 3.3179e-06 - dense_20_loss: 2.3088e-06 - dense_21_loss: 2.6088e-06 - dense_22_loss: 2.8777e-06 - dense_23_loss: 2.5408e-06 - dense_24_loss: 2.5445e-06 - dense_25_loss: 2.8015e-06 - val_loss: 1.5847e-05 - val_dense_18_loss: 1.5745e-06 - val_dense_19_loss: 2.6218e-06 - val_dense_20_loss: 1.5977e-06 - val_dense_21_loss: 1.8680e-06 - val_dense_22_loss: 2.3966e-06 - val_dense_23_loss: 2.0356e-06 - val_dense_24_loss: 2.1826e-06 - val_dense_25_loss: 1.5703e-06\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 1.2654e-05 - dense_18_loss: 1.2689e-06 - dense_19_loss: 2.2000e-06 - dense_20_loss: 1.4156e-06 - dense_21_loss: 1.4377e-06 - dense_22_loss: 1.7074e-06 - dense_23_loss: 1.7246e-06 - dense_24_loss: 1.6564e-06 - dense_25_loss: 1.2433e-06 - val_loss: 8.8253e-06 - val_dense_18_loss: 8.9505e-07 - val_dense_19_loss: 1.7024e-06 - val_dense_20_loss: 9.4931e-07 - val_dense_21_loss: 9.7981e-07 - val_dense_22_loss: 9.8414e-07 - val_dense_23_loss: 1.2666e-06 - val_dense_24_loss: 1.0464e-06 - val_dense_25_loss: 1.0016e-06\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 7.8330e-06 - dense_18_loss: 8.3562e-07 - dense_19_loss: 1.5660e-06 - dense_20_loss: 8.2543e-07 - dense_21_loss: 1.0030e-06 - dense_22_loss: 7.8221e-07 - dense_23_loss: 1.0493e-06 - dense_24_loss: 8.0231e-07 - dense_25_loss: 9.6924e-07 - val_loss: 6.3571e-06 - val_dense_18_loss: 5.3293e-07 - val_dense_19_loss: 1.3949e-06 - val_dense_20_loss: 6.3657e-07 - val_dense_21_loss: 9.5444e-07 - val_dense_22_loss: 6.2973e-07 - val_dense_23_loss: 8.1661e-07 - val_dense_24_loss: 6.1750e-07 - val_dense_25_loss: 7.7446e-07\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.5951e-06 - dense_18_loss: 4.8582e-07 - dense_19_loss: 1.3174e-06 - dense_20_loss: 5.2440e-07 - dense_21_loss: 7.5846e-07 - dense_22_loss: 5.4701e-07 - dense_23_loss: 7.2343e-07 - dense_24_loss: 5.2057e-07 - dense_25_loss: 7.1804e-07 - val_loss: 4.7356e-06 - val_dense_18_loss: 3.8402e-07 - val_dense_19_loss: 1.1972e-06 - val_dense_20_loss: 3.8424e-07 - val_dense_21_loss: 6.5543e-07 - val_dense_22_loss: 5.0292e-07 - val_dense_23_loss: 6.4092e-07 - val_dense_24_loss: 4.4471e-07 - val_dense_25_loss: 5.2617e-07\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 4.3711e-06 - dense_18_loss: 3.2256e-07 - dense_19_loss: 1.0929e-06 - dense_20_loss: 3.5052e-07 - dense_21_loss: 5.9950e-07 - dense_22_loss: 4.5551e-07 - dense_23_loss: 6.0240e-07 - dense_24_loss: 4.3371e-07 - dense_25_loss: 5.1392e-07 - val_loss: 3.6176e-06 - val_dense_18_loss: 2.4868e-07 - val_dense_19_loss: 9.5211e-07 - val_dense_20_loss: 2.6868e-07 - val_dense_21_loss: 5.1726e-07 - val_dense_22_loss: 3.4946e-07 - val_dense_23_loss: 5.2379e-07 - val_dense_24_loss: 3.3225e-07 - val_dense_25_loss: 4.2542e-07\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 3.6075e-06 - dense_18_loss: 2.4191e-07 - dense_19_loss: 9.6610e-07 - dense_20_loss: 2.7485e-07 - dense_21_loss: 5.0426e-07 - dense_22_loss: 3.3048e-07 - dense_23_loss: 5.1395e-07 - dense_24_loss: 3.3617e-07 - dense_25_loss: 4.3977e-07 - val_loss: 3.2743e-06 - val_dense_18_loss: 2.3449e-07 - val_dense_19_loss: 9.0070e-07 - val_dense_20_loss: 2.4184e-07 - val_dense_21_loss: 4.7189e-07 - val_dense_22_loss: 2.8784e-07 - val_dense_23_loss: 4.5891e-07 - val_dense_24_loss: 2.8985e-07 - val_dense_25_loss: 3.8875e-07\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 3.1756e-06 - dense_18_loss: 2.0564e-07 - dense_19_loss: 9.0891e-07 - dense_20_loss: 2.2530e-07 - dense_21_loss: 4.4829e-07 - dense_22_loss: 2.6930e-07 - dense_23_loss: 4.5823e-07 - dense_24_loss: 2.7951e-07 - dense_25_loss: 3.8040e-07 - val_loss: 2.9420e-06 - val_dense_18_loss: 1.8474e-07 - val_dense_19_loss: 8.5119e-07 - val_dense_20_loss: 1.9188e-07 - val_dense_21_loss: 4.2211e-07 - val_dense_22_loss: 2.5123e-07 - val_dense_23_loss: 4.3141e-07 - val_dense_24_loss: 2.5821e-07 - val_dense_25_loss: 3.5119e-07\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 2.9543e-06 - dense_18_loss: 1.7143e-07 - dense_19_loss: 8.6987e-07 - dense_20_loss: 1.9818e-07 - dense_21_loss: 4.2932e-07 - dense_22_loss: 2.4929e-07 - dense_23_loss: 4.2943e-07 - dense_24_loss: 2.4893e-07 - dense_25_loss: 3.5784e-07 - val_loss: 2.9926e-06 - val_dense_18_loss: 1.7617e-07 - val_dense_19_loss: 8.4476e-07 - val_dense_20_loss: 1.9993e-07 - val_dense_21_loss: 4.2215e-07 - val_dense_22_loss: 2.6435e-07 - val_dense_23_loss: 4.3835e-07 - val_dense_24_loss: 2.6562e-07 - val_dense_25_loss: 3.8130e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 3.4153e-06 - dense_18_loss: 2.3419e-07 - dense_19_loss: 9.1941e-07 - dense_20_loss: 2.4498e-07 - dense_21_loss: 4.8452e-07 - dense_22_loss: 3.3033e-07 - dense_23_loss: 4.7224e-07 - dense_24_loss: 2.9140e-07 - dense_25_loss: 4.3826e-07 - val_loss: 4.5035e-06 - val_dense_18_loss: 4.2493e-07 - val_dense_19_loss: 1.0554e-06 - val_dense_20_loss: 3.2778e-07 - val_dense_21_loss: 6.1125e-07 - val_dense_22_loss: 4.9178e-07 - val_dense_23_loss: 5.8087e-07 - val_dense_24_loss: 4.4543e-07 - val_dense_25_loss: 5.6602e-07\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 4.4775e-06 - dense_18_loss: 4.3395e-07 - dense_19_loss: 1.0664e-06 - dense_20_loss: 3.4337e-07 - dense_21_loss: 5.9211e-07 - dense_22_loss: 4.5994e-07 - dense_23_loss: 5.7695e-07 - dense_24_loss: 4.5448e-07 - dense_25_loss: 5.5036e-07 - val_loss: 2.8661e-06 - val_dense_18_loss: 1.7613e-07 - val_dense_19_loss: 8.1994e-07 - val_dense_20_loss: 2.0834e-07 - val_dense_21_loss: 4.0345e-07 - val_dense_22_loss: 2.4581e-07 - val_dense_23_loss: 4.1010e-07 - val_dense_24_loss: 2.5126e-07 - val_dense_25_loss: 3.5108e-07\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 3.2966e-06 - dense_18_loss: 2.3265e-07 - dense_19_loss: 9.0446e-07 - dense_20_loss: 2.4426e-07 - dense_21_loss: 4.6623e-07 - dense_22_loss: 3.0824e-07 - dense_23_loss: 4.5801e-07 - dense_24_loss: 2.8520e-07 - dense_25_loss: 3.9755e-07 - val_loss: 3.5554e-06 - val_dense_18_loss: 2.5767e-07 - val_dense_19_loss: 9.0957e-07 - val_dense_20_loss: 2.8863e-07 - val_dense_21_loss: 5.2536e-07 - val_dense_22_loss: 3.4079e-07 - val_dense_23_loss: 4.9009e-07 - val_dense_24_loss: 2.8136e-07 - val_dense_25_loss: 4.6189e-07\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 3.0211e-06 - dense_18_loss: 1.8664e-07 - dense_19_loss: 8.5718e-07 - dense_20_loss: 2.0951e-07 - dense_21_loss: 4.3343e-07 - dense_22_loss: 2.6828e-07 - dense_23_loss: 4.3126e-07 - dense_24_loss: 2.5632e-07 - dense_25_loss: 3.7843e-07 - val_loss: 2.7335e-06 - val_dense_18_loss: 1.5943e-07 - val_dense_19_loss: 7.8889e-07 - val_dense_20_loss: 1.7730e-07 - val_dense_21_loss: 3.9546e-07 - val_dense_22_loss: 2.4166e-07 - val_dense_23_loss: 3.9383e-07 - val_dense_24_loss: 2.4018e-07 - val_dense_25_loss: 3.3676e-07\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 2.9968e-06 - dense_18_loss: 1.8882e-07 - dense_19_loss: 8.5321e-07 - dense_20_loss: 1.9132e-07 - dense_21_loss: 4.2069e-07 - dense_22_loss: 2.7036e-07 - dense_23_loss: 4.2332e-07 - dense_24_loss: 2.7581e-07 - dense_25_loss: 3.7330e-07 - val_loss: 3.0420e-06 - val_dense_18_loss: 2.2153e-07 - val_dense_19_loss: 8.3873e-07 - val_dense_20_loss: 1.9646e-07 - val_dense_21_loss: 4.2886e-07 - val_dense_22_loss: 2.8798e-07 - val_dense_23_loss: 4.3384e-07 - val_dense_24_loss: 2.6225e-07 - val_dense_25_loss: 3.7236e-07\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 3.0443e-06 - dense_18_loss: 2.0292e-07 - dense_19_loss: 8.6244e-07 - dense_20_loss: 1.9689e-07 - dense_21_loss: 4.3819e-07 - dense_22_loss: 2.8788e-07 - dense_23_loss: 4.3224e-07 - dense_24_loss: 2.4889e-07 - dense_25_loss: 3.7488e-07 - val_loss: 2.8865e-06 - val_dense_18_loss: 1.8103e-07 - val_dense_19_loss: 8.2308e-07 - val_dense_20_loss: 1.9701e-07 - val_dense_21_loss: 4.1816e-07 - val_dense_22_loss: 2.7975e-07 - val_dense_23_loss: 3.9720e-07 - val_dense_24_loss: 2.4767e-07 - val_dense_25_loss: 3.4255e-07\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 3.0837e-06 - dense_18_loss: 2.1809e-07 - dense_19_loss: 8.5433e-07 - dense_20_loss: 2.1411e-07 - dense_21_loss: 4.3130e-07 - dense_22_loss: 2.9051e-07 - dense_23_loss: 4.3033e-07 - dense_24_loss: 2.8483e-07 - dense_25_loss: 3.6019e-07 - val_loss: 3.6993e-06 - val_dense_18_loss: 3.3146e-07 - val_dense_19_loss: 8.9194e-07 - val_dense_20_loss: 2.9974e-07 - val_dense_21_loss: 4.9175e-07 - val_dense_22_loss: 3.8503e-07 - val_dense_23_loss: 5.0153e-07 - val_dense_24_loss: 3.8161e-07 - val_dense_25_loss: 4.1627e-07\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 6.2542e-06 - dense_18_loss: 7.1375e-07 - dense_19_loss: 1.2104e-06 - dense_20_loss: 5.8254e-07 - dense_21_loss: 8.3463e-07 - dense_22_loss: 7.3221e-07 - dense_23_loss: 7.7508e-07 - dense_24_loss: 6.7300e-07 - dense_25_loss: 7.3262e-07 - val_loss: 8.4646e-06 - val_dense_18_loss: 1.0930e-06 - val_dense_19_loss: 1.4690e-06 - val_dense_20_loss: 7.6538e-07 - val_dense_21_loss: 1.1006e-06 - val_dense_22_loss: 1.0426e-06 - val_dense_23_loss: 1.0284e-06 - val_dense_24_loss: 9.7401e-07 - val_dense_25_loss: 9.9163e-07\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 5.5163e-06 - dense_18_loss: 5.8203e-07 - dense_19_loss: 1.1562e-06 - dense_20_loss: 4.7122e-07 - dense_21_loss: 7.2730e-07 - dense_22_loss: 6.2335e-07 - dense_23_loss: 7.0067e-07 - dense_24_loss: 6.0535e-07 - dense_25_loss: 6.5013e-07 - val_loss: 5.5467e-06 - val_dense_18_loss: 5.7771e-07 - val_dense_19_loss: 1.0455e-06 - val_dense_20_loss: 5.5392e-07 - val_dense_21_loss: 6.4987e-07 - val_dense_22_loss: 6.2896e-07 - val_dense_23_loss: 7.0020e-07 - val_dense_24_loss: 7.5154e-07 - val_dense_25_loss: 6.3898e-07\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 3.9428e-06 - dense_18_loss: 3.4109e-07 - dense_19_loss: 9.0327e-07 - dense_20_loss: 3.4078e-07 - dense_21_loss: 4.9149e-07 - dense_22_loss: 4.0050e-07 - dense_23_loss: 5.2296e-07 - dense_24_loss: 4.7231e-07 - dense_25_loss: 4.7036e-07 - val_loss: 4.0498e-06 - val_dense_18_loss: 3.7274e-07 - val_dense_19_loss: 8.8193e-07 - val_dense_20_loss: 3.5680e-07 - val_dense_21_loss: 4.8985e-07 - val_dense_22_loss: 4.2443e-07 - val_dense_23_loss: 5.3069e-07 - val_dense_24_loss: 5.2296e-07 - val_dense_25_loss: 4.7036e-07\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 3.3569e-06 - dense_18_loss: 2.5469e-07 - dense_19_loss: 8.4484e-07 - dense_20_loss: 2.5685e-07 - dense_21_loss: 4.3566e-07 - dense_22_loss: 3.2889e-07 - dense_23_loss: 4.5961e-07 - dense_24_loss: 3.7047e-07 - dense_25_loss: 4.0586e-07 - val_loss: 3.3729e-06 - val_dense_18_loss: 2.7354e-07 - val_dense_19_loss: 8.2947e-07 - val_dense_20_loss: 2.6651e-07 - val_dense_21_loss: 4.4016e-07 - val_dense_22_loss: 3.1795e-07 - val_dense_23_loss: 4.6599e-07 - val_dense_24_loss: 3.7886e-07 - val_dense_25_loss: 4.0040e-07\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 2.9303e-06 - dense_18_loss: 1.9653e-07 - dense_19_loss: 8.0088e-07 - dense_20_loss: 2.0618e-07 - dense_21_loss: 3.9779e-07 - dense_22_loss: 2.6573e-07 - dense_23_loss: 4.1845e-07 - dense_24_loss: 2.8942e-07 - dense_25_loss: 3.5531e-07 - val_loss: 2.8161e-06 - val_dense_18_loss: 1.8515e-07 - val_dense_19_loss: 7.5899e-07 - val_dense_20_loss: 2.0910e-07 - val_dense_21_loss: 3.7195e-07 - val_dense_22_loss: 2.6597e-07 - val_dense_23_loss: 3.9926e-07 - val_dense_24_loss: 2.8943e-07 - val_dense_25_loss: 3.3622e-07\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 2.7065e-06 - dense_18_loss: 1.5487e-07 - dense_19_loss: 7.7519e-07 - dense_20_loss: 1.8686e-07 - dense_21_loss: 3.7195e-07 - dense_22_loss: 2.4494e-07 - dense_23_loss: 3.8994e-07 - dense_24_loss: 2.5881e-07 - dense_25_loss: 3.2397e-07 - val_loss: 2.6461e-06 - val_dense_18_loss: 1.5165e-07 - val_dense_19_loss: 7.5700e-07 - val_dense_20_loss: 1.7366e-07 - val_dense_21_loss: 3.5989e-07 - val_dense_22_loss: 2.4481e-07 - val_dense_23_loss: 3.7795e-07 - val_dense_24_loss: 2.6089e-07 - val_dense_25_loss: 3.2029e-07\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 3.3252e-06 - dense_18_loss: 2.2278e-07 - dense_19_loss: 8.7861e-07 - dense_20_loss: 2.5475e-07 - dense_21_loss: 4.4892e-07 - dense_22_loss: 3.4430e-07 - dense_23_loss: 4.5476e-07 - dense_24_loss: 3.2799e-07 - dense_25_loss: 3.9307e-07 - val_loss: 4.7247e-06 - val_dense_18_loss: 3.7677e-07 - val_dense_19_loss: 1.1125e-06 - val_dense_20_loss: 4.4343e-07 - val_dense_21_loss: 6.2445e-07 - val_dense_22_loss: 5.5817e-07 - val_dense_23_loss: 6.1242e-07 - val_dense_24_loss: 4.8932e-07 - val_dense_25_loss: 5.0766e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 5.4765e-06 - dense_18_loss: 4.9184e-07 - dense_19_loss: 1.2779e-06 - dense_20_loss: 5.0371e-07 - dense_21_loss: 7.3729e-07 - dense_22_loss: 6.4041e-07 - dense_23_loss: 7.0378e-07 - dense_24_loss: 5.2918e-07 - dense_25_loss: 5.9240e-07 - val_loss: 6.0916e-06 - val_dense_18_loss: 7.1156e-07 - val_dense_19_loss: 1.2910e-06 - val_dense_20_loss: 4.7242e-07 - val_dense_21_loss: 8.3678e-07 - val_dense_22_loss: 6.6483e-07 - val_dense_23_loss: 8.0590e-07 - val_dense_24_loss: 5.4252e-07 - val_dense_25_loss: 7.6663e-07\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 9.2106e-06 - dense_18_loss: 1.1155e-06 - dense_19_loss: 1.6444e-06 - dense_20_loss: 8.4064e-07 - dense_21_loss: 1.2009e-06 - dense_22_loss: 1.1192e-06 - dense_23_loss: 1.0885e-06 - dense_24_loss: 9.1668e-07 - dense_25_loss: 1.2847e-06 - val_loss: 7.9905e-06 - val_dense_18_loss: 7.3990e-07 - val_dense_19_loss: 1.4155e-06 - val_dense_20_loss: 8.2213e-07 - val_dense_21_loss: 1.0445e-06 - val_dense_22_loss: 1.1218e-06 - val_dense_23_loss: 9.1296e-07 - val_dense_24_loss: 8.3378e-07 - val_dense_25_loss: 1.0999e-06\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 6.0187e-06 - dense_18_loss: 6.0818e-07 - dense_19_loss: 1.2057e-06 - dense_20_loss: 5.3397e-07 - dense_21_loss: 7.8205e-07 - dense_22_loss: 6.9636e-07 - dense_23_loss: 7.6403e-07 - dense_24_loss: 6.4877e-07 - dense_25_loss: 7.7962e-07 - val_loss: 3.5612e-06 - val_dense_18_loss: 2.8167e-07 - val_dense_19_loss: 8.3949e-07 - val_dense_20_loss: 2.8285e-07 - val_dense_21_loss: 4.7603e-07 - val_dense_22_loss: 3.9931e-07 - val_dense_23_loss: 4.8071e-07 - val_dense_24_loss: 3.3506e-07 - val_dense_25_loss: 4.6605e-07\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 4.0869e-06 - dense_18_loss: 3.7577e-07 - dense_19_loss: 9.5098e-07 - dense_20_loss: 3.0545e-07 - dense_21_loss: 5.3607e-07 - dense_22_loss: 4.1148e-07 - dense_23_loss: 5.6405e-07 - dense_24_loss: 4.2919e-07 - dense_25_loss: 5.1389e-07 - val_loss: 2.6123e-06 - val_dense_18_loss: 1.5747e-07 - val_dense_19_loss: 7.4430e-07 - val_dense_20_loss: 1.6718e-07 - val_dense_21_loss: 3.4863e-07 - val_dense_22_loss: 2.5366e-07 - val_dense_23_loss: 3.7696e-07 - val_dense_24_loss: 2.4882e-07 - val_dense_25_loss: 3.1525e-07\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 3.2983e-06 - dense_18_loss: 2.5440e-07 - dense_19_loss: 8.4646e-07 - dense_20_loss: 2.1554e-07 - dense_21_loss: 4.4136e-07 - dense_22_loss: 3.0841e-07 - dense_23_loss: 4.8108e-07 - dense_24_loss: 3.3925e-07 - dense_25_loss: 4.1180e-07 - val_loss: 2.8044e-06 - val_dense_18_loss: 1.7984e-07 - val_dense_19_loss: 7.7486e-07 - val_dense_20_loss: 1.8544e-07 - val_dense_21_loss: 3.8052e-07 - val_dense_22_loss: 2.7636e-07 - val_dense_23_loss: 3.9468e-07 - val_dense_24_loss: 2.6578e-07 - val_dense_25_loss: 3.4691e-07\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 2.8392e-06 - dense_18_loss: 1.8561e-07 - dense_19_loss: 7.8648e-07 - dense_20_loss: 1.8310e-07 - dense_21_loss: 3.8849e-07 - dense_22_loss: 2.6247e-07 - dense_23_loss: 4.0975e-07 - dense_24_loss: 2.6795e-07 - dense_25_loss: 3.5534e-07 - val_loss: 2.6775e-06 - val_dense_18_loss: 1.6973e-07 - val_dense_19_loss: 7.4387e-07 - val_dense_20_loss: 1.7670e-07 - val_dense_21_loss: 3.6557e-07 - val_dense_22_loss: 2.5455e-07 - val_dense_23_loss: 3.7922e-07 - val_dense_24_loss: 2.4881e-07 - val_dense_25_loss: 3.3901e-07\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 2.6064e-06 - dense_18_loss: 1.5346e-07 - dense_19_loss: 7.5095e-07 - dense_20_loss: 1.6645e-07 - dense_21_loss: 3.5581e-07 - dense_22_loss: 2.3914e-07 - dense_23_loss: 3.7830e-07 - dense_24_loss: 2.3833e-07 - dense_25_loss: 3.2399e-07 - val_loss: 2.5253e-06 - val_dense_18_loss: 1.4607e-07 - val_dense_19_loss: 7.2221e-07 - val_dense_20_loss: 1.6479e-07 - val_dense_21_loss: 3.3889e-07 - val_dense_22_loss: 2.4928e-07 - val_dense_23_loss: 3.6454e-07 - val_dense_24_loss: 2.3318e-07 - val_dense_25_loss: 3.0629e-07\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 2.4850e-06 - dense_18_loss: 1.3755e-07 - dense_19_loss: 7.3251e-07 - dense_20_loss: 1.5499e-07 - dense_21_loss: 3.3796e-07 - dense_22_loss: 2.2869e-07 - dense_23_loss: 3.6170e-07 - dense_24_loss: 2.2827e-07 - dense_25_loss: 3.0330e-07 - val_loss: 2.4474e-06 - val_dense_18_loss: 1.4418e-07 - val_dense_19_loss: 7.1154e-07 - val_dense_20_loss: 1.5819e-07 - val_dense_21_loss: 3.2994e-07 - val_dense_22_loss: 2.2576e-07 - val_dense_23_loss: 3.5615e-07 - val_dense_24_loss: 2.2338e-07 - val_dense_25_loss: 2.9822e-07\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 2.5578e-06 - dense_18_loss: 1.4686e-07 - dense_19_loss: 7.4075e-07 - dense_20_loss: 1.7053e-07 - dense_21_loss: 3.4852e-07 - dense_22_loss: 2.3799e-07 - dense_23_loss: 3.6850e-07 - dense_24_loss: 2.3336e-07 - dense_25_loss: 3.1128e-07 - val_loss: 2.9658e-06 - val_dense_18_loss: 1.9459e-07 - val_dense_19_loss: 7.6065e-07 - val_dense_20_loss: 2.5044e-07 - val_dense_21_loss: 4.0206e-07 - val_dense_22_loss: 3.0559e-07 - val_dense_23_loss: 4.1528e-07 - val_dense_24_loss: 2.7163e-07 - val_dense_25_loss: 3.6555e-07\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 5.3872e-06 - dense_18_loss: 5.0221e-07 - dense_19_loss: 1.0859e-06 - dense_20_loss: 5.9713e-07 - dense_21_loss: 7.0996e-07 - dense_22_loss: 6.4886e-07 - dense_23_loss: 7.1042e-07 - dense_24_loss: 5.0028e-07 - dense_25_loss: 6.3248e-07 - val_loss: 1.3556e-05 - val_dense_18_loss: 1.5931e-06 - val_dense_19_loss: 2.0839e-06 - val_dense_20_loss: 1.7361e-06 - val_dense_21_loss: 1.7950e-06 - val_dense_22_loss: 1.8538e-06 - val_dense_23_loss: 1.6480e-06 - val_dense_24_loss: 1.3180e-06 - val_dense_25_loss: 1.5280e-06\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 1.1925e-05 - dense_18_loss: 1.4051e-06 - dense_19_loss: 1.9439e-06 - dense_20_loss: 1.4551e-06 - dense_21_loss: 1.5291e-06 - dense_22_loss: 1.6200e-06 - dense_23_loss: 1.4214e-06 - dense_24_loss: 1.2245e-06 - dense_25_loss: 1.3257e-06 - val_loss: 7.6484e-06 - val_dense_18_loss: 7.8282e-07 - val_dense_19_loss: 1.6005e-06 - val_dense_20_loss: 6.7409e-07 - val_dense_21_loss: 8.9168e-07 - val_dense_22_loss: 9.0865e-07 - val_dense_23_loss: 1.1225e-06 - val_dense_24_loss: 7.7288e-07 - val_dense_25_loss: 8.9531e-07\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 6.9225e-06 - dense_18_loss: 6.6700e-07 - dense_19_loss: 1.4291e-06 - dense_20_loss: 6.8332e-07 - dense_21_loss: 8.6106e-07 - dense_22_loss: 8.3618e-07 - dense_23_loss: 9.6069e-07 - dense_24_loss: 6.6870e-07 - dense_25_loss: 8.1644e-07 - val_loss: 6.8023e-06 - val_dense_18_loss: 6.9537e-07 - val_dense_19_loss: 1.4022e-06 - val_dense_20_loss: 6.1279e-07 - val_dense_21_loss: 8.6620e-07 - val_dense_22_loss: 8.8478e-07 - val_dense_23_loss: 9.4956e-07 - val_dense_24_loss: 6.4698e-07 - val_dense_25_loss: 7.4449e-07\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 5.4438e-06 - dense_18_loss: 5.2981e-07 - dense_19_loss: 1.1218e-06 - dense_20_loss: 5.2889e-07 - dense_21_loss: 6.7873e-07 - dense_22_loss: 6.5496e-07 - dense_23_loss: 7.2903e-07 - dense_24_loss: 5.6864e-07 - dense_25_loss: 6.3200e-07 - val_loss: 4.0272e-06 - val_dense_18_loss: 3.2783e-07 - val_dense_19_loss: 9.4535e-07 - val_dense_20_loss: 3.4238e-07 - val_dense_21_loss: 4.9767e-07 - val_dense_22_loss: 4.4130e-07 - val_dense_23_loss: 5.8189e-07 - val_dense_24_loss: 3.9487e-07 - val_dense_25_loss: 4.9591e-07\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 3.7897e-06 - dense_18_loss: 3.2718e-07 - dense_19_loss: 9.0580e-07 - dense_20_loss: 3.1133e-07 - dense_21_loss: 4.7683e-07 - dense_22_loss: 4.0621e-07 - dense_23_loss: 5.3153e-07 - dense_24_loss: 3.7137e-07 - dense_25_loss: 4.5945e-07 - val_loss: 3.1312e-06 - val_dense_18_loss: 2.3945e-07 - val_dense_19_loss: 7.8869e-07 - val_dense_20_loss: 2.3584e-07 - val_dense_21_loss: 4.0831e-07 - val_dense_22_loss: 3.3570e-07 - val_dense_23_loss: 4.5143e-07 - val_dense_24_loss: 2.9790e-07 - val_dense_25_loss: 3.7388e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 3.0577e-06 - dense_18_loss: 2.2938e-07 - dense_19_loss: 7.9987e-07 - dense_20_loss: 2.3564e-07 - dense_21_loss: 3.9948e-07 - dense_22_loss: 3.0051e-07 - dense_23_loss: 4.2643e-07 - dense_24_loss: 3.0433e-07 - dense_25_loss: 3.6210e-07 - val_loss: 2.7227e-06 - val_dense_18_loss: 1.7931e-07 - val_dense_19_loss: 7.3336e-07 - val_dense_20_loss: 1.9848e-07 - val_dense_21_loss: 3.7060e-07 - val_dense_22_loss: 2.5850e-07 - val_dense_23_loss: 3.8740e-07 - val_dense_24_loss: 2.6363e-07 - val_dense_25_loss: 3.3139e-07\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 2.7824e-06 - dense_18_loss: 1.7902e-07 - dense_19_loss: 7.4765e-07 - dense_20_loss: 2.0772e-07 - dense_21_loss: 3.7909e-07 - dense_22_loss: 2.6381e-07 - dense_23_loss: 3.9261e-07 - dense_24_loss: 2.7174e-07 - dense_25_loss: 3.4075e-07 - val_loss: 2.6916e-06 - val_dense_18_loss: 1.7297e-07 - val_dense_19_loss: 7.1758e-07 - val_dense_20_loss: 1.8711e-07 - val_dense_21_loss: 3.6896e-07 - val_dense_22_loss: 2.7766e-07 - val_dense_23_loss: 3.7688e-07 - val_dense_24_loss: 2.6014e-07 - val_dense_25_loss: 3.3033e-07\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 75ms/step - loss: 2.6246e-06 - dense_18_loss: 1.6236e-07 - dense_19_loss: 7.2894e-07 - dense_20_loss: 1.7705e-07 - dense_21_loss: 3.5165e-07 - dense_22_loss: 2.5642e-07 - dense_23_loss: 3.7567e-07 - dense_24_loss: 2.5236e-07 - dense_25_loss: 3.2019e-07 - val_loss: 2.5881e-06 - val_dense_18_loss: 1.5441e-07 - val_dense_19_loss: 7.3168e-07 - val_dense_20_loss: 1.7035e-07 - val_dense_21_loss: 3.4325e-07 - val_dense_22_loss: 2.6160e-07 - val_dense_23_loss: 3.6313e-07 - val_dense_24_loss: 2.5561e-07 - val_dense_25_loss: 3.0803e-07\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 2.8159e-06 - dense_18_loss: 1.7397e-07 - dense_19_loss: 7.7360e-07 - dense_20_loss: 1.9461e-07 - dense_21_loss: 3.7235e-07 - dense_22_loss: 2.8888e-07 - dense_23_loss: 3.9958e-07 - dense_24_loss: 2.7294e-07 - dense_25_loss: 3.3996e-07 - val_loss: 3.4375e-06 - val_dense_18_loss: 2.3830e-07 - val_dense_19_loss: 8.2687e-07 - val_dense_20_loss: 2.8972e-07 - val_dense_21_loss: 4.5305e-07 - val_dense_22_loss: 3.6760e-07 - val_dense_23_loss: 4.7729e-07 - val_dense_24_loss: 3.6075e-07 - val_dense_25_loss: 4.2396e-07\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 5.0838e-06 - dense_18_loss: 3.8871e-07 - dense_19_loss: 1.1002e-06 - dense_20_loss: 4.6566e-07 - dense_21_loss: 6.1139e-07 - dense_22_loss: 6.3116e-07 - dense_23_loss: 6.8210e-07 - dense_24_loss: 5.8071e-07 - dense_25_loss: 6.2396e-07 - val_loss: 8.2099e-06 - val_dense_18_loss: 7.0144e-07 - val_dense_19_loss: 1.5544e-06 - val_dense_20_loss: 8.0320e-07 - val_dense_21_loss: 9.2072e-07 - val_dense_22_loss: 1.1445e-06 - val_dense_23_loss: 1.0865e-06 - val_dense_24_loss: 9.9772e-07 - val_dense_25_loss: 1.0015e-06\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 7.1442e-06 - dense_18_loss: 6.2949e-07 - dense_19_loss: 1.3743e-06 - dense_20_loss: 7.1919e-07 - dense_21_loss: 8.2071e-07 - dense_22_loss: 9.4023e-07 - dense_23_loss: 9.4152e-07 - dense_24_loss: 8.6764e-07 - dense_25_loss: 8.5112e-07 - val_loss: 3.2136e-06 - val_dense_18_loss: 2.5095e-07 - val_dense_19_loss: 7.8411e-07 - val_dense_20_loss: 2.5342e-07 - val_dense_21_loss: 4.3406e-07 - val_dense_22_loss: 3.4323e-07 - val_dense_23_loss: 4.1672e-07 - val_dense_24_loss: 3.3612e-07 - val_dense_25_loss: 3.9497e-07\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 5.0141e-06 - dense_18_loss: 4.2887e-07 - dense_19_loss: 9.9956e-07 - dense_20_loss: 4.0104e-07 - dense_21_loss: 6.4893e-07 - dense_22_loss: 6.4438e-07 - dense_23_loss: 6.6501e-07 - dense_24_loss: 5.7641e-07 - dense_25_loss: 6.4993e-07 - val_loss: 4.7903e-06 - val_dense_18_loss: 4.3134e-07 - val_dense_19_loss: 9.4489e-07 - val_dense_20_loss: 4.0579e-07 - val_dense_21_loss: 6.6668e-07 - val_dense_22_loss: 5.4394e-07 - val_dense_23_loss: 6.2835e-07 - val_dense_24_loss: 5.4160e-07 - val_dense_25_loss: 6.2770e-07\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 5.0849e-06 - dense_18_loss: 4.6841e-07 - dense_19_loss: 1.0879e-06 - dense_20_loss: 4.7641e-07 - dense_21_loss: 6.8876e-07 - dense_22_loss: 5.4765e-07 - dense_23_loss: 6.3434e-07 - dense_24_loss: 5.5750e-07 - dense_25_loss: 6.2392e-07 - val_loss: 3.1341e-06 - val_dense_18_loss: 2.4688e-07 - val_dense_19_loss: 7.7079e-07 - val_dense_20_loss: 2.3614e-07 - val_dense_21_loss: 4.3259e-07 - val_dense_22_loss: 3.1284e-07 - val_dense_23_loss: 4.1978e-07 - val_dense_24_loss: 3.3335e-07 - val_dense_25_loss: 3.8170e-07\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 74ms/step - loss: 2.9605e-06 - dense_18_loss: 2.1691e-07 - dense_19_loss: 7.7179e-07 - dense_20_loss: 2.1205e-07 - dense_21_loss: 3.9103e-07 - dense_22_loss: 2.9892e-07 - dense_23_loss: 4.0230e-07 - dense_24_loss: 3.0212e-07 - dense_25_loss: 3.6536e-07 - val_loss: 2.7911e-06 - val_dense_18_loss: 1.9443e-07 - val_dense_19_loss: 7.1090e-07 - val_dense_20_loss: 2.0750e-07 - val_dense_21_loss: 3.8546e-07 - val_dense_22_loss: 2.9495e-07 - val_dense_23_loss: 3.8385e-07 - val_dense_24_loss: 2.6942e-07 - val_dense_25_loss: 3.4458e-07\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 2.9469e-06 - dense_18_loss: 1.9884e-07 - dense_19_loss: 7.5180e-07 - dense_20_loss: 2.1179e-07 - dense_21_loss: 3.9987e-07 - dense_22_loss: 3.0283e-07 - dense_23_loss: 4.1638e-07 - dense_24_loss: 2.9794e-07 - dense_25_loss: 3.6745e-07 - val_loss: 2.5511e-06 - val_dense_18_loss: 1.7205e-07 - val_dense_19_loss: 6.7626e-07 - val_dense_20_loss: 1.8306e-07 - val_dense_21_loss: 3.3259e-07 - val_dense_22_loss: 2.3625e-07 - val_dense_23_loss: 3.6833e-07 - val_dense_24_loss: 2.6763e-07 - val_dense_25_loss: 3.1497e-07\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 3.2024e-06 - dense_18_loss: 2.4234e-07 - dense_19_loss: 7.5833e-07 - dense_20_loss: 2.5938e-07 - dense_21_loss: 4.2247e-07 - dense_22_loss: 2.9137e-07 - dense_23_loss: 4.5622e-07 - dense_24_loss: 3.6726e-07 - dense_25_loss: 4.0501e-07 - val_loss: 4.9849e-06 - val_dense_18_loss: 4.3346e-07 - val_dense_19_loss: 9.7720e-07 - val_dense_20_loss: 4.6026e-07 - val_dense_21_loss: 6.6762e-07 - val_dense_22_loss: 4.9339e-07 - val_dense_23_loss: 6.8737e-07 - val_dense_24_loss: 6.2659e-07 - val_dense_25_loss: 6.3901e-07\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 1.0129e-05 - dense_18_loss: 9.6836e-07 - dense_19_loss: 1.5151e-06 - dense_20_loss: 1.1511e-06 - dense_21_loss: 1.3297e-06 - dense_22_loss: 1.0750e-06 - dense_23_loss: 1.3291e-06 - dense_24_loss: 1.4319e-06 - dense_25_loss: 1.3284e-06 - val_loss: 1.6406e-05 - val_dense_18_loss: 1.6751e-06 - val_dense_19_loss: 2.2077e-06 - val_dense_20_loss: 2.0630e-06 - val_dense_21_loss: 2.1350e-06 - val_dense_22_loss: 1.8262e-06 - val_dense_23_loss: 2.0302e-06 - val_dense_24_loss: 2.3995e-06 - val_dense_25_loss: 2.0694e-06\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 9.6783e-06 - dense_18_loss: 9.2731e-07 - dense_19_loss: 1.5590e-06 - dense_20_loss: 1.1276e-06 - dense_21_loss: 1.2072e-06 - dense_22_loss: 1.1146e-06 - dense_23_loss: 1.2067e-06 - dense_24_loss: 1.3015e-06 - dense_25_loss: 1.2345e-06 - val_loss: 9.5912e-06 - val_dense_18_loss: 9.3549e-07 - val_dense_19_loss: 1.3759e-06 - val_dense_20_loss: 1.1897e-06 - val_dense_21_loss: 1.1644e-06 - val_dense_22_loss: 1.1366e-06 - val_dense_23_loss: 1.1562e-06 - val_dense_24_loss: 1.3352e-06 - val_dense_25_loss: 1.2978e-06\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 1s 188ms/step - loss: 5.7917e-06 - dense_18_loss: 5.0141e-07 - dense_19_loss: 1.0379e-06 - dense_20_loss: 6.1978e-07 - dense_21_loss: 7.6094e-07 - dense_22_loss: 6.4894e-07 - dense_23_loss: 7.3529e-07 - dense_24_loss: 7.3108e-07 - dense_25_loss: 7.5637e-07 - val_loss: 2.7840e-06 - val_dense_18_loss: 1.8464e-07 - val_dense_19_loss: 7.2870e-07 - val_dense_20_loss: 2.0258e-07 - val_dense_21_loss: 3.7242e-07 - val_dense_22_loss: 2.9015e-07 - val_dense_23_loss: 3.8421e-07 - val_dense_24_loss: 2.7993e-07 - val_dense_25_loss: 3.4132e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00056: early stopping\n",
      "\n",
      "Now training model 2/2\n",
      "Epoch 1/500\n",
      "6/6 [==============================] - 2s 103ms/step - loss: 0.0024 - dense_28_loss: 4.0423e-04 - dense_29_loss: 3.2125e-04 - dense_30_loss: 6.1134e-04 - dense_31_loss: 0.0011 - val_loss: 0.0015 - val_dense_28_loss: 1.8458e-04 - val_dense_29_loss: 1.6109e-04 - val_dense_30_loss: 3.8019e-04 - val_dense_31_loss: 8.1060e-04\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 0.0012 - dense_28_loss: 1.3886e-04 - dense_29_loss: 1.3142e-04 - dense_30_loss: 2.8883e-04 - dense_31_loss: 6.4401e-04 - val_loss: 7.3698e-04 - val_dense_28_loss: 8.2649e-05 - val_dense_29_loss: 8.3000e-05 - val_dense_30_loss: 1.4952e-04 - val_dense_31_loss: 4.2181e-04\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 5.0965e-04 - dense_28_loss: 5.8956e-05 - dense_29_loss: 5.8899e-05 - dense_30_loss: 9.0937e-05 - dense_31_loss: 3.0086e-04 - val_loss: 2.7339e-04 - val_dense_28_loss: 3.7487e-05 - val_dense_29_loss: 3.5998e-05 - val_dense_30_loss: 4.1217e-05 - val_dense_31_loss: 1.5869e-04\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 2.1174e-04 - dense_28_loss: 3.7338e-05 - dense_29_loss: 3.1479e-05 - dense_30_loss: 3.6994e-05 - dense_31_loss: 1.0593e-04 - val_loss: 1.4550e-04 - val_dense_28_loss: 2.7574e-05 - val_dense_29_loss: 2.6435e-05 - val_dense_30_loss: 3.1557e-05 - val_dense_31_loss: 5.9934e-05\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.1330e-04 - dense_28_loss: 2.2376e-05 - dense_29_loss: 2.0252e-05 - dense_30_loss: 2.5349e-05 - dense_31_loss: 4.5323e-05 - val_loss: 7.6797e-05 - val_dense_28_loss: 1.7813e-05 - val_dense_29_loss: 1.3754e-05 - val_dense_30_loss: 1.4719e-05 - val_dense_31_loss: 3.0511e-05\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 6.2225e-05 - dense_28_loss: 1.5008e-05 - dense_29_loss: 1.1498e-05 - dense_30_loss: 1.1980e-05 - dense_31_loss: 2.3739e-05 - val_loss: 4.6038e-05 - val_dense_28_loss: 1.2914e-05 - val_dense_29_loss: 8.5872e-06 - val_dense_30_loss: 8.6716e-06 - val_dense_31_loss: 1.5865e-05\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 3.7225e-05 - dense_28_loss: 1.1074e-05 - dense_29_loss: 8.0004e-06 - dense_30_loss: 7.6212e-06 - dense_31_loss: 1.0530e-05 - val_loss: 2.6493e-05 - val_dense_28_loss: 8.4886e-06 - val_dense_29_loss: 6.2056e-06 - val_dense_30_loss: 5.6396e-06 - val_dense_31_loss: 6.1588e-06\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.2961e-05 - dense_28_loss: 7.4181e-06 - dense_29_loss: 5.3875e-06 - dense_30_loss: 4.3108e-06 - dense_31_loss: 5.8445e-06 - val_loss: 1.9482e-05 - val_dense_28_loss: 6.5265e-06 - val_dense_29_loss: 3.9902e-06 - val_dense_30_loss: 3.2066e-06 - val_dense_31_loss: 5.7592e-06\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.8122e-05 - dense_28_loss: 6.2835e-06 - dense_29_loss: 3.4775e-06 - dense_30_loss: 2.9698e-06 - dense_31_loss: 5.3913e-06 - val_loss: 1.6663e-05 - val_dense_28_loss: 5.8970e-06 - val_dense_29_loss: 3.2452e-06 - val_dense_30_loss: 2.5404e-06 - val_dense_31_loss: 4.9807e-06\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.5407e-05 - dense_28_loss: 5.5468e-06 - dense_29_loss: 3.1211e-06 - dense_30_loss: 2.0187e-06 - dense_31_loss: 4.7207e-06 - val_loss: 1.4131e-05 - val_dense_28_loss: 5.2639e-06 - val_dense_29_loss: 2.8379e-06 - val_dense_30_loss: 1.5260e-06 - val_dense_31_loss: 4.5028e-06\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.3260e-05 - dense_28_loss: 5.0425e-06 - dense_29_loss: 2.5820e-06 - dense_30_loss: 1.3646e-06 - dense_31_loss: 4.2707e-06 - val_loss: 1.2656e-05 - val_dense_28_loss: 4.9421e-06 - val_dense_29_loss: 2.3303e-06 - val_dense_30_loss: 1.2328e-06 - val_dense_31_loss: 4.1510e-06\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.2242e-05 - dense_28_loss: 4.8579e-06 - dense_29_loss: 2.2766e-06 - dense_30_loss: 1.1488e-06 - dense_31_loss: 3.9590e-06 - val_loss: 1.1919e-05 - val_dense_28_loss: 4.8409e-06 - val_dense_29_loss: 2.1951e-06 - val_dense_30_loss: 9.9260e-07 - val_dense_31_loss: 3.8903e-06\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.1568e-05 - dense_28_loss: 4.7070e-06 - dense_29_loss: 2.1753e-06 - dense_30_loss: 9.2971e-07 - dense_31_loss: 3.7562e-06 - val_loss: 1.1416e-05 - val_dense_28_loss: 4.6768e-06 - val_dense_29_loss: 2.1053e-06 - val_dense_30_loss: 8.7350e-07 - val_dense_31_loss: 3.7600e-06\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.1238e-05 - dense_28_loss: 4.6002e-06 - dense_29_loss: 2.0854e-06 - dense_30_loss: 8.7933e-07 - dense_31_loss: 3.6733e-06 - val_loss: 1.1267e-05 - val_dense_28_loss: 4.6422e-06 - val_dense_29_loss: 2.0380e-06 - val_dense_30_loss: 8.6488e-07 - val_dense_31_loss: 3.7217e-06\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.1075e-05 - dense_28_loss: 4.5542e-06 - dense_29_loss: 2.0218e-06 - dense_30_loss: 8.5483e-07 - dense_31_loss: 3.6438e-06 - val_loss: 1.1099e-05 - val_dense_28_loss: 4.5793e-06 - val_dense_29_loss: 2.0053e-06 - val_dense_30_loss: 8.2622e-07 - val_dense_31_loss: 3.6881e-06\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.0906e-05 - dense_28_loss: 4.4977e-06 - dense_29_loss: 1.9924e-06 - dense_30_loss: 8.1725e-07 - dense_31_loss: 3.5990e-06 - val_loss: 1.0944e-05 - val_dense_28_loss: 4.5170e-06 - val_dense_29_loss: 1.9657e-06 - val_dense_30_loss: 8.0957e-07 - val_dense_31_loss: 3.6515e-06\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0790e-05 - dense_28_loss: 4.4514e-06 - dense_29_loss: 1.9655e-06 - dense_30_loss: 8.0869e-07 - dense_31_loss: 3.5644e-06 - val_loss: 1.0881e-05 - val_dense_28_loss: 4.4972e-06 - val_dense_29_loss: 1.9489e-06 - val_dense_30_loss: 8.1205e-07 - val_dense_31_loss: 3.6230e-06\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1.0699e-05 - dense_28_loss: 4.4115e-06 - dense_29_loss: 1.9425e-06 - dense_30_loss: 8.0802e-07 - dense_31_loss: 3.5369e-06 - val_loss: 1.0782e-05 - val_dense_28_loss: 4.4489e-06 - val_dense_29_loss: 1.9287e-06 - val_dense_30_loss: 8.0621e-07 - val_dense_31_loss: 3.5981e-06\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.0616e-05 - dense_28_loss: 4.3730e-06 - dense_29_loss: 1.9262e-06 - dense_30_loss: 8.0502e-07 - dense_31_loss: 3.5122e-06 - val_loss: 1.0677e-05 - val_dense_28_loss: 4.3993e-06 - val_dense_29_loss: 1.9082e-06 - val_dense_30_loss: 8.0432e-07 - val_dense_31_loss: 3.5650e-06\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.0528e-05 - dense_28_loss: 4.3297e-06 - dense_29_loss: 1.9058e-06 - dense_30_loss: 8.0631e-07 - dense_31_loss: 3.4859e-06 - val_loss: 1.0607e-05 - val_dense_28_loss: 4.3648e-06 - val_dense_29_loss: 1.8948e-06 - val_dense_30_loss: 8.0776e-07 - val_dense_31_loss: 3.5396e-06\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.0441e-05 - dense_28_loss: 4.2863e-06 - dense_29_loss: 1.8872e-06 - dense_30_loss: 8.0773e-07 - dense_31_loss: 3.4600e-06 - val_loss: 1.0519e-05 - val_dense_28_loss: 4.3221e-06 - val_dense_29_loss: 1.8732e-06 - val_dense_30_loss: 8.1127e-07 - val_dense_31_loss: 3.5127e-06\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.0355e-05 - dense_28_loss: 4.2426e-06 - dense_29_loss: 1.8676e-06 - dense_30_loss: 8.1119e-07 - dense_31_loss: 3.4334e-06 - val_loss: 1.0433e-05 - val_dense_28_loss: 4.2762e-06 - val_dense_29_loss: 1.8539e-06 - val_dense_30_loss: 8.1580e-07 - val_dense_31_loss: 3.4868e-06\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.0259e-05 - dense_28_loss: 4.1909e-06 - dense_29_loss: 1.8459e-06 - dense_30_loss: 8.1603e-07 - dense_31_loss: 3.4060e-06 - val_loss: 1.0322e-05 - val_dense_28_loss: 4.2149e-06 - val_dense_29_loss: 1.8286e-06 - val_dense_30_loss: 8.2053e-07 - val_dense_31_loss: 3.4583e-06\n",
      "Epoch 24/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 25ms/step - loss: 1.0159e-05 - dense_28_loss: 4.1378e-06 - dense_29_loss: 1.8229e-06 - dense_30_loss: 8.2149e-07 - dense_31_loss: 3.3765e-06 - val_loss: 1.0224e-05 - val_dense_28_loss: 4.1610e-06 - val_dense_29_loss: 1.8081e-06 - val_dense_30_loss: 8.2748e-07 - val_dense_31_loss: 3.4270e-06\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.0050e-05 - dense_28_loss: 4.0789e-06 - dense_29_loss: 1.7989e-06 - dense_30_loss: 8.2695e-07 - dense_31_loss: 3.3450e-06 - val_loss: 1.0101e-05 - val_dense_28_loss: 4.0957e-06 - val_dense_29_loss: 1.7802e-06 - val_dense_30_loss: 8.3312e-07 - val_dense_31_loss: 3.3917e-06\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 9.9324e-06 - dense_28_loss: 4.0153e-06 - dense_29_loss: 1.7716e-06 - dense_30_loss: 8.3461e-07 - dense_31_loss: 3.3109e-06 - val_loss: 9.9765e-06 - val_dense_28_loss: 4.0272e-06 - val_dense_29_loss: 1.7534e-06 - val_dense_30_loss: 8.4192e-07 - val_dense_31_loss: 3.3539e-06\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 9.7939e-06 - dense_28_loss: 3.9402e-06 - dense_29_loss: 1.7419e-06 - dense_30_loss: 8.4079e-07 - dense_31_loss: 3.2710e-06 - val_loss: 9.8305e-06 - val_dense_28_loss: 3.9517e-06 - val_dense_29_loss: 1.7213e-06 - val_dense_30_loss: 8.4704e-07 - val_dense_31_loss: 3.3104e-06\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 9.6500e-06 - dense_28_loss: 3.8620e-06 - dense_29_loss: 1.7108e-06 - dense_30_loss: 8.4885e-07 - dense_31_loss: 3.2284e-06 - val_loss: 9.6692e-06 - val_dense_28_loss: 3.8620e-06 - val_dense_29_loss: 1.6861e-06 - val_dense_30_loss: 8.5668e-07 - val_dense_31_loss: 3.2644e-06\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 9.4895e-06 - dense_28_loss: 3.7751e-06 - dense_29_loss: 1.6761e-06 - dense_30_loss: 8.5729e-07 - dense_31_loss: 3.1810e-06 - val_loss: 9.4963e-06 - val_dense_28_loss: 3.7654e-06 - val_dense_29_loss: 1.6500e-06 - val_dense_30_loss: 8.6691e-07 - val_dense_31_loss: 3.2140e-06\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 9.3164e-06 - dense_28_loss: 3.6782e-06 - dense_29_loss: 1.6399e-06 - dense_30_loss: 8.6793e-07 - dense_31_loss: 3.1303e-06 - val_loss: 9.3218e-06 - val_dense_28_loss: 3.6746e-06 - val_dense_29_loss: 1.6134e-06 - val_dense_30_loss: 8.7464e-07 - val_dense_31_loss: 3.1592e-06\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 9.1152e-06 - dense_28_loss: 3.5722e-06 - dense_29_loss: 1.5979e-06 - dense_30_loss: 8.7491e-07 - dense_31_loss: 3.0701e-06 - val_loss: 9.0949e-06 - val_dense_28_loss: 3.5501e-06 - val_dense_29_loss: 1.5679e-06 - val_dense_30_loss: 8.8378e-07 - val_dense_31_loss: 3.0932e-06\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 8.8955e-06 - dense_28_loss: 3.4537e-06 - dense_29_loss: 1.5532e-06 - dense_30_loss: 8.8354e-07 - dense_31_loss: 3.0051e-06 - val_loss: 8.8586e-06 - val_dense_28_loss: 3.4226e-06 - val_dense_29_loss: 1.5212e-06 - val_dense_30_loss: 8.9354e-07 - val_dense_31_loss: 3.0213e-06\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 8.6549e-06 - dense_28_loss: 3.3263e-06 - dense_29_loss: 1.5048e-06 - dense_30_loss: 8.9167e-07 - dense_31_loss: 2.9321e-06 - val_loss: 8.6099e-06 - val_dense_28_loss: 3.2910e-06 - val_dense_29_loss: 1.4717e-06 - val_dense_30_loss: 9.0306e-07 - val_dense_31_loss: 2.9442e-06\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 8.4020e-06 - dense_28_loss: 3.1930e-06 - dense_29_loss: 1.4551e-06 - dense_30_loss: 8.9909e-07 - dense_31_loss: 2.8548e-06 - val_loss: 8.3353e-06 - val_dense_28_loss: 3.1479e-06 - val_dense_29_loss: 1.4200e-06 - val_dense_30_loss: 9.0847e-07 - val_dense_31_loss: 2.8590e-06\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 8.1112e-06 - dense_28_loss: 3.0408e-06 - dense_29_loss: 1.3991e-06 - dense_30_loss: 9.0478e-07 - dense_31_loss: 2.7665e-06 - val_loss: 8.0489e-06 - val_dense_28_loss: 2.9986e-06 - val_dense_29_loss: 1.3662e-06 - val_dense_30_loss: 9.1367e-07 - val_dense_31_loss: 2.7705e-06\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 7.8171e-06 - dense_28_loss: 2.8884e-06 - dense_29_loss: 1.3443e-06 - dense_30_loss: 9.0982e-07 - dense_31_loss: 2.6745e-06 - val_loss: 7.7174e-06 - val_dense_28_loss: 2.8260e-06 - val_dense_29_loss: 1.3032e-06 - val_dense_30_loss: 9.2188e-07 - val_dense_31_loss: 2.6663e-06\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 7.4794e-06 - dense_28_loss: 2.7188e-06 - dense_29_loss: 1.2810e-06 - dense_30_loss: 9.1086e-07 - dense_31_loss: 2.5688e-06 - val_loss: 7.3589e-06 - val_dense_28_loss: 2.6555e-06 - val_dense_29_loss: 1.2380e-06 - val_dense_30_loss: 9.1383e-07 - val_dense_31_loss: 2.5516e-06\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 7.1385e-06 - dense_28_loss: 2.5527e-06 - dense_29_loss: 1.2196e-06 - dense_30_loss: 9.0808e-07 - dense_31_loss: 2.4581e-06 - val_loss: 6.9848e-06 - val_dense_28_loss: 2.4720e-06 - val_dense_29_loss: 1.1727e-06 - val_dense_30_loss: 9.0978e-07 - val_dense_31_loss: 2.4304e-06\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 6.7689e-06 - dense_28_loss: 2.3722e-06 - dense_29_loss: 1.1560e-06 - dense_30_loss: 9.0286e-07 - dense_31_loss: 2.3379e-06 - val_loss: 6.6267e-06 - val_dense_28_loss: 2.2952e-06 - val_dense_29_loss: 1.1148e-06 - val_dense_30_loss: 9.0800e-07 - val_dense_31_loss: 2.3087e-06\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 6.3861e-06 - dense_28_loss: 2.1887e-06 - dense_29_loss: 1.0908e-06 - dense_30_loss: 8.9484e-07 - dense_31_loss: 2.2117e-06 - val_loss: 6.1971e-06 - val_dense_28_loss: 2.0997e-06 - val_dense_29_loss: 1.0406e-06 - val_dense_30_loss: 8.8736e-07 - val_dense_31_loss: 2.1695e-06\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 5.9674e-06 - dense_28_loss: 2.0004e-06 - dense_29_loss: 1.0188e-06 - dense_30_loss: 8.7491e-07 - dense_31_loss: 2.0733e-06 - val_loss: 5.7815e-06 - val_dense_28_loss: 1.9115e-06 - val_dense_29_loss: 9.7282e-07 - val_dense_30_loss: 8.6970e-07 - val_dense_31_loss: 2.0274e-06\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 5.5588e-06 - dense_28_loss: 1.8173e-06 - dense_29_loss: 9.5184e-07 - dense_30_loss: 8.5450e-07 - dense_31_loss: 1.9352e-06 - val_loss: 5.3696e-06 - val_dense_28_loss: 1.7311e-06 - val_dense_29_loss: 9.0828e-07 - val_dense_30_loss: 8.4545e-07 - val_dense_31_loss: 1.8847e-06\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 5.1477e-06 - dense_28_loss: 1.6400e-06 - dense_29_loss: 8.8676e-07 - dense_30_loss: 8.2871e-07 - dense_31_loss: 1.7922e-06 - val_loss: 4.9530e-06 - val_dense_28_loss: 1.5564e-06 - val_dense_29_loss: 8.4283e-07 - val_dense_30_loss: 8.1682e-07 - val_dense_31_loss: 1.7370e-06\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 4.7491e-06 - dense_28_loss: 1.4719e-06 - dense_29_loss: 8.2690e-07 - dense_30_loss: 7.9924e-07 - dense_31_loss: 1.6511e-06 - val_loss: 4.5342e-06 - val_dense_28_loss: 1.3868e-06 - val_dense_29_loss: 7.7865e-07 - val_dense_30_loss: 7.7932e-07 - val_dense_31_loss: 1.5894e-06\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 4.3396e-06 - dense_28_loss: 1.3084e-06 - dense_29_loss: 7.6310e-07 - dense_30_loss: 7.6225e-07 - dense_31_loss: 1.5059e-06 - val_loss: 4.1408e-06 - val_dense_28_loss: 1.2326e-06 - val_dense_29_loss: 7.2002e-07 - val_dense_30_loss: 7.4333e-07 - val_dense_31_loss: 1.4449e-06\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 3.9659e-06 - dense_28_loss: 1.1626e-06 - dense_29_loss: 7.0782e-07 - dense_30_loss: 7.2592e-07 - dense_31_loss: 1.3696e-06 - val_loss: 3.7757e-06 - val_dense_28_loss: 1.0927e-06 - val_dense_29_loss: 6.6719e-07 - val_dense_30_loss: 7.0614e-07 - val_dense_31_loss: 1.3096e-06\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 3.6062e-06 - dense_28_loss: 1.0314e-06 - dense_29_loss: 6.5339e-07 - dense_30_loss: 6.8424e-07 - dense_31_loss: 1.2372e-06 - val_loss: 3.4176e-06 - val_dense_28_loss: 9.6549e-07 - val_dense_29_loss: 6.1603e-07 - val_dense_30_loss: 6.5879e-07 - val_dense_31_loss: 1.1773e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 3.2715e-06 - dense_28_loss: 9.1302e-07 - dense_29_loss: 6.0319e-07 - dense_30_loss: 6.4193e-07 - dense_31_loss: 1.1134e-06 - val_loss: 3.1073e-06 - val_dense_28_loss: 8.5788e-07 - val_dense_29_loss: 5.6975e-07 - val_dense_30_loss: 6.2024e-07 - val_dense_31_loss: 1.0594e-06\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 2.9782e-06 - dense_28_loss: 8.1269e-07 - dense_29_loss: 5.5979e-07 - dense_30_loss: 6.0320e-07 - dense_31_loss: 1.0026e-06 - val_loss: 2.8234e-06 - val_dense_28_loss: 7.6436e-07 - val_dense_29_loss: 5.2882e-07 - val_dense_30_loss: 5.8021e-07 - val_dense_31_loss: 9.5002e-07\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 2.7121e-06 - dense_28_loss: 7.2539e-07 - dense_29_loss: 5.2093e-07 - dense_30_loss: 5.6422e-07 - dense_31_loss: 9.0157e-07 - val_loss: 2.5600e-06 - val_dense_28_loss: 6.8123e-07 - val_dense_29_loss: 4.8854e-07 - val_dense_30_loss: 5.3895e-07 - val_dense_31_loss: 8.5130e-07\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 2.4705e-06 - dense_28_loss: 6.5071e-07 - dense_29_loss: 4.8314e-07 - dense_30_loss: 5.2662e-07 - dense_31_loss: 8.1003e-07 - val_loss: 2.3472e-06 - val_dense_28_loss: 6.1565e-07 - val_dense_29_loss: 4.5706e-07 - val_dense_30_loss: 5.0539e-07 - val_dense_31_loss: 7.6909e-07\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.2911e-06 - dense_28_loss: 5.9730e-07 - dense_29_loss: 4.5655e-07 - dense_30_loss: 4.9899e-07 - dense_31_loss: 7.3830e-07 - val_loss: 2.1999e-06 - val_dense_28_loss: 5.7170e-07 - val_dense_29_loss: 4.3849e-07 - val_dense_30_loss: 4.8342e-07 - val_dense_31_loss: 7.0630e-07\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 2.1699e-06 - dense_28_loss: 5.6172e-07 - dense_29_loss: 4.4082e-07 - dense_30_loss: 4.8177e-07 - dense_31_loss: 6.8561e-07 - val_loss: 2.1455e-06 - val_dense_28_loss: 5.5460e-07 - val_dense_29_loss: 4.3798e-07 - val_dense_30_loss: 4.7963e-07 - val_dense_31_loss: 6.7324e-07\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 2.1384e-06 - dense_28_loss: 5.4755e-07 - dense_29_loss: 4.4307e-07 - dense_30_loss: 4.8509e-07 - dense_31_loss: 6.6274e-07 - val_loss: 1.9570e-06 - val_dense_28_loss: 5.0415e-07 - val_dense_29_loss: 4.0170e-07 - val_dense_30_loss: 4.4474e-07 - val_dense_31_loss: 6.0642e-07\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.9071e-06 - dense_28_loss: 4.8844e-07 - dense_29_loss: 3.9735e-07 - dense_30_loss: 4.3564e-07 - dense_31_loss: 5.8570e-07 - val_loss: 1.9307e-06 - val_dense_28_loss: 4.9818e-07 - val_dense_29_loss: 3.9874e-07 - val_dense_30_loss: 4.4197e-07 - val_dense_31_loss: 5.9185e-07\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.8449e-06 - dense_28_loss: 4.7272e-07 - dense_29_loss: 3.8894e-07 - dense_30_loss: 4.2665e-07 - dense_31_loss: 5.5657e-07 - val_loss: 1.7580e-06 - val_dense_28_loss: 4.4931e-07 - val_dense_29_loss: 3.7106e-07 - val_dense_30_loss: 4.0656e-07 - val_dense_31_loss: 5.3102e-07\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.7293e-06 - dense_28_loss: 4.4264e-07 - dense_29_loss: 3.6884e-07 - dense_30_loss: 4.0261e-07 - dense_31_loss: 5.1519e-07 - val_loss: 1.6663e-06 - val_dense_28_loss: 4.3209e-07 - val_dense_29_loss: 3.5378e-07 - val_dense_30_loss: 3.8715e-07 - val_dense_31_loss: 4.9329e-07\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.6691e-06 - dense_28_loss: 4.2899e-07 - dense_29_loss: 3.5802e-07 - dense_30_loss: 3.9289e-07 - dense_31_loss: 4.8926e-07 - val_loss: 1.6093e-06 - val_dense_28_loss: 4.1610e-07 - val_dense_29_loss: 3.4334e-07 - val_dense_30_loss: 3.7828e-07 - val_dense_31_loss: 4.7158e-07\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 28ms/step - loss: 1.5991e-06 - dense_28_loss: 4.1251e-07 - dense_29_loss: 3.4621e-07 - dense_30_loss: 3.7750e-07 - dense_31_loss: 4.6288e-07 - val_loss: 1.5564e-06 - val_dense_28_loss: 4.0556e-07 - val_dense_29_loss: 3.3560e-07 - val_dense_30_loss: 3.6774e-07 - val_dense_31_loss: 4.4752e-07\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 1.5788e-06 - dense_28_loss: 4.0806e-07 - dense_29_loss: 3.4554e-07 - dense_30_loss: 3.7501e-07 - dense_31_loss: 4.5017e-07 - val_loss: 1.5284e-06 - val_dense_28_loss: 4.0043e-07 - val_dense_29_loss: 3.3118e-07 - val_dense_30_loss: 3.6232e-07 - val_dense_31_loss: 4.3442e-07\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.5273e-06 - dense_28_loss: 3.9666e-07 - dense_29_loss: 3.3459e-07 - dense_30_loss: 3.6474e-07 - dense_31_loss: 4.3128e-07 - val_loss: 1.4982e-06 - val_dense_28_loss: 3.9319e-07 - val_dense_29_loss: 3.2580e-07 - val_dense_30_loss: 3.5684e-07 - val_dense_31_loss: 4.2231e-07\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.5380e-06 - dense_28_loss: 3.9975e-07 - dense_29_loss: 3.3923e-07 - dense_30_loss: 3.6894e-07 - dense_31_loss: 4.3013e-07 - val_loss: 1.5391e-06 - val_dense_28_loss: 4.0148e-07 - val_dense_29_loss: 3.3802e-07 - val_dense_30_loss: 3.6811e-07 - val_dense_31_loss: 4.3152e-07\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 23ms/step - loss: 1.5684e-06 - dense_28_loss: 4.0876e-07 - dense_29_loss: 3.4839e-07 - dense_30_loss: 3.7687e-07 - dense_31_loss: 4.3437e-07 - val_loss: 1.5483e-06 - val_dense_28_loss: 4.0729e-07 - val_dense_29_loss: 3.3938e-07 - val_dense_30_loss: 3.7241e-07 - val_dense_31_loss: 4.2918e-07\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.5404e-06 - dense_28_loss: 4.0211e-07 - dense_29_loss: 3.4179e-07 - dense_30_loss: 3.7301e-07 - dense_31_loss: 4.2352e-07 - val_loss: 1.4775e-06 - val_dense_28_loss: 3.8875e-07 - val_dense_29_loss: 3.2944e-07 - val_dense_30_loss: 3.5491e-07 - val_dense_31_loss: 4.0444e-07\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.5165e-06 - dense_28_loss: 3.9399e-07 - dense_29_loss: 3.4359e-07 - dense_30_loss: 3.6553e-07 - dense_31_loss: 4.1335e-07 - val_loss: 1.5011e-06 - val_dense_28_loss: 3.9393e-07 - val_dense_29_loss: 3.4121e-07 - val_dense_30_loss: 3.5804e-07 - val_dense_31_loss: 4.0791e-07\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.5259e-06 - dense_28_loss: 3.9990e-07 - dense_29_loss: 3.4428e-07 - dense_30_loss: 3.6881e-07 - dense_31_loss: 4.1296e-07 - val_loss: 1.5261e-06 - val_dense_28_loss: 4.0027e-07 - val_dense_29_loss: 3.4660e-07 - val_dense_30_loss: 3.6555e-07 - val_dense_31_loss: 4.1367e-07\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 1.6770e-06 - dense_28_loss: 4.2569e-07 - dense_29_loss: 3.9074e-07 - dense_30_loss: 4.0265e-07 - dense_31_loss: 4.5793e-07 - val_loss: 1.7623e-06 - val_dense_28_loss: 4.5271e-07 - val_dense_29_loss: 4.1378e-07 - val_dense_30_loss: 4.1771e-07 - val_dense_31_loss: 4.7806e-07\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 1.6989e-06 - dense_28_loss: 4.3908e-07 - dense_29_loss: 3.9843e-07 - dense_30_loss: 4.0446e-07 - dense_31_loss: 4.5689e-07 - val_loss: 1.4280e-06 - val_dense_28_loss: 3.8033e-07 - val_dense_29_loss: 3.2209e-07 - val_dense_30_loss: 3.4577e-07 - val_dense_31_loss: 3.7985e-07\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.4600e-06 - dense_28_loss: 3.8508e-07 - dense_29_loss: 3.3246e-07 - dense_30_loss: 3.5421e-07 - dense_31_loss: 3.8827e-07 - val_loss: 1.5429e-06 - val_dense_28_loss: 4.0226e-07 - val_dense_29_loss: 3.5915e-07 - val_dense_30_loss: 3.7124e-07 - val_dense_31_loss: 4.1022e-07\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.5309e-06 - dense_28_loss: 3.9828e-07 - dense_29_loss: 3.5258e-07 - dense_30_loss: 3.7237e-07 - dense_31_loss: 4.0769e-07 - val_loss: 1.4803e-06 - val_dense_28_loss: 3.9512e-07 - val_dense_29_loss: 3.3204e-07 - val_dense_30_loss: 3.6212e-07 - val_dense_31_loss: 3.9101e-07\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.5897e-06 - dense_28_loss: 4.2290e-07 - dense_29_loss: 3.5811e-07 - dense_30_loss: 3.8928e-07 - dense_31_loss: 4.1944e-07 - val_loss: 1.6747e-06 - val_dense_28_loss: 4.4663e-07 - val_dense_29_loss: 3.7377e-07 - val_dense_30_loss: 4.1062e-07 - val_dense_31_loss: 4.4366e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.6522e-06 - dense_28_loss: 4.3818e-07 - dense_29_loss: 3.7552e-07 - dense_30_loss: 4.0480e-07 - dense_31_loss: 4.3366e-07 - val_loss: 1.4568e-06 - val_dense_28_loss: 3.9272e-07 - val_dense_29_loss: 3.3052e-07 - val_dense_30_loss: 3.5097e-07 - val_dense_31_loss: 3.8261e-07\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4529e-06 - dense_28_loss: 3.8497e-07 - dense_29_loss: 3.3393e-07 - dense_30_loss: 3.5317e-07 - dense_31_loss: 3.8085e-07 - val_loss: 1.4717e-06 - val_dense_28_loss: 3.9070e-07 - val_dense_29_loss: 3.4131e-07 - val_dense_30_loss: 3.5521e-07 - val_dense_31_loss: 3.8451e-07\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 26ms/step - loss: 1.4588e-06 - dense_28_loss: 3.8410e-07 - dense_29_loss: 3.3710e-07 - dense_30_loss: 3.5661e-07 - dense_31_loss: 3.8098e-07 - val_loss: 1.3648e-06 - val_dense_28_loss: 3.6844e-07 - val_dense_29_loss: 3.0933e-07 - val_dense_30_loss: 3.3270e-07 - val_dense_31_loss: 3.5428e-07\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.4026e-06 - dense_28_loss: 3.7410e-07 - dense_29_loss: 3.1961e-07 - dense_30_loss: 3.4369e-07 - dense_31_loss: 3.6521e-07 - val_loss: 1.4075e-06 - val_dense_28_loss: 3.7796e-07 - val_dense_29_loss: 3.2175e-07 - val_dense_30_loss: 3.4211e-07 - val_dense_31_loss: 3.6572e-07\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.4678e-06 - dense_28_loss: 3.8477e-07 - dense_29_loss: 3.4246e-07 - dense_30_loss: 3.5911e-07 - dense_31_loss: 3.8142e-07 - val_loss: 1.4974e-06 - val_dense_28_loss: 3.9184e-07 - val_dense_29_loss: 3.5325e-07 - val_dense_30_loss: 3.6065e-07 - val_dense_31_loss: 3.9168e-07\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 1.7412e-06 - dense_28_loss: 4.4119e-07 - dense_29_loss: 4.1870e-07 - dense_30_loss: 4.1690e-07 - dense_31_loss: 4.6446e-07 - val_loss: 2.2298e-06 - val_dense_28_loss: 5.5927e-07 - val_dense_29_loss: 5.4310e-07 - val_dense_30_loss: 5.2049e-07 - val_dense_31_loss: 6.0697e-07\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 2.3901e-06 - dense_28_loss: 5.7899e-07 - dense_29_loss: 5.8615e-07 - dense_30_loss: 5.6121e-07 - dense_31_loss: 6.6377e-07 - val_loss: 1.8030e-06 - val_dense_28_loss: 4.6063e-07 - val_dense_29_loss: 4.1911e-07 - val_dense_30_loss: 4.3610e-07 - val_dense_31_loss: 4.8717e-07\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 1.6835e-06 - dense_28_loss: 4.3868e-07 - dense_29_loss: 3.9851e-07 - dense_30_loss: 4.0484e-07 - dense_31_loss: 4.4146e-07 - val_loss: 1.9488e-06 - val_dense_28_loss: 4.9002e-07 - val_dense_29_loss: 4.7981e-07 - val_dense_30_loss: 4.5671e-07 - val_dense_31_loss: 5.2230e-07\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.6724e-06 - dense_28_loss: 4.3229e-07 - dense_29_loss: 3.9773e-07 - dense_30_loss: 4.0044e-07 - dense_31_loss: 4.4194e-07 - val_loss: 1.6210e-06 - val_dense_28_loss: 4.3261e-07 - val_dense_29_loss: 3.8549e-07 - val_dense_30_loss: 3.8617e-07 - val_dense_31_loss: 4.1671e-07\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.5956e-06 - dense_28_loss: 4.1857e-07 - dense_29_loss: 3.7787e-07 - dense_30_loss: 3.8263e-07 - dense_31_loss: 4.1652e-07 - val_loss: 1.4004e-06 - val_dense_28_loss: 3.8257e-07 - val_dense_29_loss: 3.2088e-07 - val_dense_30_loss: 3.4114e-07 - val_dense_31_loss: 3.5586e-07\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1.4683e-06 - dense_28_loss: 3.9569e-07 - dense_29_loss: 3.4098e-07 - dense_30_loss: 3.5674e-07 - dense_31_loss: 3.7487e-07 - val_loss: 1.3338e-06 - val_dense_28_loss: 3.6401e-07 - val_dense_29_loss: 3.0299e-07 - val_dense_30_loss: 3.2548e-07 - val_dense_31_loss: 3.4132e-07\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.3885e-06 - dense_28_loss: 3.7297e-07 - dense_29_loss: 3.1909e-07 - dense_30_loss: 3.3915e-07 - dense_31_loss: 3.5729e-07 - val_loss: 1.3648e-06 - val_dense_28_loss: 3.7264e-07 - val_dense_29_loss: 3.1313e-07 - val_dense_30_loss: 3.3080e-07 - val_dense_31_loss: 3.4822e-07\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.3882e-06 - dense_28_loss: 3.7442e-07 - dense_29_loss: 3.1851e-07 - dense_30_loss: 3.3919e-07 - dense_31_loss: 3.5606e-07 - val_loss: 1.4073e-06 - val_dense_28_loss: 3.8693e-07 - val_dense_29_loss: 3.2055e-07 - val_dense_30_loss: 3.3846e-07 - val_dense_31_loss: 3.6133e-07\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.4721e-06 - dense_28_loss: 3.9481e-07 - dense_29_loss: 3.4217e-07 - dense_30_loss: 3.5680e-07 - dense_31_loss: 3.7834e-07 - val_loss: 1.4834e-06 - val_dense_28_loss: 3.9640e-07 - val_dense_29_loss: 3.4893e-07 - val_dense_30_loss: 3.5907e-07 - val_dense_31_loss: 3.7897e-07\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 25ms/step - loss: 1.5485e-06 - dense_28_loss: 4.0806e-07 - dense_29_loss: 3.7130e-07 - dense_30_loss: 3.7216e-07 - dense_31_loss: 3.9699e-07 - val_loss: 1.5578e-06 - val_dense_28_loss: 4.1314e-07 - val_dense_29_loss: 3.6613e-07 - val_dense_30_loss: 3.7475e-07 - val_dense_31_loss: 4.0380e-07\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 1.5138e-06 - dense_28_loss: 4.0313e-07 - dense_29_loss: 3.5283e-07 - dense_30_loss: 3.7026e-07 - dense_31_loss: 3.8759e-07 - val_loss: 1.3778e-06 - val_dense_28_loss: 3.7879e-07 - val_dense_29_loss: 3.1078e-07 - val_dense_30_loss: 3.3645e-07 - val_dense_31_loss: 3.5177e-07\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 1.4229e-06 - dense_28_loss: 3.8608e-07 - dense_29_loss: 3.2493e-07 - dense_30_loss: 3.4696e-07 - dense_31_loss: 3.6494e-07 - val_loss: 1.4829e-06 - val_dense_28_loss: 4.0212e-07 - val_dense_29_loss: 3.4223e-07 - val_dense_30_loss: 3.6132e-07 - val_dense_31_loss: 3.7723e-07\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 1.5752e-06 - dense_28_loss: 4.1839e-07 - dense_29_loss: 3.7125e-07 - dense_30_loss: 3.8056e-07 - dense_31_loss: 4.0496e-07 - val_loss: 1.5499e-06 - val_dense_28_loss: 4.1594e-07 - val_dense_29_loss: 3.6528e-07 - val_dense_30_loss: 3.7248e-07 - val_dense_31_loss: 3.9620e-07\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 1.4527e-06 - dense_28_loss: 3.9248e-07 - dense_29_loss: 3.3593e-07 - dense_30_loss: 3.5346e-07 - dense_31_loss: 3.7083e-07 - val_loss: 1.3416e-06 - val_dense_28_loss: 3.6563e-07 - val_dense_29_loss: 3.0870e-07 - val_dense_30_loss: 3.2683e-07 - val_dense_31_loss: 3.4047e-07\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 1.4477e-06 - dense_28_loss: 3.8190e-07 - dense_29_loss: 3.4346e-07 - dense_30_loss: 3.5139e-07 - dense_31_loss: 3.7090e-07 - val_loss: 1.5030e-06 - val_dense_28_loss: 3.9570e-07 - val_dense_29_loss: 3.6096e-07 - val_dense_30_loss: 3.5875e-07 - val_dense_31_loss: 3.8762e-07\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.4639e-06 - dense_28_loss: 3.9097e-07 - dense_29_loss: 3.4388e-07 - dense_30_loss: 3.5528e-07 - dense_31_loss: 3.7379e-07 - val_loss: 1.3924e-06 - val_dense_28_loss: 3.8541e-07 - val_dense_29_loss: 3.1507e-07 - val_dense_30_loss: 3.3937e-07 - val_dense_31_loss: 3.5253e-07\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 27ms/step - loss: 1.4523e-06 - dense_28_loss: 3.9022e-07 - dense_29_loss: 3.3305e-07 - dense_30_loss: 3.5472e-07 - dense_31_loss: 3.7432e-07 - val_loss: 1.5114e-06 - val_dense_28_loss: 4.0613e-07 - val_dense_29_loss: 3.5260e-07 - val_dense_30_loss: 3.6384e-07 - val_dense_31_loss: 3.8882e-07\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 1.4981e-06 - dense_28_loss: 3.9761e-07 - dense_29_loss: 3.4872e-07 - dense_30_loss: 3.6248e-07 - dense_31_loss: 3.8931e-07 - val_loss: 1.3577e-06 - val_dense_28_loss: 3.7026e-07 - val_dense_29_loss: 3.1169e-07 - val_dense_30_loss: 3.3004e-07 - val_dense_31_loss: 3.4567e-07\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00094: early stopping\n",
      "42.2275657504797\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders)\n",
    "stop = time.perf_counter()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1167afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the Models for WP definition\n",
    "def evaluate_models_train(histstruct):\n",
    "    \n",
    "    for histgroup in histnames:\n",
    "        print('evaluating model for '+histgroup[0])\n",
    "        print(histstruct.evaluate_classifier(histgroup)[0].shape)\n",
    "    \n",
    "    # get mse for training set\n",
    "    if 'training' in histstruct.masks.keys(): masknames = ['dcson','highstat', 'training']\n",
    "    else: masknames = ['dcson','highstat']\n",
    "    mse_train = histstruct.get_scores_array( masknames=masknames )\n",
    "    print('Found mse array for training set of following shape: {}'.format(mse_train.shape))\n",
    "    \n",
    "    # get mse for good set\n",
    "    if 'good' in histstruct.masks.keys():\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            mse_good.append(histstruct.get_scores( histname=histname, masknames=['dcson','highstat','good'] ))\n",
    "    else:\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            hists_good = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), 1000 )\n",
    "            thismse = histstruct.classifiers[histname].evaluate( hists_good )\n",
    "            mse_good.append( thismse )\n",
    "            print(run)\n",
    "    mse_good = np.array(mse_good)\n",
    "    mse_good = np.transpose(mse_good)\n",
    "    print('Found mse array for good set of following shape: {}'.format(mse_good.shape))\n",
    "    \n",
    "    # get mse for bad sets\n",
    "    mse_bad = []\n",
    "    for i in range(nbadruns):\n",
    "        mse_bad.append( histstruct.get_scores_array( masknames=['dcson','highstat','bad{}'.format(i)] ) )\n",
    "        print('Found mse array for bad set of following shape: {}'.format(mse_bad[i].shape))\n",
    "        \n",
    "    return [mse_train, mse_good, mse_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f34546ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating model for NormalizedHitResiduals_TIB__Layer__1\n",
      "(9290,)\n",
      "evaluating model for chargeInner_PXLayer_1\n",
      "(9290,)\n",
      "Found mse array for training set of following shape: (4878, 12)\n",
      "Found mse array for good set of following shape: (2091, 12)\n",
      "Found mse array for bad set of following shape: (604, 12)\n",
      "Found mse array for bad set of following shape: (103, 12)\n",
      "Found mse array for bad set of following shape: (278, 12)\n"
     ]
    }
   ],
   "source": [
    "(mse_train, mse_good_eval, mse_bad_eval) = evaluate_models_train(histstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7105b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots and Distribution Analysis\n",
    "def fit_mse_distribution(histstruct, mse_train):\n",
    "    dimslist = []\n",
    "    fitfunclist = []\n",
    "    \n",
    "    \n",
    "    nhisttypes = len(histstruct.histnames)\n",
    "    for i in range(0,nhisttypes-1):\n",
    "        for j in range(i+1,nhisttypes):\n",
    "            dimslist.append((i, j))\n",
    "    \n",
    "    plt.close('all')\n",
    "    (npoints,ndims) = mse_train.shape\n",
    "    \n",
    "    \n",
    "    # settings for GaussianKdeFitter\n",
    "    scott_bw = npoints**(-1./(ndims+4))\n",
    "    bw_method = 20*scott_bw\n",
    "    # settings for HyperRectangleFitter\n",
    "    quantiles = ([0.00062,0.0006,0.00015,0.00015,\n",
    "                 0.0003,0.0003,0.00053,0.00065])\n",
    "    \n",
    "    \n",
    "    #for dims in dimslist:\n",
    "    #    thismse = mse_train[:,dims]\n",
    "    #    if training_mode=='global': \n",
    "    #        fitfunc = SeminormalFitter.SeminormalFitter(thismse)\n",
    "    #        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(thismse, \n",
    "    #        #                                                    [quantiles[dims[0]],quantiles[dims[1]]],\n",
    "    #        #                                                    'up')\n",
    "    #    else: fitfunc = GaussianKdeFitter.GaussianKdeFitter(thismse,bw_method=bw_method)\n",
    "    #    #pu.plot_fit_2d(thismse, fitfunc=fitfunc, logprob=True, clipprob=True,\n",
    "    #    #                onlycontour=False, xlims=30, ylims=30, \n",
    "    #    #                onlypositive=True, transparency=0.5,\n",
    "    #    #                xaxtitle=histstruct.histnames[dims[0]], \n",
    "    #    #                yaxtitle=histstruct.histnames[dims[1]],\n",
    "    #    #                title='density fit of lumisection MSE')\n",
    "    #    ##plt.close('all') # release plot memory\n",
    "    #    fitfunclist.append(fitfunc)\n",
    "    # \n",
    "    #    \n",
    "    if training_mode=='global': \n",
    "        fitfunc = SeminormalFitter.SeminormalFitter(mse_train)\n",
    "        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(mse_train, quantiles, 'up')\n",
    "    else: \n",
    "        fitfunc = GaussianKdeFitter.GaussianKdeFitter()\n",
    "        fitfunc.fit(mse_train,bw_method=bw_method)\n",
    "    \n",
    "    return fitfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4988fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitfunc = fit_mse_distribution(histstruct, mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7437c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare MSEs for Working Point Definition\n",
    "def mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc):\n",
    "    \n",
    "    # Get the minimum log probability of histograms in good set\n",
    "    print('--- good lumesections ---')\n",
    "    logprob_good = np.log(fitfunc.pdf(mse_good_eval))\n",
    "    print('length of log prob array: '+str(len(logprob_good)))\n",
    "    print('minimum of log prob: '+str(np.min(logprob_good)))\n",
    "    #print(sorted(logprob_good))\n",
    "    \n",
    "    print('--- bad lumisections ---')\n",
    "    logprob_bad_parts = [np.log(fitfunc.pdf(mse_bad_eval[j])) for j in range(len(mse_bad_eval))]\n",
    "    #for lp in logprob_bad_parts: print(str(sorted(lp))+'\\n\\n')\n",
    "    logprob_bad = np.concatenate(tuple(logprob_bad_parts))\n",
    "    \n",
    "    print('length of log prob array: '+str(len(logprob_bad)))\n",
    "    print('maximum of log prob: '+str(np.max(logprob_bad)))\n",
    "    #print(sorted(logprob_good))\n",
    "    #print(sorted(logprob_bad))\n",
    "    #print(logprob_bad)\n",
    "    \n",
    "    sep = np.min(logprob_good) - np.max(logprob_bad)\n",
    "    print('Separability: ' + str(sep))\n",
    "    \n",
    "    return [logprob_good, logprob_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a8ed5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- good lumesections ---\n",
      "length of log prob array: 2091\n",
      "minimum of log prob: 127.85388830818651\n",
      "--- bad lumisections ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27974/2141632033.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  logprob_bad_parts = [np.log(fitfunc.pdf(mse_bad_eval[j])) for j in range(len(mse_bad_eval))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of log prob array: 985\n",
      "maximum of log prob: 131.47420506398103\n",
      "Separability: -3.6203167557945193\n"
     ]
    }
   ],
   "source": [
    "(logprob_good, logprob_bad) = mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "09b777c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoder_individual(histstruct):\n",
    "    \n",
    "    msewps = []\n",
    "    for histname in histstruct.histnames:\n",
    "        \n",
    "        # Get histograms from histstruct\n",
    "        X_test_good = np.array([hu.normalize(histstruct.get_histograms(\n",
    "                histname = hname, masknames = ['dcson','highstat', 'good']), \n",
    "                                                 norm=\"l1\", axis=1) \n",
    "                                       for hname in histnamegroup]).transpose((1,0,2))\n",
    "        \n",
    "        X_test_bad = X_test_bad = np.array([hu.normalize(histstruct.get_histograms(\n",
    "                histname = hname, masknames = ['dcson','highstat', 'bad']), \n",
    "                                                 norm=\"l1\", axis=1) \n",
    "                                       for hname in histnamegroup]).transpose((1,0,2))\n",
    "        \n",
    "        # Get each model from the histstruct\n",
    "        autoencoder = histstruct.get_classifier(histname)\n",
    "        \n",
    "        # Getting evaluation criteria\n",
    "        prediction_test_good = autoencoder.reconstruct(X_test_good)\n",
    "        mse_test_good = aeu.mseTopNRaw(X_test_good, prediction_test_good, n=10 )\n",
    "        prediction_test_bad = autoencoder.reconstruct(X_test_bad)\n",
    "        mse_test_bad = aeu.mseTopNRaw(X_test_bad, prediction_test_bad, n=10 )\n",
    "        \n",
    "        if userfriendly:\n",
    "            print('Average MSE on good set: ' + str(np.mean(mse_test_good)))\n",
    "            print('Average MSE on bad set: ' + str(np.mean(mse_test_bad)))\n",
    "        \n",
    "        if createPlots:\n",
    "            # Number of plots of each type to generate per model (so nplot * 2 * len(model))\n",
    "            nplot = 3\n",
    "            \n",
    "            # Good examples\n",
    "            print('Examples of good histograms and reconstruction:')\n",
    "            randint = np.random.choice(np.arange(len(X_test_good)),size=nplot,replace=False)\n",
    "            for i in randint: \n",
    "                histlist = [X_test_good[int(i),:],prediction_test_good[int(i),:]]\n",
    "                labellist = ['data','reconstruction']\n",
    "                colorlist = ['black','blue']\n",
    "                pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist)\n",
    "                plt.show()\n",
    "            \n",
    "            # Bad examples\n",
    "            print('Examples of bad histograms and reconstruction:')\n",
    "            randint = np.random.choice(np.arange(len(X_test_bad)),size=nplot,replace=False)\n",
    "            for i in randint:\n",
    "                histlist = [X_test_bad[int(i),:],prediction_test_bad[int(i),:]]\n",
    "                labellist = ['data','reconstruction']\n",
    "                colorlist = ['black','blue']\n",
    "                pu.plot_hists(histlist,colorlist=colorlist,labellist=labellist)\n",
    "                plt.show()\n",
    "        \n",
    "        # Attaching the bad histograms as a new set of rows under the good histograms\n",
    "        validation_data = np.vstack((X_test_good, X_test_bad))\n",
    "        validation_preds = np.vstack((prediction_test_good, prediction_test_bad))\n",
    "        # Creating labels to differentiate the data when we go to compare predictions\n",
    "        #     with actual label\n",
    "        labels = np.hstack((np.zeros(len(X_test_good)), np.ones(len(X_test_bad))))\n",
    "        \n",
    "        # Pick a working point to see \n",
    "        msewp = 0.5*(np.mean(mse_test_bad) - np.mean(mse_test_good))\n",
    "        print(\"Selected working point: \" + str(msewp))\n",
    "        \n",
    "        # Get data to pick a good working point for future evaluation\n",
    "        scores = aeu.mseTop10Raw(validation_data, validation_preds)\n",
    "        nsig = np.sum(labels)\n",
    "        nback = np.sum(1-labels)\n",
    "        \n",
    "        # Get some metrics for the user\n",
    "        tp = np.sum(np.where((labels==1) & (scores>msewp),1,0))/nsig\n",
    "        fp = np.sum(np.where((labels==0) & (scores>msewp),1,0))/nback\n",
    "        tn = 1-fp\n",
    "        fn = 1-tp\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = (2*precision*recall) / (precision + recall)\n",
    "        \n",
    "        if userfriendly:\n",
    "            print(accuracy)\n",
    "            print(precision)\n",
    "            print(recall)\n",
    "            print(f1)\n",
    "        \n",
    "    return msewps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "891a2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor):\n",
    "    labels_good = np.zeros(len(logprob_good)) # background: label = 0\n",
    "    labels_bad = np.ones(len(logprob_bad)) # signal: label = 1\n",
    "    \n",
    "    badMin = min(np.where(logprob_bad != -np.inf, logprob_bad, -1))\n",
    "    goodMax = max(np.where(logprob_good != np.inf, logprob_good, 10001))\n",
    "    \n",
    "    logprob_good = np.where(logprob_good != np.inf, logprob_good, goodMax)\n",
    "    logprob_bad = np.where(logprob_bad != -np.inf, logprob_bad, badMin)\n",
    "    \n",
    "    # These only take effect if a histogram is grossly misclassified\n",
    "    logprob_good[logprob_good == -np.inf] = badMin\n",
    "    logprob_bad[logprob_bad == np.inf] = goodMax\n",
    "    \n",
    "    labels = np.concatenate(tuple([labels_good,labels_bad]))\n",
    "    scores = np.concatenate(tuple([-logprob_good,-logprob_bad]))\n",
    "    scores = aeu.clip_scores( scores )\n",
    "    \n",
    "    avSep = np.mean(logprob_good) - np.mean(logprob_bad)\n",
    "    \n",
    "    print('Average Separation: ' + str(avSep))\n",
    "    \n",
    "    pu.plot_score_dist(scores, labels, siglabel='anomalous', sigcolor='r', \n",
    "                       bcklabel='good', bckcolor='g', \n",
    "                       nbins=200, normalize=True,\n",
    "                       xaxtitle='negative logarithmic probability',\n",
    "                       yaxtitle='number of lumisections (normalized)')\n",
    "      \n",
    "    # Plot ROC curve for analysis\n",
    "    auc = aeu.get_roc(scores, labels, mode='geom', doprint=False)\n",
    "    \n",
    "    # Setting a threshold, below this working point defines anomalous data\n",
    "    # Average is biased towards better recall per user specifications\n",
    "    logprob_threshold = (1/(wpBiasFactor + 1)) * (wpBiasFactor*np.mean(logprob_good) + np.mean(logprob_bad))\n",
    "    # Or set manual\n",
    "    # logprob_threshold = 100\n",
    "    (_, _, _, tp, fp, tn, fn) = aeu.get_confusion_matrix(scores,labels,-logprob_threshold)\n",
    "    print('Selected logprob threshold of ' + str(logprob_threshold))\n",
    "    \n",
    "    # Get metrics for analysis\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_measure = (1 + fmBiasFactor * fmBiasFactor) * ((precision * recall) / ((fmBiasFactor * fmBiasFactor * precision) + recall)) \n",
    "    \n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Precision: ' + str(precision))\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('F-Measure: ' + str(f_measure))\n",
    "    \n",
    "    return logprob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bbddf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Separation: 180.84337670762622\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected logprob threshold of 122.81446766362305\n",
      "Accuracy: 0.8065989847715735\n",
      "Precision: 1.0\n",
      "Recall: 0.6131979695431472\n",
      "F-Measure: 0.664612676056338\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logprob_threshold = evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
