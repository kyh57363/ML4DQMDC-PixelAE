{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c47c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 18:47:22.085506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.3-88592/x86_64-centos7-gcc11-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.3-35f7a/x86_64-centos7-gcc11-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jaxlib/mlir/_mlir_libs:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/onnxruntime/capi/:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/11.0.14p1-8284a/x86_64-centos7-gcc11-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.37-355ed/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/4.1.2-f9ee4/x86_64-centos7-gcc11-opt/lib64/R/library/readr/rcon\n",
      "2022-08-02 18:47:22.085565: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'HyperRectangleFitter' from '/eos/home-i01/k/khowey/SWAN_projects/ML4DQMDC-PixelAE/KH-AutoencoderTest/../src/cloudfitters/HyperRectangleFitter.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import importlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# local modules\n",
    "sys.path.append('../utils')\n",
    "import csv_utils as csvu\n",
    "import json_utils as jsonu\n",
    "import dataframe_utils as dfu\n",
    "import hist_utils as hu\n",
    "import autoencoder_utils as aeu\n",
    "import plot_utils as pu\n",
    "import generate_data_utils as gdu\n",
    "import refruns_utils as rru\n",
    "importlib.reload(csvu)\n",
    "importlib.reload(jsonu)\n",
    "importlib.reload(dfu)\n",
    "importlib.reload(hu)\n",
    "importlib.reload(aeu)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(gdu)\n",
    "importlib.reload(rru)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/classifiers')\n",
    "sys.path.append('../src/cloudfitters')\n",
    "import HistStruct\n",
    "importlib.reload(HistStruct)\n",
    "import SubHistStruct\n",
    "importlib.reload(SubHistStruct)\n",
    "import FlexiStruct\n",
    "importlib.reload(FlexiStruct)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "import AutoEncoder\n",
    "importlib.reload(AutoEncoder)\n",
    "import SeminormalFitter\n",
    "import GaussianKdeFitter\n",
    "import HyperRectangleFitter\n",
    "importlib.reload(SeminormalFitter)\n",
    "importlib.reload(GaussianKdeFitter)\n",
    "importlib.reload(HyperRectangleFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa69b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speed Controls and Run Mode\n",
    "\n",
    "# Disables all plots for large datasets where speed is more important\n",
    "createPlots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6540dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Parameters\n",
    "\n",
    "# Select the bias towards recall against precision, treated as a factor (so < 1 biases towards precision, 1 is equal importance, and > 1 biases towards recall)\n",
    "wpBiasFactor = 20\n",
    "fmBiasFactor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03dd133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining bad runs\n",
    "badruns = {'2017B':\n",
    "                [\n",
    "                    297048,\n",
    "                    297282,\n",
    "                    297283,\n",
    "                    297284,\n",
    "                    297287,\n",
    "                    297288,\n",
    "                    297289,\n",
    "                    299316,\n",
    "                    299317,\n",
    "                    299318,\n",
    "                    299324,\n",
    "                    299326,\n",
    "                    301086,\n",
    "                    301086,\n",
    "                    303948,\n",
    "                    297047, #close but, true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297211, #Reconstructs well\n",
    "                    299325, #Reconstructs well\n",
    "                    297664, #true bad for all 8\n",
    "                    299317, #true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297502\n",
    "                ],\n",
    "             '2017C':[\n",
    "                  300781, # bad for tracking (pixels were excluded.\n",
    "                  300079, # is bad for strips and then also for tracking\n",
    "                  302029, # Poor detector elements for strips - Should be mildly anomalous, but technically good \n",
    "                  300576, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300574, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300282, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  301912, # Half bad for pixels (lost HV or readout card)  \n",
    "                  301086, # Half bad for pixels (lost HV or readout card)  \n",
    "                  300283, # Half bad for pixels (lost HV or readout card) \n",
    "                  300282, # Half bad for pixels (lost HV or readout card) \n",
    "                  300281, # Half bad for pixels (lost HV or readout card) \n",
    "                  300239, # Half bad for pixels (lost HV or readout card)\n",
    "                  301394, # Marginal for pixels\n",
    "                  301183, # Marginal for pixels\n",
    "                  300398, # Marginal for pixels\n",
    "                  300389, # Marginal for pixels\n",
    "                  300365  # Marginal for pixels\n",
    "             ],\n",
    "             '2017E':[\n",
    "                 304740, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304776, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304506, # Portcard problem for pixels\n",
    "                 304507, # Portcard problem for pixels \n",
    "                 303989, # Bad for pixels, power supply died\n",
    "                 303824  # Partly bad for strips due to a test\n",
    "             ],\n",
    "             '2017F':[\n",
    "                 306422, # Partly bad for strips - 2 data readouts failed \n",
    "                 306423, # Partly bad for strips - 2 data readouts failed\n",
    "                 306425, # Partly bad for strips - 2 data readouts failed\n",
    "                 305440, # Partly bad for strips - 1 data readout failed\n",
    "                 305441, # Partly bad for strips - 1 data readout failed\n",
    "                 305249, # Bad for pixels - half of disk failed \n",
    "                 305250, # Bad for pixels - half of disk failed\n",
    "                 305064, # Marginal for pixels - some readout failed\n",
    "             ],\n",
    "            '2018': # needs to be re-checked, not guaranteed to be fully correct or representative.\n",
    "                [\n",
    "                317479,\n",
    "                317480,\n",
    "                317481,\n",
    "                317482,\n",
    "                319847\n",
    "                ]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75d38bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad run :{'run_number': 303989, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 304740, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 305250, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 306422, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 305249, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n"
     ]
    }
   ],
   "source": [
    "### Select a reference run and get data\n",
    "rundict = jsonu.loadjson('../jsons/CertHelperRefRuns.json')\n",
    "\n",
    "# Select any run numbers to get a training set from that run's reference.\n",
    "runNums = [303824, 305249]\n",
    "refRuns = []\n",
    "eras = []\n",
    "years = []\n",
    "dataDict = {}\n",
    "badrunsls = {}\n",
    "trainrunsls = {}\n",
    "goodrunsls = {}\n",
    "refrunsls = {}\n",
    "for runNum in runNums:\n",
    "    runls = {}\n",
    "    for run in rundict:\n",
    "        if run['run_number'] == runNum:\n",
    "            runls.update(run)\n",
    "    if runls == {}:\n",
    "        raise Exception('Run not found - ' + str(runNum))\n",
    "    \n",
    "    year = runls['dataset'][11:15]\n",
    "    if year not in years: years.append(year)\n",
    "    era = runls['dataset'][15]\n",
    "    if era not in eras: eras.append(era)\n",
    "    ref_run = runls['reference_run_number']\n",
    "    \n",
    "    # Don't need duplicates\n",
    "    if ref_run in refRuns:\n",
    "        continue\n",
    "    refRuns.append(ref_run)\n",
    "    \n",
    "    # Get the runs associated with found reference\n",
    "    outputRuns = {}\n",
    "    outputBad = {}\n",
    "    for run in rundict:\n",
    "        tempRef = run['reference_run_number']\n",
    "        if tempRef == ref_run:\n",
    "            runls = {}\n",
    "            runls[str(run['run_number'])] = [[-1]]\n",
    "            if run['run_number'] in badruns[year+era]:\n",
    "                if run['run_number'] == 303824:\n",
    "                    continue\n",
    "                print('Found bad run :' + str(run))\n",
    "                outputBad.update(runls)\n",
    "            else:\n",
    "                outputRuns.update(runls)\n",
    "    \n",
    "    # Perform structuring for compatibility with autoencoders\n",
    "    dataDict[year + era] = outputRuns\n",
    "    badrunsls[year + era] = outputBad\n",
    "    trainrunsls[year + era] = {}\n",
    "    goodrunsls[year + era] = {}\n",
    "    refrunsls[year + era] = {}\n",
    "    refrunsls[year + era][str(ref_run)] = [[-1]]\n",
    "    \n",
    "    # Select training and testing set\n",
    "    for i,run in enumerate(dataDict[year + era]):\n",
    "        if i > 5 and i < 11:\n",
    "            goodrunsls[year + era][str(run)] = [[-1]]\n",
    "        else:\n",
    "            trainrunsls[year + era][str(run)] = [[-1]]\n",
    "\n",
    "if len(years) != 1: raise Exception('Year of length 0 or >1 unimplemented!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dd9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Controls and Selection - 1D Autoncoder\n",
    "\n",
    "# The directory data is located in\n",
    "datadir = {}\n",
    "for era in eras:\n",
    "    datadir[year + era] = '../data/' + year + era + '/'\n",
    "\n",
    "# Create a list of histograms to include\n",
    "# Pair histograms to be combined on the same line\n",
    "histnames = [['chargeInner_PXLayer_1', 'chargeInner_PXLayer_2', 'chargeInner_PXLayer_3', 'chargeInner_PXLayer_4', 'chargeOuter_PXLayer_1', 'chargeOuter_PXLayer_2', 'chargeOuter_PXLayer_3', 'chargeOuter_PXLayer_4', 'size_PXLayer_1', 'size_PXLayer_2', 'size_PXLayer_3', 'size_PXLayer_4'], ['charge_PXDisk_+1', 'charge_PXDisk_+2', 'charge_PXDisk_+3', 'size_PXDisk_+1', 'size_PXDisk_+2', 'size_PXDisk_+3'], ['charge_PXDisk_-1', 'charge_PXDisk_-2', 'charge_PXDisk_-3', 'size_PXDisk_-1', 'size_PXDisk_-2', 'size_PXDisk_-3'], ['Summary_ClusterStoNCorr__OnTrack__TIB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4'], ['Summary_ClusterStoNCorr__OnTrack__TOB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__4', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__5', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__6'], ['Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9'], ['Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9'], ['NumberOfRecHitsPerTrack_lumiFlag_GenTk', 'goodvtxNbr'], ['num_clusters_ontrack_PXBarrel', 'num_clusters_ontrack_PXForward']]\n",
    "\n",
    "#Read new data or use previously saved data & should data be saved\n",
    "readnew = True\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98ac2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Run Properties - Combined Autoencoder\n",
    "# in this cell all major run properties are going to be set,\n",
    "\n",
    "# Set whether to train globally or locally or in a development/testing mode\n",
    "training_mode = 'development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a9f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Controls and Selection - 1D Autoencoder\n",
    "\n",
    "plotNames = 'Test'\n",
    "name = plotNames+'plots'\n",
    "\n",
    "# Choose whether to train a new model or load one\n",
    "trainnew = True\n",
    "savemodel = True\n",
    "modelname = plotNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b28091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected runs/lumisections for training: \n",
      "{'303819': [[-1]], '303999': [[-1]], '304119': [[-1]], '304120': [[-1]], '304197': [[-1]], '304505': [[-1]], '304449': [[-1]], '304452': [[-1]], '304508': [[-1]], '304625': [[-1]], '304655': [[-1]], '304737': [[-1]], '304778': [[-1]], '306459': [[-1]], '304196': [[-1]], '305310': [[-1]], '305040': [[-1]], '305043': [[-1]], '305185': [[-1]], '305204': [[-1]], '305234': [[-1]], '305376': [[-1]], '306042': [[-1]], '306051': [[-1]], '305406': [[-1]], '306122': [[-1]], '306134': [[-1]], '306137': [[-1]], '306154': [[-1]], '306170': [[-1]], '306417': [[-1]], '306432': [[-1]], '306456': [[-1]], '305516': [[-1]], '305586': [[-1]], '305588': [[-1]], '305590': [[-1]], '305809': [[-1]], '305832': [[-1]], '305840': [[-1]], '305898': [[-1]], '306029': [[-1]], '306037': [[-1]], '306095': [[-1]]}\n",
      "selected runs/lumisections as good test set:\n",
      "{'304198': [[-1]], '304199': [[-1]], '304209': [[-1]], '304333': [[-1]], '304446': [[-1]], '305247': [[-1]], '305313': [[-1]], '305338': [[-1]], '305350': [[-1]], '305364': [[-1]]}\n",
      "selected runs/lumisections as bad test set:\n",
      "{'303989': [[-1]], '304740': [[-1]], '305250': [[-1]], '306422': [[-1]], '305249': [[-1]]}\n",
      "selected/runs/lumisections as reference set:\n",
      "{'304158': [[-1]], '306459': [[-1]]}\n"
     ]
    }
   ],
   "source": [
    "### Define Training Mode Parameters - Combined Autoencoder\n",
    "if training_mode == 'global':\n",
    "    runsls_training = None # use none to not add a mask for training (can e.g. use DCS-bit on mask)\n",
    "    runsls_good = None # use none to not add a mask for good runs (can e.g. use averages of training set)\n",
    "    runsls_bad = badrunsls[year] # predefined bad runs\n",
    "    print('selected runs/lumisections for training: all')\n",
    "    \n",
    "elif training_mode == 'local':\n",
    "    # train locally on a small set of runs\n",
    "    # - either on n runs preceding a chosen application run,\n",
    "    # - or on the run associated as reference to the chosen application run.\n",
    "    # - this only works for a single era\n",
    "    \n",
    "    available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+era+'_'+histnames[0][0]+'.csv') ) )\n",
    "    # Cherry picked really bad run\n",
    "    run_application = 299316\n",
    "    #run_application = 299317\n",
    "    run_application_index = available_runs.index(run_application)\n",
    "    # select training set\n",
    "    usereference = False\n",
    "    if usereference:\n",
    "        run_reference = rru.get_reference_run( run_application, jsonfile='../utils/json_allRunsRefRuns.json' )\n",
    "        if run_reference<0:\n",
    "            raise Exception('no valid reference run has been defined for run {}'.format(run_application))\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(run_reference,[-1])])\n",
    "    else:\n",
    "        ntraining = 5\n",
    "        offset = 0 # normal case: offset = 0 (just use 5 previous runs)\n",
    "        \n",
    "        # Selects the 5 previous runs for training\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining-offset:run_application_index-offset]])\n",
    "    #runsls_bad = badrunsls[year]\n",
    "    #runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_bad = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_good = runsls_training\n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)\n",
    "        \n",
    "elif training_mode == 'development':\n",
    "    # train on a user-defined subset of runs\n",
    "    \n",
    "   # Select runs to be used in training from the user-defined list\n",
    "    runsls_training = {}\n",
    "    runsls_bad = {}\n",
    "    runsls_good = {}\n",
    "    runsls_ref = {}\n",
    "    for era in eras:\n",
    "        runsls_training.update(trainrunsls[year + era])\n",
    "        # Select bad runs to test on in the user-defined list\n",
    "        runsls_bad.update(badrunsls[year + era])\n",
    "        # Select good runs to test on in the user-defined list\n",
    "        runsls_good.update(goodrunsls[year + era])\n",
    "        \n",
    "        runsls_ref.update(refrunsls[year+era])\n",
    "    \n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)\n",
    "    print('selected/runs/lumisections as reference set:')\n",
    "    print(runsls_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58bc956b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Classifiers and masks cleared to preserve consistency\n",
      "Adding chargeInner_PXLayer_1E...\n",
      "Adding chargeInner_PXLayer_2E...\n",
      "Adding chargeInner_PXLayer_3E...\n",
      "Adding chargeInner_PXLayer_4E...\n",
      "Adding chargeOuter_PXLayer_1E...\n",
      "Adding chargeOuter_PXLayer_2E...\n",
      "Adding chargeOuter_PXLayer_3E...\n",
      "Adding chargeOuter_PXLayer_4E...\n",
      "Adding size_PXLayer_1E...\n",
      "Adding size_PXLayer_2E...\n",
      "Adding size_PXLayer_3E...\n",
      "Adding size_PXLayer_4E...\n",
      "Adding charge_PXDisk_+1E...\n",
      "Adding charge_PXDisk_+2E...\n",
      "Adding charge_PXDisk_+3E...\n",
      "Adding size_PXDisk_+1E...\n",
      "Adding size_PXDisk_+2E...\n",
      "Adding size_PXDisk_+3E...\n",
      "Adding charge_PXDisk_-1E...\n",
      "Adding charge_PXDisk_-2E...\n",
      "Adding charge_PXDisk_-3E...\n",
      "Adding size_PXDisk_-1E...\n",
      "Adding size_PXDisk_-2E...\n",
      "Adding size_PXDisk_-3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9E...\n",
      "Adding NumberOfRecHitsPerTrack_lumiFlag_GenTkE...\n",
      "Adding goodvtxNbrE...\n",
      "Adding num_clusters_ontrack_PXBarrelE...\n",
      "Adding num_clusters_ontrack_PXForwardE...\n",
      "Found 10778 histograms\n",
      "\n",
      "Adding chargeInner_PXLayer_1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeInner_PXLayer_2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeInner_PXLayer_3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeInner_PXLayer_4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeOuter_PXLayer_1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeOuter_PXLayer_2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeOuter_PXLayer_3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding chargeOuter_PXLayer_4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXLayer_1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXLayer_2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXLayer_3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXLayer_4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_+1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_+2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_+3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_+1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_+2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_+3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_-1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_-2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding charge_PXDisk_-3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_-1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_-2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding size_PXDisk_-3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__5F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__6F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9F...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding NumberOfRecHitsPerTrack_lumiFlag_GenTkF...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding goodvtxNbrF...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding num_clusters_ontrack_PXBarrelF...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Adding num_clusters_ontrack_PXForwardF...\n",
      "WARNING: Histogram already in HistStruct. Error checking is disabled, so ensure histograms have same features.\n",
      "Found 27164 histograms\n",
      "\n",
      "Created a histstruct with the following properties:\n",
      "- number of histogram types: 62\n",
      "- number of lumisections: 27164\n"
     ]
    }
   ],
   "source": [
    "### Data Import\n",
    "\n",
    "# Create a new HistStruct from the data\n",
    "if readnew:\n",
    "    # Initializations\n",
    "    dloader = DataLoader.DataLoader()\n",
    "    histstruct = FlexiStruct.FlexiStruct()\n",
    "    histstruct.reset_histlist(histnames)\n",
    "    \n",
    "    # Unpack histnames and add every histogram individually\n",
    "    for era in eras:\n",
    "        for histnamegroup in histnames:\n",
    "            for histname in histnamegroup:\n",
    "                print('Adding {}...'.format(histname + era))\n",
    "                \n",
    "                # Bring the histograms into memory from storage for later use\n",
    "                filename = datadir[year+era] + 'DF' + year + era + '_' + histname + '.csv'\n",
    "                df = dloader.get_dataframe_from_file( filename )\n",
    "                \n",
    "                # In case of local training, we can remove most of the histograms\n",
    "                if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ):\n",
    "                    runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad, runsls_ref) for k, v in d.items()}\n",
    "                    df = dfu.select_runsls( df, runsls_total )    \n",
    "                \n",
    "                df = dfu.rm_duplicates(df)\n",
    "                # Store the data in the histstruct object managing this whole thing\n",
    "                histstruct.add_dataframe( df, rebinningfactor = 1, standardbincount = 102 )\n",
    "        print('Found {} histograms\\n'.format(len(histstruct.runnbs)))\n",
    "\n",
    "# Load a previously saved HistStruct\n",
    "else:\n",
    "    # Load histstruct from storage\n",
    "    histstruct = SubHistStruct.SubHistStruct.load( 'histstruct_global_20220201.zip', verbose=False )\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if ('bad' in name and name!='bad')])\n",
    "    \n",
    "    print('loaded a histstruct with the following properties:')\n",
    "    print(histstruct)\n",
    "    # Count of bad runs, presumably for later use\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name])\n",
    "    \n",
    "print('Created a histstruct with the following properties:')\n",
    "print('- number of histogram types: {}'.format(len(histstruct.histnames)))\n",
    "print('- number of lumisections: {}'.format(len(histstruct.lsnbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33327278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'303989': [[-1]], '304740': [[-1]], '305250': [[-1]], '306422': [[-1]], '305249': [[-1]]}\n",
      "Assigned masks: ['dcson', 'golden', 'highstat', 'lowstat', 'training', 'good', 'ref', 'bad', 'bad0', 'bad1', 'bad2', 'bad3', 'bad4']\n"
     ]
    }
   ],
   "source": [
    "### Add Masks to Data\n",
    "\n",
    "if readnew:\n",
    "    histstruct.add_dcsonjson_mask( 'dcson' )\n",
    "    histstruct.add_goldenjson_mask('golden' )\n",
    "    histstruct.add_highstat_mask( 'highstat', entries_to_bins_ratio=0 )\n",
    "    histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=0 )\n",
    "    if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training )\n",
    "    if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good )\n",
    "    if runsls_ref is not None: histstruct.add_json_mask('ref', runsls_ref)\n",
    "        \n",
    "    # Distinguishing bad runs\n",
    "    nbadruns = 0\n",
    "    if runsls_bad is not None:\n",
    "        print(runsls_bad)\n",
    "        histstruct.add_json_mask( 'bad', runsls_bad )\n",
    "        \n",
    "        # Special case for bad runs: add a mask per run (different bad runs have different characteristics)\n",
    "        nbadruns = len(runsls_bad.keys())\n",
    "        for i,badrun in enumerate(runsls_bad.keys()):\n",
    "            histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} )\n",
    "            \n",
    "    if save:\n",
    "        histstruct.save('test.pk1')\n",
    "print('Assigned masks: {}'.format(list(histstruct.masks.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0565c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trains a combined autoencoder for every merge set\n",
    "def load_concatamash_autoencoder(histstruct, histnames):\n",
    "    \n",
    "    for i in range(len(histnames)):\n",
    "        print('Now loading model ' + str(i))\n",
    "        classifier = AutoEncoder.AutoEncoder(model=keras.models.load_model('../SavedModels/Concatamash/AE{}/'.format(i)))\n",
    "        histstruct.add_classifier(histnames[i][0], classifier) \n",
    "        K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a109e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading model 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 18:54:39.310740: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.3-88592/x86_64-centos7-gcc11-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.3-35f7a/x86_64-centos7-gcc11-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jaxlib/mlir/_mlir_libs:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/onnxruntime/capi/:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/11.0.14p1-8284a/x86_64-centos7-gcc11-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.37-355ed/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/4.1.2-f9ee4/x86_64-centos7-gcc11-opt/lib64/R/library/readr/rcon\n",
      "2022-08-02 18:54:39.311023: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-02 18:54:39.311104: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-khowey): /proc/driver/nvidia/version does not exist\n",
      "2022-08-02 18:54:39.312739: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now loading model 1\n",
      "Now loading model 2\n",
      "Now loading model 3\n",
      "Now loading model 4\n",
      "Now loading model 5\n",
      "Now loading model 6\n",
      "Now loading model 7\n",
      "Now loading model 8\n",
      "Now loading model 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"concatenate_9\" (type Concatenate).\n\nA merge layer should be called on a list of inputs. Received: inputs=Tensor(\"Placeholder:0\", shape=(None, 102), dtype=float32) (not a list of tensors)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 102), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106876/2079726797.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mload_concatamash_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiststruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_106876/997406222.py\u001b[0m in \u001b[0;36mload_concatamash_autoencoder\u001b[0;34m(histstruct, histnames)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Now loading model '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../SavedModels/Concatamash/AE{}/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mhiststruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m    121\u001b[0m           \u001b[0;34m'A merge layer should be called on a list of inputs. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m           f'Received: inputs={inputs} (not a list of tensors)')\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"concatenate_9\" (type Concatenate).\n\nA merge layer should be called on a list of inputs. Received: inputs=Tensor(\"Placeholder:0\", shape=(None, 102), dtype=float32) (not a list of tensors)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(None, 102), dtype=float32)"
     ]
    }
   ],
   "source": [
    "load_concatamash_autoencoder(histstruct, histnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the Models for WP definition\n",
    "def evaluate_models_train(histstruct):\n",
    "    \n",
    "    for histgroup in histnames:\n",
    "        print('evaluating model for '+histgroup[0])\n",
    "        print(histstruct.evaluate_classifier(histgroup)[0].shape)\n",
    "    \n",
    "    # get mse for training set\n",
    "    if 'training' in histstruct.masks.keys(): masknames = ['dcson','highstat', 'training']\n",
    "    else: masknames = ['dcson','highstat']\n",
    "    mse_train = histstruct.get_scores_array( masknames=masknames )\n",
    "    print('Found mse array for training set of following shape: {}'.format(mse_train.shape))\n",
    "    \n",
    "    if 'ref' in histstruct.masks.keys():\n",
    "        mse_ref = []\n",
    "        for histname in histstruct.histnames:\n",
    "            mse_ref.append(histstruct.get_scores( histname=histname, masknames=['dcson','highstat','ref'] ))\n",
    "        mse_ref = np.array(mse_ref)\n",
    "        mse_ref = np.transpose(mse_ref)\n",
    "        print('Found mse array for reference set of following shape: {}'.format(mse_ref.shape))\n",
    "    \n",
    "    # get mse for good set\n",
    "    if 'good' in histstruct.masks.keys():\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            mse_good.append(histstruct.get_scores( histname=histname, masknames=['dcson','highstat','good'] ))\n",
    "    else:\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            hists_good = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), 1000 )\n",
    "            thismse = histstruct.classifiers[histname].evaluate( hists_good )\n",
    "            mse_good.append( thismse )\n",
    "            print(run)\n",
    "    mse_good = np.array(mse_good)\n",
    "    mse_good = np.transpose(mse_good)\n",
    "    print('Found mse array for good set of following shape: {}'.format(mse_good.shape))\n",
    "    \n",
    "    # get mse for bad sets\n",
    "    mse_bad = []\n",
    "    for i in range(nbadruns):\n",
    "        mse_bad.append( histstruct.get_scores_array( masknames=['dcson','highstat','bad{}'.format(i)] ) )\n",
    "        print('Found mse array for bad set of following shape: {}'.format(mse_bad[i].shape))\n",
    "        \n",
    "    return [mse_train, mse_good, mse_bad, mse_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mse_train, mse_good_eval, mse_bad_eval, mse_ref) = evaluate_models_train(histstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c53b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluates Autoencoders based on raw MSEs\n",
    "def concatamash_mse_comparison(mse_train_eval, mse_good_eval, mse_bad_eval, mse_ref_eval):\n",
    "    mse_train = np.transpose(mse_train_eval)\n",
    "    mse_good = np.transpose(mse_good_eval)\n",
    "    mse_bad = []\n",
    "    for bad in mse_bad_eval:\n",
    "        mse_bad.append(np.transpose(bad))\n",
    "    mse_ref = np.transpose(mse_ref_eval)\n",
    "    \n",
    "    delta_good_list = []\n",
    "    delta_bad_list = []\n",
    "    delta_train_list = []\n",
    "    for i in range(len(mse_ref)):\n",
    "        delta_good_list.append(np.mean(mse_good[i]) - np.mean(mse_ref[i]))\n",
    "        delta_train_list.append(np.mean(mse_train[i]) - np.mean(mse_ref[i]))\n",
    "        for j in range(len(mse_bad)):\n",
    "            delta_bad_list.append(np.mean(mse_bad[j][i]) - np.mean(mse_ref[i]))\n",
    "            \n",
    "    delta_good = np.mean(np.array(delta_good_list))\n",
    "    delta_bad = np.mean(np.array(delta_bad_list))\n",
    "    delta_train = np.mean(np.array(delta_train_list))\n",
    "    \n",
    "    three_sigma = 3*np.std(mse_ref)\n",
    "    \n",
    "    mse_bad = np.concatenate(mse_bad_eval)\n",
    "    \n",
    "    print(mse_ref_eval.shape)\n",
    "    print(mse_good_eval.shape)\n",
    "    print(mse_bad.shape)\n",
    "    print(mse_train_eval.shape)\n",
    "    \n",
    "    scores = np.vstack([mse_ref_eval, mse_train_eval, mse_good_eval, mse_bad])\n",
    "    print(scores.shape)\n",
    "    labels = np.hstack([np.zeros(len(mse_ref_eval) + len(mse_train_eval) + len(mse_good_eval)), np.ones(len(mse_bad))])\n",
    "    print(labels.shape)\n",
    "    \n",
    "    (fig, ax) = pu.plot_score_dist(scores, labels, siglabel='anomalous', sigcolor='r', \n",
    "                           bcklabel='good', bckcolor='g', \n",
    "                           nbins=200, normalize=True,\n",
    "                           xaxtitle='negative logarithmic probability',\n",
    "                           yaxtitle='number of lumisections (normalized)', doshow=False)\n",
    "    \n",
    "    plt.axvline(x=-np.mean(mse_ref_eval), color='b', label='WP', linestyle='-')\n",
    "    plt.axvline(x=-np.mean(mse_ref_eval) + three_sigma, color='k', label='Max', linestyle='-.')\n",
    "    plt.axvline(x=-np.mean(mse_ref_eval) - three_sigma, color='m', label='Min', linestyle='--')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(delta_good)\n",
    "    print(delta_bad)\n",
    "    print(delta_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ce995",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatamash_mse_comparison(mse_train, mse_good_eval, mse_bad_eval, mse_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots and Distribution Analysis\n",
    "def fit_mse_distribution(histstruct, mse_train):\n",
    "    dimslist = []\n",
    "    fitfunclist = []\n",
    "    \n",
    "    \n",
    "    nhisttypes = len(histstruct.histnames)\n",
    "    for i in range(0,nhisttypes-1):\n",
    "        for j in range(i+1,nhisttypes):\n",
    "            dimslist.append((i, j))\n",
    "    \n",
    "    plt.close('all')\n",
    "    (npoints,ndims) = mse_train.shape\n",
    "    \n",
    "    \n",
    "    # settings for GaussianKdeFitter\n",
    "    scott_bw = npoints**(-1./(ndims+4))\n",
    "    bw_method = 20*scott_bw\n",
    "    # settings for HyperRectangleFitter\n",
    "    quantiles = ([0.00062,0.0006,0.00015,0.00015,\n",
    "                 0.0003,0.0003,0.00053,0.00065])\n",
    "    \n",
    "    \n",
    "    #for dims in dimslist:\n",
    "    #    thismse = mse_train[:,dims]\n",
    "    #    if training_mode=='global': \n",
    "    #        fitfunc = SeminormalFitter.SeminormalFitter(thismse)\n",
    "    #        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(thismse, \n",
    "    #        #                                                    [quantiles[dims[0]],quantiles[dims[1]]],\n",
    "    #        #                                                    'up')\n",
    "    #    else: fitfunc = GaussianKdeFitter.GaussianKdeFitter(thismse,bw_method=bw_method)\n",
    "    #    #pu.plot_fit_2d(thismse, fitfunc=fitfunc, logprob=True, clipprob=True,\n",
    "    #    #                onlycontour=False, xlims=30, ylims=30, \n",
    "    #    #                onlypositive=True, transparency=0.5,\n",
    "    #    #                xaxtitle=histstruct.histnames[dims[0]], \n",
    "    #    #                yaxtitle=histstruct.histnames[dims[1]],\n",
    "    #    #                title='density fit of lumisection MSE')\n",
    "    #    ##plt.close('all') # release plot memory\n",
    "    #    fitfunclist.append(fitfunc)\n",
    "    # \n",
    "    #    \n",
    "    if training_mode=='global': \n",
    "        fitfunc = SeminormalFitter.SeminormalFitter(mse_train)\n",
    "        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(mse_train, quantiles, 'up')\n",
    "    else: \n",
    "        fitfunc = GaussianKdeFitter.GaussianKdeFitter()\n",
    "        fitfunc.fit(mse_ref,bw_method=bw_method)\n",
    "    \n",
    "    return fitfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitfunc = fit_mse_distribution(histstruct, mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d55762",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare MSEs for Working Point Definition\n",
    "def mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc):\n",
    "    \n",
    "    # Get the minimum log probability of histograms in good set\n",
    "    print('--- good lumesections ---')\n",
    "    logprob_good = np.log(fitfunc.pdf(mse_ref))\n",
    "    print('length of log prob array: '+str(len(logprob_good)))\n",
    "    print('minimum of log prob: '+str(np.min(logprob_good)))\n",
    "    #print(sorted(logprob_good))\n",
    "    \n",
    "    print('--- bad lumisections ---')\n",
    "    logprob_bad_parts = [np.log(fitfunc.pdf(mse_bad_eval[j])) for j in range(len(mse_bad_eval))]\n",
    "\n",
    "    #for lp in logprob_bad_parts: print(str(sorted(lp))+'\\n\\n')\n",
    "    logprob_bad = np.concatenate(tuple(logprob_bad_parts))\n",
    "    \n",
    "    print('length of log prob array: '+str(len(logprob_bad)))\n",
    "    print('maximum of log prob: '+str(np.max(logprob_bad)))\n",
    "    #print(sorted(logprob_good))\n",
    "    #print(sorted(logprob_bad))\n",
    "    #print(logprob_bad)\n",
    "    \n",
    "    sep = np.min(logprob_good) - np.max(logprob_bad)\n",
    "    print('Separability: ' + str(sep))\n",
    "    \n",
    "    return [logprob_good, logprob_bad, logprob_bad_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "(logprob_good, logprob_bad, logprob_bad_parts) = mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor):\n",
    "    labels_good = np.zeros(len(logprob_good)) # background: label = 0\n",
    "    labels_bad = np.ones(len(logprob_bad)) # signal: label = 1\n",
    "    \n",
    "    badMin = min(np.where(logprob_bad != -np.inf, logprob_bad, -1))\n",
    "    goodMax = max(np.where(logprob_good != np.inf, logprob_good, 10001))\n",
    "    \n",
    "    logprob_good = np.where(logprob_good != np.inf, logprob_good, goodMax)\n",
    "    logprob_bad = np.where(logprob_bad != -np.inf, logprob_bad, badMin)\n",
    "    \n",
    "    # These only take effect if a histogram is grossly misclassified\n",
    "    logprob_good[logprob_good == -np.inf] = badMin\n",
    "    logprob_bad[logprob_bad == np.inf] = goodMax\n",
    "    \n",
    "    labels = np.loadtxt('./Labels.csv', delimiter = ',')\n",
    "    labels = np.concatenate(tuple([labels_good, labels_bad]))\n",
    "    scores = np.concatenate(tuple([-logprob_good,-logprob_bad]))\n",
    "    scores = aeu.clip_scores( scores )\n",
    "    \n",
    "    avSep = np.mean(logprob_good) - np.mean(logprob_bad)\n",
    "    \n",
    "    print('Average Separation: ' + str(avSep))\n",
    "    \n",
    "    goodPos = np.mean(logprob_good)\n",
    "    badPos = np.mean(logprob_bad)\n",
    "    \n",
    "    # Setting a threshold, below this working point defines anomalous data\n",
    "    # Average is biased towards better recall per user specifications\n",
    "    logprob_threshold = (1/(wpBiasFactor + 1)) * (wpBiasFactor*np.mean(logprob_good) + np.mean(logprob_bad))\n",
    "    # Or set manual\n",
    "    #logprob_threshold = 100\n",
    "    \n",
    "    (fig, ax) = pu.plot_score_dist(scores, labels, siglabel='anomalous', sigcolor='r', \n",
    "                       bcklabel='good', bckcolor='g', \n",
    "                       nbins=200, normalize=True,\n",
    "                       xaxtitle='negative logarithmic probability',\n",
    "                       yaxtitle='number of lumisections (normalized)', doshow=False)\n",
    "    \n",
    "    plt.axvline(x=-logprob_threshold, color='b', label='WP')\n",
    "    plt.show()\n",
    "      \n",
    "    # Plot ROC curve for analysis\n",
    "    auc = aeu.get_roc(scores, labels, mode='geom', doprint=False)\n",
    "    \n",
    "    mis = np.where((labels==1) & (scores<-logprob_threshold),1,0).astype(bool)\n",
    "    labels2 = np.where(mis, 0, labels)\n",
    "    mis = np.where((labels2==0) & (scores>-logprob_threshold),1,0).astype(bool)\n",
    "    labels2 = np.where(mis, 1, labels2)\n",
    "    np.savetxt('./Labels.csv', labels2, delimiter=',')\n",
    "    \n",
    "    (_, _, _, tp, fp, tn, fn) = aeu.get_confusion_matrix(scores,labels,-logprob_threshold)\n",
    "    print('Selected logprob threshold of ' + str(logprob_threshold))\n",
    "    \n",
    "    # Get metrics for analysis\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_measure = (1 + fmBiasFactor * fmBiasFactor) * ((precision * recall) / ((fmBiasFactor * fmBiasFactor * precision) + recall)) \n",
    "    \n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Precision: ' + str(precision))\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('F-Measure: ' + str(f_measure))\n",
    "    \n",
    "    return logprob_threshold, goodPos, badPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6992dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_threshold, goodPos, badPos = evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f854d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoders_individual(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor, logprob_bad_parts, goodPos, badPos):\n",
    "    for i,logprob_bad in enumerate(logprob_bad_parts):\n",
    "        \n",
    "        labels_good = np.zeros(len(logprob_good)) # background: label = 0\n",
    "        labels_bad = np.ones(len(logprob_bad)) # signal: label = 1\n",
    "        \n",
    "        badMin = min(np.where(logprob_bad != -np.inf, logprob_bad, -1))\n",
    "        goodMax = max(np.where(logprob_good != np.inf, logprob_good, 10001))\n",
    "        \n",
    "        logprob_good = np.where(logprob_good != np.inf, logprob_good, goodMax)\n",
    "        logprob_bad = np.where(logprob_bad != -np.inf, logprob_bad, badMin)\n",
    "        \n",
    "        # These only take effect if a histogram is grossly misclassified\n",
    "        logprob_good[logprob_good == -np.inf] = badMin\n",
    "        logprob_bad[logprob_bad == np.inf] = goodMax\n",
    "        \n",
    "        labels = labels_bad\n",
    "        scores = -logprob_bad\n",
    "        scores = aeu.clip_scores( scores )\n",
    "    \n",
    "        print('Showing Data from Bad Run ' + str(i))\n",
    "        (fig, ax) = pu.plot_score_dist(scores, labels, siglabel='anomalous', sigcolor='m', \n",
    "                           bcklabel='good', bckcolor='k', \n",
    "                           nbins=200, normalize=True,\n",
    "                           xaxtitle='negative logarithmic probability',\n",
    "                           yaxtitle='number of lumisections (normalized)', doshow=False)\n",
    "        \n",
    "        plt.axvline(x=-logprob_threshold, color='b', label='WP', linestyle='-')\n",
    "        plt.axvline(x=-goodPos, color='g', label='Good Avg', linestyle='-.')\n",
    "        plt.axvline(x=-badPos, color='r', label='Bad Avg', linestyle='--')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e8577",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_autoencoders_individual(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor, logprob_bad_parts, goodPos, badPos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
