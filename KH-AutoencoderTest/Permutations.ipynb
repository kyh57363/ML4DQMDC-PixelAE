{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50f6f0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'HyperRectangleFitter' from '/eos/home-i01/k/khowey/SWAN_projects/ML4DQMDC-PixelAE/KH-AutoencoderTest/../src/cloudfitters/HyperRectangleFitter.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "from os.path import exists\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import importlib\n",
    "\n",
    "# local modules\n",
    "sys.path.append('../utils')\n",
    "import csv_utils as csvu\n",
    "import json_utils as jsonu\n",
    "import dataframe_utils as dfu\n",
    "import hist_utils as hu\n",
    "import autoencoder_utils as aeu\n",
    "import plot_utils as pu\n",
    "import generate_data_utils as gdu\n",
    "import refruns_utils as rru\n",
    "importlib.reload(csvu)\n",
    "importlib.reload(jsonu)\n",
    "importlib.reload(dfu)\n",
    "importlib.reload(hu)\n",
    "importlib.reload(aeu)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(gdu)\n",
    "importlib.reload(rru)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/classifiers')\n",
    "sys.path.append('../src/cloudfitters')\n",
    "import HistStruct\n",
    "importlib.reload(HistStruct)\n",
    "import SubHistStruct\n",
    "importlib.reload(SubHistStruct)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "import AutoEncoder\n",
    "importlib.reload(AutoEncoder)\n",
    "import SeminormalFitter\n",
    "import GaussianKdeFitter\n",
    "import HyperRectangleFitter\n",
    "importlib.reload(SeminormalFitter)\n",
    "importlib.reload(GaussianKdeFitter)\n",
    "importlib.reload(HyperRectangleFitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d16d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2017'\n",
    "era = 'B'\n",
    "\n",
    "datadir = '../data/' + year + era + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e93aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "blk1Vars = ['chargeInner', 'chargeOuter', 'adc', 'size']\n",
    "blk2Vars = ['NormalizedHitResiduals', \n",
    "            'Summary_ClusterStoNCorr__OnTrack_',\n",
    "            #'Summary_TotalNumberOfDigis_'\n",
    "           ]\n",
    "blk3Vars = [\n",
    "    #'NumberOfTracks', \n",
    "    'NumberOfRecHitsPerTrack', \n",
    "    'Chi2oNDF',\n",
    "    'goodvtxNbr'] \n",
    "miscVars = [\n",
    "    #'NumberOfClustersInPixel', \n",
    "    'num_clusters_ontrack_PXBarrel', \n",
    "    'num_clusters_ontrack_PXForward', \n",
    "    #'NumberOfClustersInStrip'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0517e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the different permutations for block 1\n",
    "combosBlk1 = []\n",
    "histcount = 0\n",
    "modelcount = 0\n",
    "for size in range(1, len(blk1Vars) + 1):\n",
    "    # Get every combination of given size\n",
    "    thisList = list(itertools.combinations(blk1Vars, size))\n",
    "    \n",
    "    \n",
    "    ## Applying rules\n",
    "    for item in thisList:\n",
    "        \n",
    "        if 'chargeInner' not in item: continue\n",
    "        \n",
    "        subList = []\n",
    "        subListPX = []\n",
    "        subListDSP = []\n",
    "        subListDSN = []\n",
    "        # Getting individual histograms to set appropriate names\n",
    "        for element in item:\n",
    "            \n",
    "            # Need to treat chargeInner the same as 'charge' for disks\n",
    "            if element == 'chargeInner':\n",
    "                for i in range(1, 4):\n",
    "                    subListDSP.append('charge_PXDisk_+' + str(i))\n",
    "                    subListDSN.append('charge_PXDisk_-' + str(i))\n",
    "                    \n",
    "            elif element != 'chargeOuter':\n",
    "                for i in range(1, 4):\n",
    "                    subListDSP.append(element + '_PXDisk_+' + str(i))\n",
    "                    subListDSN.append(element + '_PXDisk_-' + str(i))\n",
    "            # PXlayers\n",
    "            for i in range(1, 5):\n",
    "                subListPX.append(element + '_PXLayer_' + str(i))\n",
    "        \n",
    "        subList.append(subListPX)\n",
    "        subList.append(subListDSP)\n",
    "        subList.append(subListDSN)\n",
    "        \n",
    "        combosBlk1.append(subList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66dce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Summary_ClusterStoNCorr__OnTrack__TIB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4'], ['Summary_ClusterStoNCorr__OnTrack__TOB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__4', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__5', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__6'], ['Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9'], ['Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9']]\n"
     ]
    }
   ],
   "source": [
    "### Permutations for block 2\n",
    "combosBlk2 = []\n",
    "for size in range(1, len(blk2Vars) + 1):\n",
    "    # Get every combination of given size\n",
    "    thisList = list(itertools.combinations(blk2Vars, size))\n",
    "    \n",
    "    \n",
    "    ## Applying rules\n",
    "    for item in thisList:\n",
    "        if 'Summary_ClusterStoNCorr__OnTrack_' not in item: continue\n",
    "            \n",
    "        subList = []\n",
    "        subTIB = []\n",
    "        subTOB = []\n",
    "        subTIDP = []\n",
    "        subTIDN = []\n",
    "        subTECP = []\n",
    "        subTECN = []\n",
    "        \n",
    "        # Getting individual histograms to set appropriate names\n",
    "        for element in item:\n",
    "            \n",
    "            # Special case\n",
    "            if element != 'NormalizedHitResiduals':\n",
    "                for i in range(1, 10):\n",
    "                    subTECN.append(element + '_TEC__MINUS__wheel__' + str(i))\n",
    "                    subTECP.append(element + '_TEC__PLUS__wheel__' + str(i))\n",
    "                    \n",
    "                for i in range(1, 4):\n",
    "                    subTIDN.append(element + '_TID__MINUS__wheel__' + str(i))\n",
    "                    subTIDP.append(element + '_TID__PLUS__wheel__' + str(i))\n",
    "                    \n",
    "                for i in range(1, 5):\n",
    "                    subTIB.append(element + '_TIB__layer__' + str(i))\n",
    "            \n",
    "                for i in range(1, 7):\n",
    "                    subTOB.append(element + '_TOB__layer__' + str(i))\n",
    "                    \n",
    "            else: \n",
    "                for i  in range(1, 10):\n",
    "                    subTECN.append(element + '_TEC__wheel__' + str(i))\n",
    "                    \n",
    "                for i in range(1, 4):\n",
    "                    subTIDN.append(element + '_TID__wheel__' + str(i))\n",
    "                    \n",
    "                for i in range(1, 5):\n",
    "                    subTIB.append(element + '_TIB__Layer__' + str(i))\n",
    "            \n",
    "                for i in range(1, 7):\n",
    "                    subTOB.append(element + '_TOB__Layer__' + str(i))\n",
    "\n",
    "        subList.append(subTIB)\n",
    "        subList.append(subTOB)\n",
    "        if len(subTIDP) > 0:\n",
    "            subList.append(subTIDP)\n",
    "        subList.append(subTIDN)\n",
    "        if len(subTECP) > 0:\n",
    "            subList.append(subTECP)\n",
    "        subList.append(subTECN)\n",
    "        combosBlk2.append(subList)\n",
    "\n",
    "print(combosBlk2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044be4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NumberOfRecHitsPerTrack_lumiFlag_GenTk']]\n"
     ]
    }
   ],
   "source": [
    "### Permutations for block 3\n",
    "combosBlk3 = []\n",
    "\n",
    "for size in range(1, len(blk3Vars) + 1):\n",
    "    # Get every combination of given size\n",
    "    thisList = list(itertools.combinations(blk3Vars, size))\n",
    "    \n",
    "    ## Applying rules\n",
    "    for item in thisList:\n",
    "        if 'NumberOfRecHitsPerTrack' not in item: continue\n",
    "         \n",
    "        subList = []\n",
    "        for element in item:\n",
    "            if element !='goodvtxNbr':\n",
    "                subList.append(element + '_lumiFlag_GenTk')\n",
    "            else:\n",
    "                subList.append(element)\n",
    "        \n",
    "        combosBlk3.append([subList])\n",
    "\n",
    "print(combosBlk3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6052c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "### Permutations for block 4\n",
    "combosBlk4 = []\n",
    "\n",
    "for size in range(0, len(miscVars) + 1):\n",
    "    # Get every combination of given size\n",
    "    thisList = list(itertools.combinations(miscVars, size))\n",
    "    \n",
    "    ## Applying rules\n",
    "    for item in thisList:\n",
    "        subList = []\n",
    "        subSubList = []\n",
    "        for element in item:\n",
    "            subSubList.append(element)\n",
    "    \n",
    "        subList.append(subSubList)\n",
    "        combosBlk4.append(subList)\n",
    "\n",
    "print(combosBlk4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3335f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to Train:\n",
      " - Concatamash: 2752\n",
      " - Combined: 17920\n",
      "\n",
      "Training Sets: 256\n",
      "histnames = [['chargeInner_PXLayer_1', 'chargeInner_PXLayer_2', 'chargeInner_PXLayer_3', 'chargeInner_PXLayer_4'], ['charge_PXDisk_+1', 'charge_PXDisk_+2', 'charge_PXDisk_+3'], ['charge_PXDisk_-1', 'charge_PXDisk_-2', 'charge_PXDisk_-3'], ['Summary_ClusterStoNCorr__OnTrack__TIB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4'], ['Summary_ClusterStoNCorr__OnTrack__TOB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__4', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__5', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__6'], ['Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9'], ['Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9'], ['NumberOfRecHitsPerTrack_lumiFlag_GenTk']]\n"
     ]
    }
   ],
   "source": [
    "### Parsing combinations to create histlists\n",
    "histlists = []\n",
    "conmodelcount = 0\n",
    "combmodelcount = 0\n",
    "for combo1 in combosBlk1:\n",
    "    for combo2 in combosBlk2:\n",
    "        for combo3 in combosBlk3:\n",
    "            for combo4 in combosBlk4:\n",
    "                curList = []\n",
    "                for element in combo1:\n",
    "                    curList.append(element)\n",
    "                for element in combo2:\n",
    "                    curList.append(element)\n",
    "                for element in combo3:\n",
    "                    curList.append(element)\n",
    "                for element in combo4:\n",
    "                    if len(element) > 0:\n",
    "                        curList.append(element)\n",
    "                \n",
    "                # Sanity check that all files exist\n",
    "                for histgroup in curList:\n",
    "                    for hist in histgroup:\n",
    "                        filename = 'DF' + year + era + '_' + hist +'.csv'\n",
    "                        path = datadir + filename\n",
    "                        if not os.path.exists(path):\n",
    "                            raise Exception('Histogram {} does not exist!'.format(hist))\n",
    "                histlists.append(curList)\n",
    "                \n",
    "for histlist in histlists:\n",
    "    for histgroup in histlist:\n",
    "        conmodelcount = conmodelcount + 1\n",
    "        for hist in histgroup:\n",
    "            combmodelcount = combmodelcount + 1\n",
    "            \n",
    "print('Models to Train:')\n",
    "print(' - Concatamash: ' + str(conmodelcount))\n",
    "print(' - Combined: ' + str(combmodelcount))\n",
    "\n",
    "print('\\nTraining Sets: ' + str(len(histlists)))\n",
    "\n",
    "print('histnames = ' + str(histlists[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b82dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Controls and Selection - 1D Autoncoder\n",
    "\n",
    "# The directory data is located in\n",
    "datadir = '../data/'\n",
    "\n",
    "# Select a list of good runs to train on in development training_mode\n",
    "# Should be validated by eye\n",
    "trainrunsls = {'2017B':{\n",
    "#                    \"297057\":[[-1]], \n",
    "#                    \"297099\":[[-1]], \n",
    "#                    \"297101\":[[-1]],\n",
    "#                    \"297113\":[[-1]], \n",
    "#                    \"297114\":[[-1]], \n",
    "                    \"297175\":[[-1]],   # A decently clean histogram\n",
    "#                    \"297177\":[[-1]],\n",
    "#                    \"297179\":[[-1]], \n",
    "#                    \"297215\":[[-1]],\n",
    "#                    \"297218\":[[-1]],\n",
    "#                    \"297225\":[[-1]],\n",
    "#                    \"297296\":[[-1]], \n",
    "#                    \"297411\":[[-1]],\n",
    "#                    \"297426\":[[-1]],  \n",
    "#                    \"297431\":[[-1]],\n",
    "#                    \"297434\":[[-1]], \n",
    "#                    \"297468\":[[-1]],\n",
    "#                    \"297483\":[[-1]],\n",
    "#                    \"297486\":[[-1]],\n",
    "#                    \"297503\":[[-1]],\n",
    "#                    \"297557\":[[-1]],\n",
    "#                   \"297598\":[[-1]],\n",
    "#                   \"297604\":[[-1]],   # A decently clean histogram\n",
    "                   \"297620\":[[-1]],   # A decently clean histogram\n",
    "                   \"297659\":[[-1]],   # An okay histogram\n",
    "                   \"297670\":[[-1]],   # A decently clean histogram\n",
    "#                    \"297674\":[[-1]],\n",
    "#                    \"297678\":[[-1]],   # A particularly messy histogram\n",
    "#                    \"297722\":[[-1]],   # A decently clean histogram\n",
    "#                    \"298997\":[[-1]],\n",
    "#                    \"299061\":[[-1]],\n",
    "                   \"299065\":[[-1]],   # A decently clean histogram\n",
    "                   \"299067\":[[-1]],   # A decently clean histogram\n",
    "                   \"299096\":[[-1]],\n",
    "                   \"299149\":[[-1]],\n",
    "#                    \"299178\":[[-1]],   # A decently clean histogram\n",
    "#                    \"299184\":[[-1]],   # A particularly messy histogram\n",
    "#                    \"299185\":[[-1]],   # A decently clean histogram\n",
    "#                    \"299327\":[[-1]],\n",
    "#                    \"299329\":[[-1]], \n",
    "#                    \"299480\":[[-1]]    # A decently clean histogram\n",
    "                    },\n",
    "                   '2017C': {\n",
    "#                      \"299370\":[[-1]],\n",
    "#                      \"299394\":[[-1]],\n",
    "#                      \"299420\":[[-1]],\n",
    "#                      \"299477\":[[-1]],\n",
    "#                      \"299593\":[[-1]],\n",
    "#                      \"299597\":[[-1]],\n",
    "#                      \"299617\":[[-1]],\n",
    "#                      \"300018\":[[-1]],\n",
    "#                      \"300105\":[[-1]],\n",
    "#                      \"300117\":[[-1]],\n",
    "#                      \"300124\":[[-1]],\n",
    "#                      \"300234\":[[-1]],\n",
    "#                      \"300237\":[[-1]],\n",
    "#                      \"300240\":[[-1]],\n",
    "#                      \"300370\":[[-1]],\n",
    "#                      \"300157\":[[-1]],\n",
    "#                      \"300373\":[[-1]],\n",
    "#                      \"300392\":[[-1]],\n",
    "#                      \"300395\":[[-1]],\n",
    "#                      \"300401\":[[-1]],\n",
    "#                      \"300462\":[[-1]],\n",
    "#                      \"300466\":[[-1]],\n",
    "                      \"300514\":[[-1]],\n",
    "                      \"300517\":[[-1]],\n",
    "                      \"300538\":[[-1]],\n",
    "                      \"300539\":[[-1]],\n",
    "                      \"300364\":[[-1]],\n",
    "                 },'2017F':{\n",
    "#                      \"305310\":[[-1]],\n",
    "#                      \"305040\":[[-1]],\n",
    "#                      \"305043\":[[-1]],\n",
    "#                      \"305185\":[[-1]],\n",
    "#                      \"305204\":[[-1]],\n",
    "                      \"305234\":[[-1]],\n",
    "                      \"305247\":[[-1]],\n",
    "                      \"305313\":[[-1]],\n",
    "                      \"305338\":[[-1]],\n",
    "                      \"305350\":[[-1]],\n",
    "                      \"305364\":[[-1]],\n",
    "                      \"305376\":[[-1]],\n",
    "                      \"306042\":[[-1]],\n",
    "                      \"306051\":[[-1]],\n",
    "                      \"305406\":[[-1]],\n",
    "                      \"306122\":[[-1]],\n",
    "                      \"306134\":[[-1]],\n",
    "                      \"306137\":[[-1]],\n",
    "                      \"306154\":[[-1]],\n",
    "                      \"306170\":[[-1]],\n",
    "                      \"306417\":[[-1]],\n",
    "                      \"306432\":[[-1]],\n",
    "                      \"306456\":[[-1]],\n",
    "                      \"305516\":[[-1]],\n",
    "                      \"305586\":[[-1]],\n",
    "                      \"305588\":[[-1]],\n",
    "                      \"305590\":[[-1]],\n",
    "                      \"305809\":[[-1]],\n",
    "                      \"305832\":[[-1]],\n",
    "                      \"305840\":[[-1]],\n",
    "                      \"305898\":[[-1]],\n",
    "                      \"306029\":[[-1]],\n",
    "                      \"306037\":[[-1]],\n",
    "                      \"306095\":[[-1]],\n",
    "                },\n",
    "                '2018':{ # needs to be re-checked, not guaranteed to be fully correct or representative.   \n",
    "                  \"315267\":[[-1]] \n",
    "              }\n",
    "}\n",
    "\n",
    "# Select a list of good runs to test on in development training_mode\n",
    "# Should be validated by eye\n",
    "goodrunsls = {'2017B':{\n",
    "#                    \"297057\":[[-1]], \n",
    "#                    \"297099\":[[-1]], \n",
    "#                    \"297101\":[[-1]],\n",
    "#                    \"297113\":[[-1]], \n",
    "#                    \"297114\":[[-1]], \n",
    "                    \"297175\":[[-1]],   # A decently clean histogram\n",
    "#                    \"297177\":[[-1]],\n",
    "#                    \"297179\":[[-1]], \n",
    "#                    \"297215\":[[-1]],\n",
    "#                    \"297218\":[[-1]],\n",
    "#                    \"297225\":[[-1]],\n",
    "#                    \"297296\":[[-1]], \n",
    "#                    \"297411\":[[-1]],\n",
    "#                    \"297426\":[[-1]],  \n",
    "#                    \"297431\":[[-1]],\n",
    "#                    \"297434\":[[-1]], \n",
    "#                    \"297468\":[[-1]],\n",
    "#                    \"297483\":[[-1]],\n",
    "#                    \"297486\":[[-1]],\n",
    "#                    \"297503\":[[-1]],\n",
    "#                    \"297557\":[[-1]],\n",
    "#                    \"297598\":[[-1]],\n",
    "#                    \"297604\":[[-1]],   # A decently clean histogram\n",
    "#                    \"297620\":[[-1]],   # A decently clean histogram\n",
    "                    \"297659\":[[-1]],   # An okay histogram\n",
    "                    \"297670\":[[-1]],   # A decently clean histogram\n",
    "                    \"297674\":[[-1]],\n",
    "#                    \"297678\":[[-1]],   # A particularly messy histogram\n",
    "                    \"297722\":[[-1]],   # A decently clean histogram\n",
    "#                    \"298997\":[[-1]],\n",
    "#                    \"299061\":[[-1]],\n",
    "                    \"299065\":[[-1]],   # A decently clean histogram\n",
    "                    \"299067\":[[-1]],   # A decently clean histogram\n",
    "#                    \"299096\":[[-1]],\n",
    "#                    \"299149\":[[-1]],\n",
    "#                    \"299178\":[[-1]],   # A decently clean histogram\n",
    "#                    \"299184\":[[-1]],   # A particularly messy histogram\n",
    "                    \"299185\":[[-1]],   # A decently clean histogram\n",
    "                    \"299327\":[[-1]],\n",
    "#                    \"299329\":[[-1]], \n",
    "                    \"299480\":[[-1]]    # A decently clean histogram\n",
    "                    },\n",
    "                '2017C':{\n",
    "#                      \"299370\":[[-1]],\n",
    "#                      \"299394\":[[-1]],\n",
    "#                      \"299420\":[[-1]],\n",
    "#                      \"299477\":[[-1]],\n",
    "#                      \"299593\":[[-1]],\n",
    "#                      \"299597\":[[-1]],\n",
    "#                      \"299617\":[[-1]],\n",
    "#                      \"300018\":[[-1]],\n",
    "#                      \"300105\":[[-1]],\n",
    "#                      \"300117\":[[-1]],\n",
    "#                      \"300124\":[[-1]],\n",
    "#                      \"300234\":[[-1]],\n",
    "#                      \"300237\":[[-1]],\n",
    "#                      \"300240\":[[-1]],\n",
    "#                      \"300370\":[[-1]],\n",
    "#                      \"300157\":[[-1]],\n",
    "#                      \"300373\":[[-1]],\n",
    "#                      \"300392\":[[-1]],\n",
    "#                      \"300395\":[[-1]],\n",
    "#                      \"300401\":[[-1]],\n",
    "#                      \"300462\":[[-1]],\n",
    "#                      \"300466\":[[-1]],\n",
    "                      \"300514\":[[-1]],\n",
    "                      \"300517\":[[-1]],\n",
    "                      \"300538\":[[-1]],\n",
    "                      \"300539\":[[-1]],\n",
    "                      \"300364\":[[-1]],\n",
    "                },'2017F':{\n",
    "                      \"305310\":[[-1]],\n",
    "                      \"305040\":[[-1]],\n",
    "                      \"305043\":[[-1]],\n",
    "                      \"305185\":[[-1]],\n",
    "                      \"305204\":[[-1]],\n",
    "#                      \"305234\":[[-1]],\n",
    "#                      \"305247\":[[-1]],\n",
    "#                      \"305313\":[[-1]],\n",
    "#                      \"305338\":[[-1]],\n",
    "#                      \"305350\":[[-1]],\n",
    "#                      \"305364\":[[-1]],\n",
    "#                      \"305376\":[[-1]],\n",
    "#                      \"306042\":[[-1]],\n",
    "#                      \"306051\":[[-1]],\n",
    "#                      \"305406\":[[-1]],\n",
    "#                      \"306122\":[[-1]],\n",
    "#                      \"306134\":[[-1]],\n",
    "#                      \"306137\":[[-1]],\n",
    "#                      \"306154\":[[-1]],\n",
    "#                      \"306170\":[[-1]],\n",
    "#                      \"306417\":[[-1]],\n",
    "#                      \"306432\":[[-1]],\n",
    "#                      \"306456\":[[-1]],\n",
    "#                      \"305516\":[[-1]],\n",
    "#                      \"305586\":[[-1]],\n",
    "#                      \"305588\":[[-1]],\n",
    "#                      \"305590\":[[-1]],\n",
    "#                      \"305809\":[[-1]],\n",
    "#                      \"305832\":[[-1]],\n",
    "#                      \"305840\":[[-1]],\n",
    "#                      \"305898\":[[-1]],\n",
    "#                      \"306029\":[[-1]],\n",
    "#                      \"306037\":[[-1]],\n",
    "#                      \"306095\":[[-1]],\n",
    "                },\n",
    "                '2018':{ # needs to be re-checked, not guaranteed to be fully correct or representative.   \n",
    "                  \"315267\":[[-1]] \n",
    "              }\n",
    "}\n",
    "\n",
    "\n",
    "badrunsls = {'2017B':\n",
    "                {\n",
    "                    #\"297048\":[[-1]],\n",
    "                    #\"297282\":[[-1]],\n",
    "                    #\"297283\":[[-1]],\n",
    "                    #\"297284\":[[-1]],\n",
    "                    #\"297287\":[[-1]],\n",
    "                    #\"297288\":[[-1]],\n",
    "                    #\"297289\":[[-1]],\n",
    "                    \"299316\":[[-1]],\n",
    "                    \"299317\":[[-1]],\n",
    "                    \"299318\":[[-1]],\n",
    "                    \"299324\":[[-1]],\n",
    "                    \"299326\":[[-1]],\n",
    "                    #\"301086\":[[88,126]],\n",
    "                    #\"301086\":[[89,89]],\n",
    "                    #\"303948\":[[1710,1710]],\n",
    "                    \"297047\":[[-1]], #close but, true bad for all 8\n",
    "                    \"297169\":[[-1]], #true bad for all 8\n",
    "#                   \"297211\":[[-1]], #Reconstructs well\n",
    "#                   \"299325\":[[-1]], #Reconstructs well\n",
    "                    \"297664\":[[-1]], #true bad for all 8\n",
    "                    \"299317\":[[-1]], #true bad for all 8\n",
    "                    \"297169\":[[-1]], #true bad for all 8\n",
    "#                   \"297502\":[[-1]]\n",
    "                },\n",
    "             '2017C':{\n",
    "                \"300079\":[[-1]],\n",
    "                \"300282\":[[-1]],\n",
    "                \"300389\":[[-1]],\n",
    "                \"300398\":[[-1]],\n",
    "                 \n",
    "                 \n",
    "#                 \"300781\":[[-1]], # bad for tracking (pixels were excluded.\n",
    "#                 \"300079\":[[-1]], # is bad for strips and then also for tracking\n",
    "#                 \"302029\":[[-1]], # Poor detector elements for strips - Should be mildly anomalous, but technically good \n",
    "#                 \"300576\":[[-1]], # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "#                 \"300574\":[[-1]], # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "#                 \"300282\":[[-1]], # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "#                 \"301912\":[[-1]], # Half bad for pixels (lost HV or readout card)  \n",
    "#                 \"301086\":[[-1]], # Half bad for pixels (lost HV or readout card)  \n",
    "#                 \"300283\":[[-1]], # Half bad for pixels (lost HV or readout card) \n",
    "#                 \"300282\":[[-1]], # Half bad for pixels (lost HV or readout card) \n",
    "#                 \"300281\":[[-1]], # Half bad for pixels (lost HV or readout card) \n",
    "#                 \"300239\":[[-1]], # Half bad for pixels (lost HV or readout card)\n",
    "#                 \"301394\":[[-1]], # Marginal for pixels\n",
    "#                 \"301183\":[[-1]], # Marginal for pixels\n",
    "#                 \"300398\":[[-1]], # Marginal for pixels\n",
    "#                 \"300389\":[[-1]], # Marginal for pixels\n",
    "#                 \"300365\":[[-1]]  # Marginal for pixels\n",
    "              },\n",
    "             '2017E':{\n",
    "                 \"304740\":[[-1]], # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 \"304776\":[[-1]], # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 \"304506\":[[-1]], # Portcard problem for pixels\n",
    "                 \"304507\":[[-1]], # Portcard problem for pixels \n",
    "                 \"303989\":[[-1]], # Bad for pixels, power supply died\n",
    "                 \"303824\":[[-1]]  # Partly bad for strips due to a test\n",
    "             },\n",
    "             '2017F':{\n",
    "                 \"306422\":[[-1]], # Partly bad for strips - 2 data readouts failed \n",
    "#                 \"306423\":[[-1]], # Partly bad for strips - 2 data readouts failed\n",
    "#                 \"306425\":[[-1]], # Partly bad for strips - 2 data readouts failed\n",
    "#                 \"305440\":[[-1]], # Partly bad for strips - 1 data readout failed\n",
    "#                 \"305441\":[[-1]], # Partly bad for strips - 1 data readout failed\n",
    "                 \"305249\":[[-1]], # Bad for pixels - half of disk failed \n",
    "                 \"305250\":[[-1]], # Bad for pixels - half of disk failed\n",
    "#                 \"305064\":[[-1]], # Marginal for pixels - some readout failed\n",
    "             },\n",
    "            '2018': # needs to be re-checked, not guaranteed to be fully correct or representative.\n",
    "                {\n",
    "                #\"317479\":[[-1]],\n",
    "                \"317480\":[[-1]],\n",
    "                \"317481\":[[-1]],\n",
    "                \"317482\":[[-1]],\n",
    "                #\"319847\":[[1,35]]\n",
    "            }}\n",
    "\n",
    "# Create a list of histograms to include\n",
    "# Pair histograms to be combined on the same line\n",
    "histnames = histlists[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "585330ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Run Properties - Combined Autoencoder\n",
    "# in this cell all major run properties are going to be set,\n",
    "\n",
    "# Select whether to save a new histstruct\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3148531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Controls and Selection - 1D Autoencoder\n",
    "\n",
    "plotNames = 'Test'\n",
    "name = plotNames+'plots'\n",
    "\n",
    "# Choose whether to train a new model or load one\n",
    "trainnew = True\n",
    "savemodel = True\n",
    "modelname = plotNames\n",
    "\n",
    "# Bias Factors\n",
    "fmBiasFactor = 2\n",
    "wpBiasFactor = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ccbedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected runs/lumisections for training: \n",
      "{'297175': [[-1]], '297620': [[-1]], '297659': [[-1]], '297670': [[-1]], '299065': [[-1]], '299067': [[-1]], '299096': [[-1]], '299149': [[-1]]}\n",
      "selected runs/lumisections as good test set:\n",
      "{'297175': [[-1]], '297659': [[-1]], '297670': [[-1]], '297674': [[-1]], '297722': [[-1]], '299065': [[-1]], '299067': [[-1]], '299185': [[-1]], '299327': [[-1]], '299480': [[-1]]}\n",
      "selected runs/lumisections as bad test set:\n",
      "{'299316': [[-1]], '299317': [[-1]], '299318': [[-1]], '299324': [[-1]], '299326': [[-1]], '297047': [[-1]], '297169': [[-1]], '297664': [[-1]]}\n"
     ]
    }
   ],
   "source": [
    "# train on a user-defined subset of runs\n",
    "    \n",
    "# Select runs to be used in training from the user-defined list\n",
    "runsls_training = trainrunsls[year + era]\n",
    "# Select bad runs to test on in the user-defined list\n",
    "runsls_bad = badrunsls[year + era]\n",
    "# Select good runs to test on in the user-defined list\n",
    "runsls_good = goodrunsls[year + era]\n",
    "\n",
    "print('selected runs/lumisections for training: ')\n",
    "print(runsls_training)\n",
    "print('selected runs/lumisections as good test set:')\n",
    "print(runsls_good)\n",
    "print('selected runs/lumisections as bad test set:')\n",
    "print(runsls_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b5b458",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Classifiers and masks cleared to preserve consistency\n",
      "Adding chargeInner_PXLayer_1...\n",
      "Adding chargeInner_PXLayer_2...\n",
      "Adding chargeInner_PXLayer_3...\n",
      "Adding chargeInner_PXLayer_4...\n",
      "Adding chargeOuter_PXLayer_1...\n",
      "Adding chargeOuter_PXLayer_2...\n",
      "Adding chargeOuter_PXLayer_3...\n",
      "Adding chargeOuter_PXLayer_4...\n",
      "Adding adc_PXLayer_1...\n",
      "Adding adc_PXLayer_2...\n",
      "Adding adc_PXLayer_3...\n",
      "Adding adc_PXLayer_4...\n",
      "Adding size_PXLayer_1...\n",
      "Adding size_PXLayer_2...\n",
      "Adding size_PXLayer_3...\n",
      "Adding size_PXLayer_4...\n",
      "Adding charge_PXDisk_+1...\n",
      "Adding charge_PXDisk_+2...\n",
      "Adding charge_PXDisk_+3...\n",
      "Adding adc_PXDisk_+1...\n",
      "Adding adc_PXDisk_+2...\n",
      "Adding adc_PXDisk_+3...\n",
      "Adding size_PXDisk_+1...\n",
      "Adding size_PXDisk_+2...\n",
      "Adding size_PXDisk_+3...\n",
      "Adding charge_PXDisk_-1...\n",
      "Adding charge_PXDisk_-2...\n",
      "Adding charge_PXDisk_-3...\n",
      "Adding adc_PXDisk_-1...\n",
      "Adding adc_PXDisk_-2...\n",
      "Adding adc_PXDisk_-3...\n",
      "Adding size_PXDisk_-1...\n",
      "Adding size_PXDisk_-2...\n",
      "Adding size_PXDisk_-3...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__1...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__2...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__3...\n",
      "Adding NormalizedHitResiduals_TIB__Layer__4...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__1...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__2...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__3...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__4...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__5...\n",
      "Adding NormalizedHitResiduals_TOB__Layer__6...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__4...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__5...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__6...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3...\n",
      "Adding NormalizedHitResiduals_TID__wheel__1...\n",
      "Adding NormalizedHitResiduals_TID__wheel__2...\n",
      "Adding NormalizedHitResiduals_TID__wheel__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__1...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__2...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__3...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__4...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__5...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__6...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__7...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__8...\n",
      "Adding NormalizedHitResiduals_TEC__wheel__9...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9...\n",
      "Adding NumberOfRecHitsPerTrack_lumiFlag_GenTk...\n",
      "Adding Chi2oNDF_lumiFlag_GenTk...\n",
      "Adding goodvtxNbr...\n",
      "Adding num_clusters_ontrack_PXBarrel...\n",
      "Adding num_clusters_ontrack_PXForward...\n",
      "Found 2831 histograms\n",
      "Created a histstruct with the following properties:\n",
      "- number of histogram types: 95\n",
      "- number of lumisections: 2831\n"
     ]
    }
   ],
   "source": [
    "# Initializations\n",
    "dloader = DataLoader.DataLoader()\n",
    "histstruct = SubHistStruct.SubHistStruct()\n",
    "histstruct.reset_histlist(histnames)\n",
    "\n",
    "# Unpack histnames and add every histogram individually\n",
    "for histnamegroup in histnames:\n",
    "    for histname in histnamegroup:\n",
    "        print('Adding {}...'.format(histname))\n",
    "        \n",
    "        # Bring the histograms into memory from storage for later use\n",
    "        filename = datadir + year + era + '/DF' + year + era + '_' + histname + '.csv'\n",
    "        df = dloader.get_dataframe_from_file( filename )\n",
    "        \n",
    "        # In case of local training, we can remove most of the histograms\n",
    "        if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ):\n",
    "            runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()}\n",
    "            df = dfu.select_runsls( df, runsls_total )\n",
    "        \n",
    "        df = dfu.rm_duplicates(df)\n",
    "        \n",
    "        try:\n",
    "            # Store the data in the histstruct object managing this whole thing\n",
    "            histstruct.add_dataframe( df, rebinningfactor = 1, standardbincount = 102 )\n",
    "        except:\n",
    "            print(\"WARNING: Could not add \" + histname, file=sys.stderr)\n",
    "    \n",
    "print('Found {} histograms'.format(len(histstruct.runnbs)))\n",
    "print('Created a histstruct with the following properties:')\n",
    "print('- number of histogram types: {}'.format(len(histstruct.histnames)))\n",
    "print('- number of lumisections: {}'.format(len(histstruct.lsnbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98b24bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add Masks to Data\n",
    "def assignMasks(histstruct, runsls_training, runsls_good, runsls_bad):\n",
    "\n",
    "    histstruct.add_dcsonjson_mask( 'dcson' )\n",
    "    histstruct.add_goldenjson_mask('golden' )\n",
    "    histstruct.add_highstat_mask( 'highstat', entries_to_bins_ratio=0)\n",
    "    histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=0)\n",
    "    if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training )\n",
    "    if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good )\n",
    "        \n",
    "    # Distinguishing bad runs\n",
    "    nbadruns = 0\n",
    "    if runsls_bad is not None:\n",
    "        histstruct.add_json_mask( 'bad', runsls_bad )\n",
    "        \n",
    "        # Special case for bad runs: add a mask per run (different bad runs have different characteristics)\n",
    "        nbadruns = len(runsls_bad.keys())\n",
    "        for i,badrun in enumerate(runsls_bad.keys()):\n",
    "            histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} )\n",
    "            \n",
    "    if save:\n",
    "        histstruct.save('test.pk1')\n",
    "    return histstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f3bc308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_concatamash_autoencoder(histstruct):\n",
    "    \n",
    "    histslist = []\n",
    "    vallist = []\n",
    "    autoencoders = []\n",
    "    if trainnew:\n",
    "        for i,histnamegroup in enumerate(histnames):\n",
    "            \n",
    "            train_normhist = np.array([hu.normalize(histstruct.get_histograms(\n",
    "                histname = hname, masknames = ['dcson','highstat', 'training']), \n",
    "                                                 norm=\"l1\", axis=1) \n",
    "                                       for hname in histnamegroup]).transpose((1,0,2))\n",
    "            X_train, X_val = train_test_split(train_normhist, test_size=0.4, random_state=42)\n",
    "            \n",
    "            # Half the total bin count\n",
    "            arch = 51 * len(histnamegroup)\n",
    "            \n",
    "            ## Model parameters\n",
    "            input_dim = X_train.shape[2] #num of predictor variables\n",
    "            Input_layers=[Input(shape=input_dim) for i in range((X_train.shape[1]))]\n",
    "            \n",
    "            # Defining layers\n",
    "            conc_layer = Concatenate()(Input_layers)\n",
    "            encoder = Dense(arch * 2, activation=\"tanh\")(conc_layer)\n",
    "            encoder = Dense(arch, activation='relu')(encoder)\n",
    "            \n",
    "            encoder = Dense(arch/2, activation='relu')(encoder)\n",
    "            \n",
    "            decoder = Dense(arch, activation=\"relu\")(encoder)\n",
    "            decoder = Dense(arch * 2, activation=\"tanh\")(decoder)\n",
    "            \n",
    "            Output_layers=[Dense(input_dim, activation=\"tanh\")(decoder) for i in range(X_train.shape[1])]\n",
    "\n",
    "            autoencoder = Model(inputs=Input_layers, outputs=Output_layers)\n",
    "            autoencoders.append(autoencoder)\n",
    "            \n",
    "            histslist.append(X_train)\n",
    "            vallist.append(X_val)\n",
    "     \n",
    "    # Return the histograms stored 2-Dimensionally and the autoencoders corresponding\n",
    "    return(histslist, vallist, autoencoders, train_normhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "944e9968",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trains a combined autoencoder for every merge set\n",
    "def train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders):\n",
    "    \n",
    "    # Iterate through the training data to train corresponding autoencoders\n",
    "    for i in range(len(histslist)):\n",
    "        \n",
    "        sys.stdout.write('\\rNow training model {}/'.format(i + 1) + str(len(histslist)))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Set variables to temporary values for better transparency\n",
    "        X_train = histslist[i]\n",
    "        X_val = vallist[i]\n",
    "        autoencoder = autoencoders[i]\n",
    "        \n",
    "        \n",
    "        ## Model parameters\n",
    "        nb_epoch = 500\n",
    "        batch_size = 50\n",
    "        \n",
    "        #checkpoint_filepath = 'checkpoint'\n",
    "        #model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #   filepath=checkpoint_filepath,\n",
    "        #   save_weights_only=False,\n",
    "        #   verbose=1,\n",
    "        #   save_best_only=True,\n",
    "        #   monitor='val_loss',\n",
    "        #   mode='min')\n",
    "        \n",
    "        # Tell the model when to stop\n",
    "        earlystop = EarlyStopping(monitor='val_loss',\n",
    "            min_delta=1e-7,\n",
    "            patience=20,\n",
    "            verbose=0,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        lr =0.001\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        autoencoder.compile(loss='mse',\n",
    "                            optimizer=opt)\n",
    "        \n",
    "        ## Train autoencoder\n",
    "        train = autoencoder.fit(x=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                                y=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                            epochs=nb_epoch,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            validation_data=([X_val[:,i] for i in range(X_val.shape[1])], [X_val[:,i] for i in range(X_val.shape[1])]),\n",
    "                            verbose=0,\n",
    "                            callbacks= [earlystop],    \n",
    "                            )\n",
    "        \n",
    "        # Create a plot of the model\n",
    "        \n",
    "        tf.keras.utils.plot_model(\n",
    "            autoencoder,\n",
    "            to_file=\"models/modelConcatamash{}.png\".format(i),\n",
    "            show_shapes=True,\n",
    "            show_dtype=False,\n",
    "            show_layer_names=False,\n",
    "            rankdir=\"TB\")\n",
    "        \n",
    "        # Save classifier for evaluation\n",
    "        classifier = AutoEncoder.AutoEncoder(model=autoencoder)\n",
    "        histstruct.add_classifier(histnames[i][0], classifier)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f6a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the Models for WP definition\n",
    "def evaluate_models_train(histstruct):\n",
    "    \n",
    "    for histgroup in histnames:\n",
    "        histstruct.evaluate_classifier(histgroup)\n",
    "    \n",
    "    # get mse for training set\n",
    "    if 'training' in histstruct.masks.keys(): masknames = ['dcson','highstat', 'training']\n",
    "    else: masknames = ['dcson','highstat']\n",
    "    mse_train = histstruct.get_scores_array( masknames=masknames )\n",
    "    \n",
    "    # get mse for good set\n",
    "    if 'good' in histstruct.masks.keys():\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            mse_good.append(histstruct.get_scores( histname=histname, masknames=['dcson','highstat','good'] ))\n",
    "    else:\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            hists_good = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), 1000 )\n",
    "            thismse = histstruct.classifiers[histname].evaluate( hists_good )\n",
    "            mse_good.append( thismse )\n",
    "            print(run)\n",
    "    mse_good = np.array(mse_good)\n",
    "    mse_good = np.transpose(mse_good)\n",
    "    \n",
    "    # get mse for bad sets\n",
    "    mse_bad = []\n",
    "    nbadruns = len([name for name in runsls_bad])\n",
    "    for i in range(nbadruns):\n",
    "        mse_bad.append( histstruct.get_scores_array( masknames=['dcson','highstat','bad{}'.format(i)] ) )\n",
    "        \n",
    "    return [mse_train, mse_good, mse_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecb421c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots and Distribution Analysis\n",
    "def fit_mse_distribution(histstruct, mse_train):\n",
    "    dimslist = []\n",
    "    fitfunclist = []\n",
    "    \n",
    "    \n",
    "    nhisttypes = len(histstruct.histnames)\n",
    "    for i in range(0,nhisttypes-1):\n",
    "        for j in range(i+1,nhisttypes):\n",
    "            dimslist.append((i, j))\n",
    "    \n",
    "    plt.close('all')\n",
    "    (npoints,ndims) = mse_train.shape\n",
    "    \n",
    "    \n",
    "    # settings for GaussianKdeFitter\n",
    "    scott_bw = npoints**(-1./(ndims+4))\n",
    "    bw_method = 20*scott_bw\n",
    "    # settings for HyperRectangleFitter\n",
    "    quantiles = ([0.00062,0.0006,0.00015,0.00015,\n",
    "                 0.0003,0.0003,0.00053,0.00065])\n",
    "    \n",
    "    \n",
    "    #for dims in dimslist:\n",
    "    #    thismse = mse_train[:,dims]\n",
    "    #    if training_mode=='global': \n",
    "    #        fitfunc = SeminormalFitter.SeminormalFitter(thismse)\n",
    "    #        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(thismse, \n",
    "    #        #                                                    [quantiles[dims[0]],quantiles[dims[1]]],\n",
    "    #        #                                                    'up')\n",
    "    #    else: fitfunc = GaussianKdeFitter.GaussianKdeFitter(thismse,bw_method=bw_method)\n",
    "    #    #pu.plot_fit_2d(thismse, fitfunc=fitfunc, logprob=True, clipprob=True,\n",
    "    #    #                onlycontour=False, xlims=30, ylims=30, \n",
    "    #    #                onlypositive=True, transparency=0.5,\n",
    "    #    #                xaxtitle=histstruct.histnames[dims[0]], \n",
    "    #    #                yaxtitle=histstruct.histnames[dims[1]],\n",
    "    #    #                title='density fit of lumisection MSE')\n",
    "    #    ##plt.close('all') # release plot memory\n",
    "    #    fitfunclist.append(fitfunc)\n",
    "    # \n",
    "    #    \n",
    "    fitfunc = GaussianKdeFitter.GaussianKdeFitter()\n",
    "    fitfunc.fit(mse_train,bw_method=bw_method)\n",
    "    \n",
    "    return fitfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed2234c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare MSEs for Working Point Definition\n",
    "def mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc):\n",
    "    \n",
    "    # Get the minimum log probability of histograms in good set\n",
    "    logprob_good = np.log(fitfunc.pdf(mse_good_eval))\n",
    "    #print(sorted(logprob_good))\n",
    "\n",
    "    logprob_bad_parts = [np.log(fitfunc.pdf(mse_bad_eval[j])) for j in range(len(mse_bad_eval))]\n",
    "    #for lp in logprob_bad_parts: print(str(sorted(lp))+'\\n\\n')\n",
    "    logprob_bad = np.concatenate(tuple(logprob_bad_parts))\n",
    "\n",
    "    #print(sorted(logprob_good))\n",
    "    #print(sorted(logprob_bad))\n",
    "    #print(logprob_bad)\n",
    "    \n",
    "    sep = np.min(logprob_good) - np.max(logprob_bad)\n",
    "    \n",
    "    return [logprob_good, logprob_bad, sep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32a08a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor):\n",
    "    labels_good = np.zeros(len(logprob_good)) # background: label = 0\n",
    "    labels_bad = np.ones(len(logprob_bad)) # signal: label = 1\n",
    "    \n",
    "    # Note this will give an error if there are all infinities in one or both arrays\n",
    "    badMin = min(np.where(logprob_bad != -np.inf, logprob_bad, np.inf))\n",
    "    goodMax = max(np.where(logprob_good != np.inf, logprob_good, -np.inf))\n",
    "    \n",
    "    # Getting rid of infinities\n",
    "    logprob_good[logprob_good > 500] = goodMax\n",
    "    logprob_bad[logprob_bad < 0] = badMin\n",
    "    # These only take effect if a histogram is grossly misclassified\n",
    "    logprob_good[logprob_good < badMin] = badMin\n",
    "    logprob_bad[logprob_bad > goodMax] = goodMax\n",
    "    \n",
    "    avSep = np.mean(logprob_good) - np.mean(logprob_bad)\n",
    "    \n",
    "    print('Average Separation: ' + str(avSep))\n",
    "    \n",
    "    labels = np.concatenate(tuple([labels_good,labels_bad]))\n",
    "    scores = np.concatenate(tuple([-logprob_good,-logprob_bad]))\n",
    "    scores = aeu.clip_scores( scores )\n",
    "    \n",
    "    # Setting a threshold, below this working point defines anomalous data\n",
    "    # Average is biased towards better recall per user specifications\n",
    "    logprob_threshold = (1/(wpBiasFactor + 1)) * (wpBiasFactor*np.mean(logprob_good) + np.mean(logprob_bad))\n",
    "    # Or set manual\n",
    "    # logprob_threshold = 424\n",
    "    (_, _, _, tp, fp, tn, fn) = aeu.get_confusion_matrix(scores,labels,-logprob_threshold)\n",
    "    \n",
    "    # Get metrics for analysis\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_measure = (1 + fmBiasFactor * fmBiasFactor) * ((precision * recall) / ((fmBiasFactor * fmBiasFactor * precision) + recall)) \n",
    "    \n",
    "    print('F{}-Measure: '.format(fmBiasFactor) + str(f_measure))\n",
    "    \n",
    "    return logprob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53dfe472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test 1/256\n",
      "Training complete in 94.98764951503836 seconds"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ERROR in HistStruct.get_scores: requested histogram name chargeOuter_PXLayer_1 but the scores for this histogram type were not yet initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27453/2526403639.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Evaluate models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mmse_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_good_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_bad_eval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_models_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiststruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfitfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_mse_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiststruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mlogprob_good\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprob_bad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiststruct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_good_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_bad_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27453/1286301452.py\u001b[0m in \u001b[0;36mevaluate_models_train\u001b[0;34m(histstruct)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmse_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhistname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhiststruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mmse_good\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhiststruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mhistname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasknames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dcson'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'highstat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'good'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmse_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/eos/home-i01/k/khowey/SWAN_projects/ML4DQMDC-PixelAE/KH-AutoencoderTest/../src/SubHistStruct.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, histname, masknames)\u001b[0m\n\u001b[1;32m    133\u001b[0m                                +' but this is not present in the current HistStruct.')\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhistname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 raise Exception('ERROR in HistStruct.get_scores: requested histogram name {}'.format(histname)\n\u001b[0m\u001b[1;32m    136\u001b[0m                                +' but the scores for this histogram type were not yet initialized.')\n\u001b[1;32m    137\u001b[0m             \u001b[0mhistnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhistname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: ERROR in HistStruct.get_scores: requested histogram name chargeOuter_PXLayer_1 but the scores for this histogram type were not yet initialized."
     ]
    }
   ],
   "source": [
    "### Main loop to iterate through possible histlists\n",
    "userfriendly = True\n",
    "for i,histnames in enumerate(histlists):\n",
    "    print('Running Test {}/'.format(i+1) + str(len(histlists)))\n",
    "    \n",
    "    # Update histlist to reflect new data\n",
    "    histstruct.reset_histlist(histnames, suppress=True)\n",
    "    assignMasks(histstruct, runsls_training, runsls_good, runsls_bad)\n",
    "    \n",
    "    # Build autoencoders based on new data\n",
    "    (histslist, vallist, autoencoders, train_normhist) = define_concatamash_autoencoder(histstruct)\n",
    "    \n",
    "    # Train autoencoders based on current histlist and record speed\n",
    "    start = time.perf_counter()\n",
    "    train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders)\n",
    "    stop = time.perf_counter()\n",
    "    trainTime = stop - start\n",
    "    \n",
    "    sys.stdout.write('\\rTraining complete in ' + str(trainTime) + ' seconds')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Evaluate models\n",
    "    (mse_train, mse_good_eval, mse_bad_eval) = evaluate_models_train(histstruct)\n",
    "    fitfunc = fit_mse_distribution(histstruct, mse_train)\n",
    "    (logprob_good, logprob_bad, sep) = mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc)\n",
    "    logprob_threshold = evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb3cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
