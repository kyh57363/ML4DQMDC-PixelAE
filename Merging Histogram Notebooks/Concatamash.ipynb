{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91237d6",
   "metadata": {},
   "source": [
    "# Concatamash Autoencoder\n",
    "## This notebook will demonstrate the capabilities and functionalities of the Concatamash autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba412492",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-02 18:32:44.345630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.3-88592/x86_64-centos7-gcc11-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.3-35f7a/x86_64-centos7-gcc11-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jaxlib/mlir/_mlir_libs:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/onnxruntime/capi/:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/11.0.14p1-8284a/x86_64-centos7-gcc11-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_102swan/x86_64-centos7-gcc11-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.37-355ed/x86_64-centos7/lib:/usr/local/lib/:/cvmfs/sft.cern.ch/lcg/releases/R/4.1.2-f9ee4/x86_64-centos7-gcc11-opt/lib64/R/library/readr/rcon\n",
      "2022-08-02 18:32:44.345696: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'HyperRectangleFitter' from '/eos/home-i01/k/khowey/SWAN_projects/ML4DQMDC-PixelAE/Merging Histogram Notebooks/../src/cloudfitters/HyperRectangleFitter.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "import importlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# local modules\n",
    "sys.path.append('../utils')\n",
    "import csv_utils as csvu\n",
    "import json_utils as jsonu\n",
    "import dataframe_utils as dfu\n",
    "import hist_utils as hu\n",
    "import autoencoder_utils as aeu\n",
    "import plot_utils as pu\n",
    "import generate_data_utils as gdu\n",
    "import refruns_utils as rru\n",
    "importlib.reload(csvu)\n",
    "importlib.reload(jsonu)\n",
    "importlib.reload(dfu)\n",
    "importlib.reload(hu)\n",
    "importlib.reload(aeu)\n",
    "importlib.reload(pu)\n",
    "importlib.reload(gdu)\n",
    "importlib.reload(rru)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../src/classifiers')\n",
    "sys.path.append('../src/cloudfitters')\n",
    "import HistStruct\n",
    "importlib.reload(HistStruct)\n",
    "import SubHistStruct\n",
    "importlib.reload(SubHistStruct)\n",
    "import FlexiStruct\n",
    "importlib.reload(FlexiStruct)\n",
    "import DataLoader\n",
    "importlib.reload(DataLoader)\n",
    "import AutoEncoder\n",
    "importlib.reload(AutoEncoder)\n",
    "import SeminormalFitter\n",
    "import GaussianKdeFitter\n",
    "import HyperRectangleFitter\n",
    "importlib.reload(SeminormalFitter)\n",
    "importlib.reload(GaussianKdeFitter)\n",
    "importlib.reload(HyperRectangleFitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631e8d5",
   "metadata": {},
   "source": [
    "### Controls and Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a9dbfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Speed Controls and Run Mode\n",
    "\n",
    "# Disables all plots for large datasets where speed is more important\n",
    "createPlots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2020c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation Parameters\n",
    "\n",
    "# Select the bias towards recall against precision, treated as a factor (so < 1 biases towards precision, 1 is equal importance, and > 1 biases towards recall)\n",
    "wpBiasFactor = 20\n",
    "fmBiasFactor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8eb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining bad runs\n",
    "badruns = {'2017B':\n",
    "                [\n",
    "                    297048,\n",
    "                    297282,\n",
    "                    297283,\n",
    "                    297284,\n",
    "                    297287,\n",
    "                    297288,\n",
    "                    297289,\n",
    "                    299316,\n",
    "                    299317,\n",
    "                    299318,\n",
    "                    299324,\n",
    "                    299326,\n",
    "                    301086,\n",
    "                    301086,\n",
    "                    303948,\n",
    "                    297047, #close but, true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297211, #Reconstructs well\n",
    "                    299325, #Reconstructs well\n",
    "                    297664, #true bad for all 8\n",
    "                    299317, #true bad for all 8\n",
    "                    297169, #true bad for all 8\n",
    "                    297502\n",
    "                ],\n",
    "             '2017C':[\n",
    "                  300781, # bad for tracking (pixels were excluded.\n",
    "                  300079, # is bad for strips and then also for tracking\n",
    "                  302029, # Poor detector elements for strips - Should be mildly anomalous, but technically good \n",
    "                  300576, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300574, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  300282, # Poor detector elements for strips - Should be mildly anomalous, but technically good\n",
    "                  301912, # Half bad for pixels (lost HV or readout card)  \n",
    "                  301086, # Half bad for pixels (lost HV or readout card)  \n",
    "                  300283, # Half bad for pixels (lost HV or readout card) \n",
    "                  300282, # Half bad for pixels (lost HV or readout card) \n",
    "                  300281, # Half bad for pixels (lost HV or readout card) \n",
    "                  300239, # Half bad for pixels (lost HV or readout card)\n",
    "                  301394, # Marginal for pixels\n",
    "                  301183, # Marginal for pixels\n",
    "                  300398, # Marginal for pixels\n",
    "                  300389, # Marginal for pixels\n",
    "                  300365  # Marginal for pixels\n",
    "             ],\n",
    "             '2017E':[\n",
    "                 304740, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304776, # Bad for pixels and tracking - holes in PXLayer 1\n",
    "                 304506, # Portcard problem for pixels\n",
    "                 304507, # Portcard problem for pixels \n",
    "                 303989, # Bad for pixels, power supply died\n",
    "                 303824  # Partly bad for strips due to a test\n",
    "             ],\n",
    "             '2017F':[\n",
    "                 306422, # Partly bad for strips - 2 data readouts failed \n",
    "                 306423, # Partly bad for strips - 2 data readouts failed\n",
    "                 306425, # Partly bad for strips - 2 data readouts failed\n",
    "                 305440, # Partly bad for strips - 1 data readout failed\n",
    "                 305441, # Partly bad for strips - 1 data readout failed\n",
    "                 305249, # Bad for pixels - half of disk failed \n",
    "                 305250, # Bad for pixels - half of disk failed\n",
    "                 305064, # Marginal for pixels - some readout failed\n",
    "             ],\n",
    "            '2018': # needs to be re-checked, not guaranteed to be fully correct or representative.\n",
    "                [\n",
    "                317479,\n",
    "                317480,\n",
    "                317481,\n",
    "                317482,\n",
    "                319847\n",
    "                ]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66ed68a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found bad run :{'run_number': 303824, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 303989, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 304740, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 304158, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017E_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 305250, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 306422, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n",
      "Found bad run :{'run_number': 305249, 'run_reconstruction_type': 'rerecoul', 'reference_run_number': 306459, 'reference_run_reconstruction_type': 'rerecoul', 'dataset': '/ReReco/Run2017F_UL2019/DQM'}\n"
     ]
    }
   ],
   "source": [
    "### Select a reference run and get data\n",
    "rundict = jsonu.loadjson('../jsons/CertHelperRefRuns.json')\n",
    "\n",
    "# Select any run numbers to get a training set from that run's reference.\n",
    "runNums = [303824, 305249]\n",
    "refRuns = []\n",
    "eras = []\n",
    "years = []\n",
    "dataDict = {}\n",
    "badrunsls = {}\n",
    "trainrunsls = {}\n",
    "goodrunsls = {}\n",
    "for runNum in runNums:\n",
    "    runls = {}\n",
    "    for run in rundict:\n",
    "        if run['run_number'] == runNum:\n",
    "            runls.update(run)\n",
    "    if runls == {}:\n",
    "        raise Exception('Run not found - ' + str(runNum))\n",
    "    \n",
    "    year = runls['dataset'][11:15]\n",
    "    if year not in years: years.append(year)\n",
    "    era = runls['dataset'][15]\n",
    "    if era not in eras: eras.append(era)\n",
    "    ref_run = runls['reference_run_number']\n",
    "    \n",
    "    # Don't need duplicates\n",
    "    if ref_run in refRuns:\n",
    "        continue\n",
    "    refRuns.append(ref_run)\n",
    "    \n",
    "    # Get the runs associated with found reference\n",
    "    outputRuns = {}\n",
    "    outputBad = {}\n",
    "    for run in rundict:\n",
    "        tempRef = run['reference_run_number']\n",
    "        if tempRef == ref_run:\n",
    "            runls = {}\n",
    "            runls[str(run['run_number'])] = [[-1]]\n",
    "            if run['run_number'] in badruns[year+era]:\n",
    "                print('Found bad run :' + str(run))\n",
    "                outputBad.update(runls)\n",
    "            else:\n",
    "                outputRuns.update(runls)\n",
    "    \n",
    "    # Perform structuring for compatibility with autoencoders\n",
    "    dataDict[year + era] = outputRuns\n",
    "    badrunsls[year + era] = outputBad\n",
    "    trainrunsls[year + era] = {}\n",
    "    goodrunsls[year + era] = {}\n",
    "    \n",
    "    trainrunsls[year + era][str(ref_run)] = [[-1]]\n",
    "        \n",
    "    # Select training and testing set\n",
    "    for i,run in enumerate(dataDict[year + era]):\n",
    "        goodrunsls[year + era][str(run)] = [[-1]]\n",
    "            \n",
    "\n",
    "if len(years) != 1: raise Exception('Year of length 0 or >1 unimplemented!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4590f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Controls and Selection - 1D Autoncoder\n",
    "\n",
    "# The directory data is located in\n",
    "datadir = {}\n",
    "for era in eras:\n",
    "    datadir[year + era] = '../data/' + year + era + '/'\n",
    "\n",
    "# Create a list of histograms to include\n",
    "# Pair histograms to be combined on the same line\n",
    "\n",
    "histnames = [['chargeInner_PXLayer_1', 'chargeInner_PXLayer_2', 'chargeInner_PXLayer_3', 'chargeInner_PXLayer_4'], ['charge_PXDisk_+1', 'charge_PXDisk_+2', 'charge_PXDisk_+3'], ['charge_PXDisk_-1', 'charge_PXDisk_-2', 'charge_PXDisk_-3'], ['Summary_ClusterStoNCorr__OnTrack__TIB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TIB__layer__4'], ['Summary_ClusterStoNCorr__OnTrack__TOB__layer__1', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__2', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__3', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__4', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__5', 'Summary_ClusterStoNCorr__OnTrack__TOB__layer__6'], ['Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3'], ['Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9'], ['Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8', 'Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9'], ['NumberOfRecHitsPerTrack_lumiFlag_GenTk']]\n",
    "\n",
    "# Read new data or use previously saved data & should data be saved\n",
    "readnew = True\n",
    "save = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ffebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Run Properties - Combined Autoencoder\n",
    "# in this cell all major run properties are going to be set,\n",
    "\n",
    "# Set whether to train globally or locally or in a development/testing mode\n",
    "training_mode = 'development'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6394238",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Controls and Selection - 1D Autoencoder\n",
    "\n",
    "plotNames = 'Test'\n",
    "name = plotNames+'plots'\n",
    "\n",
    "# Choose whether to train a new model or load one\n",
    "trainnew = True\n",
    "savemodel = True\n",
    "modelname = plotNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4051dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected runs/lumisections for training: \n",
      "{'304158': [[-1]], '306459': [[-1]]}\n",
      "selected runs/lumisections as good test set:\n",
      "{'303819': [[-1]], '303999': [[-1]], '304119': [[-1]], '304120': [[-1]], '304197': [[-1]], '304505': [[-1]], '304198': [[-1]], '304199': [[-1]], '304209': [[-1]], '304333': [[-1]], '304446': [[-1]], '304449': [[-1]], '304452': [[-1]], '304508': [[-1]], '304625': [[-1]], '304655': [[-1]], '304737': [[-1]], '304778': [[-1]], '306459': [[-1]], '304196': [[-1]], '305310': [[-1]], '305040': [[-1]], '305043': [[-1]], '305185': [[-1]], '305204': [[-1]], '305234': [[-1]], '305247': [[-1]], '305313': [[-1]], '305338': [[-1]], '305350': [[-1]], '305364': [[-1]], '305376': [[-1]], '306042': [[-1]], '306051': [[-1]], '305406': [[-1]], '306122': [[-1]], '306134': [[-1]], '306137': [[-1]], '306154': [[-1]], '306170': [[-1]], '306417': [[-1]], '306432': [[-1]], '306456': [[-1]], '305516': [[-1]], '305586': [[-1]], '305588': [[-1]], '305590': [[-1]], '305809': [[-1]], '305832': [[-1]], '305840': [[-1]], '305898': [[-1]], '306029': [[-1]], '306037': [[-1]], '306095': [[-1]]}\n",
      "selected runs/lumisections as bad test set:\n",
      "{'303824': [[-1]], '303989': [[-1]], '304740': [[-1]], '305250': [[-1]], '306422': [[-1]], '305249': [[-1]]}\n"
     ]
    }
   ],
   "source": [
    "### Define Training Mode Parameters - Combined Autoencoder\n",
    "if training_mode == 'global':\n",
    "    runsls_training = None # use none to not add a mask for training (can e.g. use DCS-bit on mask)\n",
    "    runsls_good = None # use none to not add a mask for good runs (can e.g. use averages of training set)\n",
    "    runsls_bad = badrunsls[year] # predefined bad runs\n",
    "    print('selected runs/lumisections for training: all')\n",
    "    \n",
    "elif training_mode == 'local':\n",
    "    # train locally on a small set of runs\n",
    "    # - either on n runs preceding a chosen application run,\n",
    "    # - or on the run associated as reference to the chosen application run.\n",
    "    # - this only works for a single era\n",
    "    \n",
    "    available_runs = dfu.get_runs( dfu.select_dcson( csvu.read_csv('../data/DF'+year+era+'_'+histnames[0][0]+'.csv') ) )\n",
    "    # Cherry picked really bad run\n",
    "    run_application = 299316\n",
    "    #run_application = 299317\n",
    "    run_application_index = available_runs.index(run_application)\n",
    "    # select training set\n",
    "    usereference = False\n",
    "    if usereference:\n",
    "        run_reference = rru.get_reference_run( run_application, jsonfile='../utils/json_allRunsRefRuns.json' )\n",
    "        if run_reference<0:\n",
    "            raise Exception('no valid reference run has been defined for run {}'.format(run_application))\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(run_reference,[-1])])\n",
    "    else:\n",
    "        ntraining = 5\n",
    "        offset = 0 # normal case: offset = 0 (just use 5 previous runs)\n",
    "        \n",
    "        # Selects the 5 previous runs for training\n",
    "        runsls_training = jsonu.tuplelist_to_jsondict([(el,[-1]) for el in available_runs[run_application_index-ntraining-offset:run_application_index-offset]])\n",
    "    #runsls_bad = badrunsls[year]\n",
    "    #runsls_good = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_bad = jsonu.tuplelist_to_jsondict([(run_application,[-1])])\n",
    "    runsls_good = runsls_training\n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)\n",
    "        \n",
    "elif training_mode == 'development':\n",
    "    # train on a user-defined subset of runs\n",
    "    \n",
    "   # Select runs to be used in training from the user-defined list\n",
    "    runsls_training = {}\n",
    "    runsls_bad = {}\n",
    "    runsls_good = {}\n",
    "    for era in eras:\n",
    "        runsls_training.update(trainrunsls[year + era])\n",
    "        # Select bad runs to test on in the user-defined list\n",
    "        runsls_bad.update(badrunsls[year + era])\n",
    "        # Select good runs to test on in the user-defined list\n",
    "        runsls_good.update(goodrunsls[year + era])\n",
    "    \n",
    "    print('selected runs/lumisections for training: ')\n",
    "    print(runsls_training)\n",
    "    print('selected runs/lumisections as good test set:')\n",
    "    print(runsls_good)\n",
    "    print('selected runs/lumisections as bad test set:')\n",
    "    print(runsls_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f344d",
   "metadata": {},
   "source": [
    "### Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54180f06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Classifiers and masks cleared to preserve consistency\n",
      "Adding chargeInner_PXLayer_1E...\n",
      "Adding chargeInner_PXLayer_2E...\n",
      "Adding chargeInner_PXLayer_3E...\n",
      "Adding chargeInner_PXLayer_4E...\n",
      "Adding charge_PXDisk_+1E...\n",
      "Adding charge_PXDisk_+2E...\n",
      "Adding charge_PXDisk_+3E...\n",
      "Adding charge_PXDisk_-1E...\n",
      "Adding charge_PXDisk_-2E...\n",
      "Adding charge_PXDisk_-3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TIB__layer__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TOB__layer__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__PLUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TID__MINUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__7E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__8E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__PLUS__wheel__9E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__1E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__2E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__3E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__4E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__5E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__6E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__7E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__8E...\n",
      "Adding Summary_ClusterStoNCorr__OnTrack__TEC__MINUS__wheel__9E...\n",
      "Adding NumberOfRecHitsPerTrack_lumiFlag_GenTkE...\n",
      "Found 11476 histograms\n",
      "\n",
      "Adding chargeInner_PXLayer_1F...\n"
     ]
    }
   ],
   "source": [
    "### Data Import\n",
    "\n",
    "# Create a new HistStruct from the data\n",
    "if readnew:\n",
    "    # Initializations\n",
    "    dloader = DataLoader.DataLoader()\n",
    "    histstruct = FlexiStruct.FlexiStruct()\n",
    "    histstruct.reset_histlist(histnames)\n",
    "    \n",
    "    # Unpack histnames and add every histogram individually\n",
    "    for era in eras:\n",
    "        for histnamegroup in histnames:\n",
    "            for histname in histnamegroup:\n",
    "                print('Adding {}...'.format(histname + era))\n",
    "                \n",
    "                # Bring the histograms into memory from storage for later use\n",
    "                filename = datadir[year+era] + 'DF' + year + era + '_' + histname + '.csv'\n",
    "                df = dloader.get_dataframe_from_file( filename )\n",
    "                \n",
    "                # In case of local training, we can remove most of the histograms\n",
    "                if( runsls_training is not None and runsls_good is not None and runsls_bad is not None ):\n",
    "                    runsls_total = {k: v for d in (runsls_training, runsls_good, runsls_bad) for k, v in d.items()}\n",
    "                    df = dfu.select_runsls( df, runsls_total )    \n",
    "                \n",
    "                df = dfu.rm_duplicates(df)\n",
    "                # Store the data in the histstruct object managing this whole thing (ignore warnings if using multiple eras)\n",
    "                histstruct.add_dataframe( df, rebinningfactor = 1, standardbincount = 102 )\n",
    "        print('Found {} histograms\\n'.format(len(histstruct.runnbs)))\n",
    "\n",
    "# Load a previously saved HistStruct\n",
    "else:\n",
    "    # Load histstruct from storage\n",
    "    histstruct = SubHistStruct.SubHistStruct.load( 'histstruct_global_20220201.zip', verbose=False )\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if ('bad' in name and name!='bad')])\n",
    "    \n",
    "    print('loaded a histstruct with the following properties:')\n",
    "    print(histstruct)\n",
    "    # Count of bad runs, presumably for later use\n",
    "    nbadruns = len([name for name in list(histstruct.masks.keys()) if 'bad' in name])\n",
    "    \n",
    "print('Created a histstruct with the following properties:')\n",
    "print('- number of histogram types: {}'.format(len(histstruct.histnames)))\n",
    "print('- number of lumisections: {}'.format(len(histstruct.lsnbs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45931fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add Masks to Data\n",
    "\n",
    "if readnew:\n",
    "    histstruct.add_dcsonjson_mask( 'dcson' )\n",
    "    histstruct.add_goldenjson_mask('golden' )\n",
    "    histstruct.add_highstat_mask( 'highstat', entries_to_bins_ratio=0 )\n",
    "    histstruct.add_stat_mask( 'lowstat', max_entries_to_bins_ratio=0 )\n",
    "    if runsls_training is not None: histstruct.add_json_mask( 'training', runsls_training )\n",
    "    if runsls_good is not None: histstruct.add_json_mask( 'good', runsls_good )\n",
    "        \n",
    "    # Distinguishing bad runs\n",
    "    nbadruns = 0\n",
    "    if runsls_bad is not None:\n",
    "        print(runsls_bad)\n",
    "        histstruct.add_json_mask( 'bad', runsls_bad )\n",
    "        \n",
    "        # Special case for bad runs: add a mask per run (different bad runs have different characteristics)\n",
    "        nbadruns = len(runsls_bad.keys())\n",
    "        for i,badrun in enumerate(runsls_bad.keys()):\n",
    "            histstruct.add_json_mask( 'bad{}'.format(i), {badrun:runsls_bad[badrun]} )\n",
    "            \n",
    "    if save:\n",
    "        histstruct.save('test.pk1')\n",
    "print('Assigned masks: {}'.format(list(histstruct.masks.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5ffcc4",
   "metadata": {},
   "source": [
    "### Plotting and Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6a8ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Plotting the input data for analysis\n",
    "\n",
    "if((training_mode=='local' or training_mode == 'development') and createPlots):\n",
    "\n",
    "    # training and application runs\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','training'],['dcson','highstat','good']],\n",
    "                                labellist = ['training','testing'],\n",
    "                                colorlist = ['blue','green']\n",
    "                              )\n",
    "    \n",
    "    # application run and bad test runs\n",
    "    histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad0']],\n",
    "                                labellist = ['good','bad'],\n",
    "                                colorlist = ['green','red']\n",
    "                              )\n",
    "    \n",
    "elif( training_mode=='global' and createPlots):\n",
    "    \n",
    "    # bad test runs\n",
    "    for i in [0,1,2,3,4,5,6]:\n",
    "        histstruct.plot_histograms( masknames=[['dcson','highstat','good'],['dcson','highstat','bad{}'.format(i)]],\n",
    "                                labellist = ['typical good histograms','bad'],\n",
    "                                colorlist = ['blue','red'],\n",
    "                                transparencylist = [0.01,1.]\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dd4d22",
   "metadata": {},
   "source": [
    "### Concatamash Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc93811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_concatamash_autoencoder(histstruct):\n",
    "    \n",
    "    histslist = []\n",
    "    vallist = []\n",
    "    autoencoders = []\n",
    "    if trainnew:\n",
    "        for i,histnamegroup in enumerate(histnames):\n",
    "            \n",
    "            train_normhist = np.array([hu.normalize(histstruct.get_histograms(\n",
    "                histname = hname, masknames = ['dcson','highstat', 'training']), \n",
    "                                                 norm=\"l1\", axis=1) \n",
    "                                       for hname in histnamegroup]).transpose((1,0,2))\n",
    "            X_train, X_val = train_test_split(train_normhist, test_size=0.4, random_state=42)\n",
    "            \n",
    "            # Half the total bin count\n",
    "            arch = 51 * len(histnamegroup)\n",
    "            \n",
    "            ## Model parameters\n",
    "            input_dim = X_train.shape[2] #num of predictor variables\n",
    "            Input_layers=[Input(shape=input_dim) for i in range((X_train.shape[1]))]\n",
    "            \n",
    "            # Defining layers\n",
    "            conc_layer = Concatenate()(Input_layers)\n",
    "            encoder = Dense(arch * 2, activation=\"tanh\")(conc_layer)\n",
    "            encoder = Dense(arch/2, activation='relu')(encoder)\n",
    "            encoder = Dense(arch/8, activation='relu')(encoder)\n",
    "            encoder = Dense(arch/16, activation='relu')(encoder)\n",
    "            decoder = Dense(arch/8, activation=\"relu\")(encoder)\n",
    "            decoder = Dense(arch/2, activation='relu')(encoder)\n",
    "            decoder = Dense(arch * 2, activation=\"tanh\")(decoder)\n",
    "            \n",
    "            Output_layers=[Dense(input_dim, activation=\"tanh\")(decoder) for i in range(X_train.shape[1])]\n",
    "\n",
    "            autoencoder = Model(inputs=Input_layers, outputs=Output_layers)\n",
    "            autoencoders.append(autoencoder)\n",
    "            \n",
    "            histslist.append(X_train)\n",
    "            vallist.append(X_val)\n",
    "     \n",
    "    # Return the histograms stored 2-Dimensionally and the autoencoders corresponding\n",
    "    return(histslist, vallist, autoencoders, train_normhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76ea4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(histslist, vallist, autoencoders, train_normhist) = define_concatamash_autoencoder(histstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trains a combined autoencoder for every merge set\n",
    "def train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders):\n",
    "    \n",
    "    autoencodersTrain = []\n",
    "    for i in range(len(histslist)):\n",
    "        \n",
    "        print('Now training model {}/'.format(i + 1) + str(len(histslist)))\n",
    "        \n",
    "        # Set variables to temporary values for better transparency\n",
    "        X_train = histslist[i]\n",
    "        X_val = vallist[i]\n",
    "        autoencoder = autoencoders[i]\n",
    "        \n",
    "        \n",
    "        ## Model parameters\n",
    "        nb_epoch = 500\n",
    "        batch_size = 10000\n",
    "        \n",
    "        #checkpoint_filepath = 'checkpoint'\n",
    "        #model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #   filepath=checkpoint_filepath,\n",
    "        #   save_weights_only=False,\n",
    "        #   verbose=1,\n",
    "        #   save_best_only=True,\n",
    "        #   monitor='val_loss',\n",
    "        #   mode='min')\n",
    "        \n",
    "        # Tell the model when to stop\n",
    "        earlystop = EarlyStopping(monitor='val_loss',\n",
    "            min_delta=1e-7,\n",
    "            patience=20,\n",
    "            verbose=1,\n",
    "            mode='auto',\n",
    "            baseline=None,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "        lr =0.001\n",
    "        opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "        \n",
    "        autoencoder.compile(loss='mse',\n",
    "                            optimizer=opt)\n",
    "        \n",
    "        ## Train autoencoder\n",
    "        train = autoencoder.fit(x=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                                y=[X_train[:,i] for i in range(X_train.shape[1])],\n",
    "                                epochs=nb_epoch,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                validation_data=([X_val[:,i] for i in range(X_val.shape[1])], [X_val[:,i] for i in range(X_val.shape[1])]),\n",
    "                                verbose=1,\n",
    "                                callbacks= [earlystop],    \n",
    "                                )\n",
    "        tf.keras.utils.plot_model(\n",
    "                    autoencoder,\n",
    "                    to_file=\"../Output Data/models/model1D{}.png\".format(i),\n",
    "                    show_shapes=True,\n",
    "                    show_dtype=False,\n",
    "                    show_layer_names=False,\n",
    "                    rankdir=\"TB\")\n",
    "        \n",
    "        # Save classifier for evaluation\n",
    "        classifier = AutoEncoder.AutoEncoder(model=autoencoder)\n",
    "        histstruct.add_classifier(histnames[i][0], classifier) \n",
    "        autoencodersTrain.append(classifier)\n",
    "        \n",
    "        autoencoder.save('../SavedModels/Concatamash/AE' + str(i))\n",
    "        K.clear_session()\n",
    "        del(autoencoder, classifier)\n",
    "    return autoencodersTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c02f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functionality doesn't seem to work at this time, so this function is empty\n",
    "def load_concatamash_autoencoder():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52397fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "if trainnew: train_concatamash_autoencoder(histstruct, histslist, vallist, autoencoders)\n",
    "else: load_concatamash_autoencoder()\n",
    "stop = time.perf_counter()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1167afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluate the Models for WP definition\n",
    "def evaluate_models_train(histstruct):\n",
    "    \n",
    "    for histgroup in histnames:\n",
    "        print('evaluating model for '+histgroup[0])\n",
    "        print(histstruct.evaluate_classifier(histgroup)[0].shape)\n",
    "    \n",
    "    # get mse for training set\n",
    "    if 'training' in histstruct.masks.keys(): masknames = ['dcson','highstat', 'training']\n",
    "    else: masknames = ['dcson','highstat']\n",
    "    mse_train = histstruct.get_scores_array( masknames=masknames )\n",
    "    print('Found mse array for training set of following shape: {}'.format(mse_train.shape))\n",
    "    \n",
    "    # get mse for good set\n",
    "    if 'good' in histstruct.masks.keys():\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            mse_good.append(histstruct.get_scores( histname=histname, masknames=['dcson','highstat','good'] ))\n",
    "    else:\n",
    "        mse_good = []\n",
    "        for histname in histstruct.histnames:\n",
    "            hists_good = hu.averagehists( histstruct.get_histograms( histname=histname, masknames=['dcson','highstat']), 1000 )\n",
    "            thismse = histstruct.classifiers[histname].evaluate( hists_good )\n",
    "            mse_good.append( thismse )\n",
    "            print(run)\n",
    "    mse_good = np.array(mse_good)\n",
    "    mse_good = np.transpose(mse_good)\n",
    "    print('Found mse array for good set of following shape: {}'.format(mse_good.shape))\n",
    "    \n",
    "    # get mse for bad sets\n",
    "    mse_bad = []\n",
    "    for i in range(nbadruns):\n",
    "        mse_bad.append( histstruct.get_scores_array( masknames=['dcson','highstat','bad{}'.format(i)] ) )\n",
    "        print('Found mse array for bad set of following shape: {}'.format(mse_bad[i].shape))\n",
    "        \n",
    "    return [mse_train, mse_good, mse_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34546ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mse_train, mse_good_eval, mse_bad_eval) = evaluate_models_train(histstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots and Distribution Analysis\n",
    "def fit_mse_distribution(histstruct, mse_train):\n",
    "    dimslist = []\n",
    "    fitfunclist = []\n",
    "    \n",
    "    \n",
    "    nhisttypes = len(histstruct.histnames)\n",
    "    for i in range(0,nhisttypes-1):\n",
    "        for j in range(i+1,nhisttypes):\n",
    "            dimslist.append((i, j))\n",
    "    \n",
    "    plt.close('all')\n",
    "    (npoints,ndims) = mse_train.shape\n",
    "    \n",
    "    \n",
    "    # settings for GaussianKdeFitter\n",
    "    scott_bw = npoints**(-1./(ndims+4))\n",
    "    bw_method = 20*scott_bw\n",
    "    # settings for HyperRectangleFitter\n",
    "    quantiles = ([0.00062,0.0006,0.00015,0.00015,\n",
    "                 0.0003,0.0003,0.00053,0.00065])\n",
    "    \n",
    "    \n",
    "    #for dims in dimslist:\n",
    "    #    thismse = mse_train[:,dims]\n",
    "    #    if training_mode=='global': \n",
    "    #        fitfunc = SeminormalFitter.SeminormalFitter(thismse)\n",
    "    #        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(thismse, \n",
    "    #        #                                                    [quantiles[dims[0]],quantiles[dims[1]]],\n",
    "    #        #                                                    'up')\n",
    "    #    else: fitfunc = GaussianKdeFitter.GaussianKdeFitter(thismse,bw_method=bw_method)\n",
    "    #    #pu.plot_fit_2d(thismse, fitfunc=fitfunc, logprob=True, clipprob=True,\n",
    "    #    #                onlycontour=False, xlims=30, ylims=30, \n",
    "    #    #                onlypositive=True, transparency=0.5,\n",
    "    #    #                xaxtitle=histstruct.histnames[dims[0]], \n",
    "    #    #                yaxtitle=histstruct.histnames[dims[1]],\n",
    "    #    #                title='density fit of lumisection MSE')\n",
    "    #    ##plt.close('all') # release plot memory\n",
    "    #    fitfunclist.append(fitfunc)\n",
    "    # \n",
    "    #    \n",
    "    if training_mode=='global': \n",
    "        fitfunc = SeminormalFitter.SeminormalFitter(mse_train)\n",
    "        #fitfunc = HyperRectangleFitter.HyperRectangleFitter(mse_train, quantiles, 'up')\n",
    "    else: \n",
    "        fitfunc = GaussianKdeFitter.GaussianKdeFitter()\n",
    "        fitfunc.fit(mse_train,bw_method=bw_method)\n",
    "    \n",
    "    return fitfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4988fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitfunc = fit_mse_distribution(histstruct, mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare MSEs for Working Point Definition\n",
    "def mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc):\n",
    "    \n",
    "    # Get the minimum log probability of histograms in good set\n",
    "    print('--- good lumesections ---')\n",
    "    logprob_good = np.log(fitfunc.pdf(mse_good_eval))\n",
    "    print('length of log prob array: '+str(len(logprob_good)))\n",
    "    print('minimum of log prob: '+str(np.min(logprob_good)))\n",
    "    #print(sorted(logprob_good))\n",
    "    \n",
    "    print('--- bad lumisections ---')\n",
    "    logprob_bad_parts = [np.log(fitfunc.pdf(mse_bad_eval[j])) for j in range(len(mse_bad_eval))]\n",
    "    #for lp in logprob_bad_parts: print(str(sorted(lp))+'\\n\\n')\n",
    "    logprob_bad = np.concatenate(tuple(logprob_bad_parts))\n",
    "    \n",
    "    print('length of log prob array: '+str(len(logprob_bad)))\n",
    "    print('maximum of log prob: '+str(np.max(logprob_bad)))\n",
    "    #print(sorted(logprob_good))\n",
    "    #print(sorted(logprob_bad))\n",
    "    #print(logprob_bad)\n",
    "    \n",
    "    sep = np.min(logprob_good) - np.max(logprob_bad)\n",
    "    print('Separability: ' + str(sep))\n",
    "    \n",
    "    return [logprob_good, logprob_bad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(logprob_good, logprob_bad) = mse_analysis(histstruct, mse_good_eval, mse_bad_eval, fitfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor):\n",
    "    labels_good = np.zeros(len(logprob_good)) # background: label = 0\n",
    "    labels_bad = np.ones(len(logprob_bad)) # signal: label = 1\n",
    "    \n",
    "    badMin = min(np.where(logprob_bad != -np.inf, logprob_bad, -1))\n",
    "    goodMax = max(np.where(logprob_good != np.inf, logprob_good, 10001))\n",
    "    \n",
    "    logprob_good = np.where(logprob_good != np.inf, logprob_good, goodMax)\n",
    "    logprob_bad = np.where(logprob_bad != -np.inf, logprob_bad, badMin)\n",
    "    \n",
    "    # These only take effect if a histogram is grossly misclassified\n",
    "    logprob_good[logprob_good == -np.inf] = badMin\n",
    "    logprob_bad[logprob_bad == np.inf] = goodMax\n",
    "    \n",
    "    labels = np.concatenate(tuple([labels_good,labels_bad]))\n",
    "    scores = np.concatenate(tuple([-logprob_good,-logprob_bad]))\n",
    "    scores = aeu.clip_scores( scores )\n",
    "    \n",
    "    avSep = np.mean(logprob_good) - np.mean(logprob_bad)\n",
    "    \n",
    "    print('Average Separation: ' + str(avSep))\n",
    "    \n",
    "    # Setting a threshold, below this working point defines anomalous data\n",
    "    # Average is biased towards better recall per user specifications\n",
    "    logprob_threshold = (1/(wpBiasFactor + 1)) * (wpBiasFactor*np.mean(logprob_good) + np.mean(logprob_bad))\n",
    "    # Or set manual\n",
    "    # logprob_threshold = 100\n",
    "    \n",
    "    (fix, ax) = pu.plot_score_dist(scores, labels, siglabel='anomalous', sigcolor='r', \n",
    "                       bcklabel='good', bckcolor='g', \n",
    "                       nbins=200, normalize=True,\n",
    "                       xaxtitle='negative logarithmic probability',\n",
    "                       yaxtitle='number of lumisections (normalized)', doshow=False)\n",
    "    plt.axvline(x=-logprob_threshold, color='b', label='WP')\n",
    "    plt.show()\n",
    "    \n",
    "    mis = np.where((labels==1) & (scores<-logprob_threshold),1,0).astype(bool)\n",
    "    labels2 = np.where(mis, 0, labels)\n",
    "    mis = np.where((labels2==0) & (scores>-logprob_threshold),1,0).astype(bool)\n",
    "    labels2 = np.where(mis, 1, labels2)\n",
    "    np.savetxt('../KH-AutoencoderTest/Labels.csv', labels2, delimiter=',')\n",
    "    \n",
    "    # Plot ROC curve for analysis\n",
    "    auc = aeu.get_roc(scores, labels, mode='geom', doprint=False)\n",
    "    \n",
    "    \n",
    "    (_, _, _, tp, fp, tn, fn) = aeu.get_confusion_matrix(scores,labels,-logprob_threshold)\n",
    "    print('Selected logprob threshold of ' + str(logprob_threshold))\n",
    "    \n",
    "    # Get metrics for analysis\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_measure = (1 + fmBiasFactor * fmBiasFactor) * ((precision * recall) / ((fmBiasFactor * fmBiasFactor * precision) + recall)) \n",
    "    \n",
    "    print('Accuracy: ' + str(accuracy))\n",
    "    print('Precision: ' + str(precision))\n",
    "    print('Recall: ' + str(recall))\n",
    "    print('F-Measure: ' + str(f_measure))\n",
    "    \n",
    "    return logprob_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbddf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprob_threshold = evaluate_autoencoders_combined(logprob_good, logprob_bad, fmBiasFactor, wpBiasFactor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
